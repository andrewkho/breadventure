{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b42b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977705c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b666437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be20236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c5666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e637aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc32ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6485a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6bc5585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[SEP]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88af1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/recipes-combined/individual/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "747195db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_recipes(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for text_file in (split_dir).iterdir():\n",
    "        texts.append(text_file.read_text())\n",
    "\n",
    "    return texts\n",
    "\n",
    "recipes = read_recipes(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e5e0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(recipes, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c6bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1db334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2067fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb1f8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = False\n",
    "max_length=1024\n",
    "train_encodings = tokenizer(train_data, truncation=True, padding=padding, max_length=max_length)\n",
    "val_encodings = tokenizer(val_data, truncation=True, padding=padding, max_length=max_length)\n",
    "test_encodings = tokenizer(test_data, truncation=True, padding=padding, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42ee188c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(val_encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5b653fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "596eedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1b5692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        self.labels = encodings['input_ids'].copy()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RecipeDataset(group_texts(train_encodings))\n",
    "val_dataset = RecipeDataset(group_texts(val_encodings))\n",
    "test_dataset = RecipeDataset(group_texts(test_encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "932f46a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1370, 151, 168)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2024d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black Friday Bread\n",
      "    \n",
      "    Black Friday — the day after Thanksgiving — is equal parts shopping and leftovers. Turkey sandwiches are a must; but what do you do with all those leftover bits of stuffing, mashed potatoes, squash, creamed onions... Well, kill two birds with one stone: make a delicious sandwich loaf, AND use those other leftovers from the Turkey Day meal right in the bread itself. This moist, flavorful bread slices beautifully; and when you use stuffing as one of the ingredients, its mild herb flavor is perfect for a turkey sandwich.\n",
      "    \n",
      "       113g King Arthur White Whole Wheat Flour 298g King Arthur Unbleached Bread Flour 28g soft butter 1 to 1 1/4 teaspoons salt, to taste 14g sugar 2 1/2 teaspoons instant yeast 152g lukewarm milk 128g prepared stuffing 213g mashed potatoes, white or sweet \n",
      "    \n",
      "     Directions   Place all of the ingredients in a bowl (or the bowl of your stand mixer; or a bread machine bucket); and mix and knead to make a smooth, elastic, and somewhat sticky dough. The dough will feel tacky, but should hold its shape nicely; you should be able to handle it easily with greased hands.  Place the dough in a lightly greased bowl or 8-cup measure (for easiest tracking of the dough as it rises). Allow it to rise until it's quite puffy, 1 1/2 to 2 hours.  Gently deflate the dough. Shape it into an 11\" to 12\" log, and place it on a parchment-lined or lightly greased baking sheet.  Cover the loaf with lightly greased plastic wrap or a dough cover (the plastic cover of a disposable supermarket deli tray or cake platter may work well for you here), and let the loaf rise until it's noticeably puffy, 1 1/2 to 2 hours. Towards the end of the rising time, preheat the oven to 350°F.   Bake the bread for 35 to 45 minutes, until it's golden brown on top, and an instant-read thermometer inserted into the center registers at least 190°F.   Remove the bread from the oven, and place it on a rack to cool. When completely cool, wrap airtight and store at room temperature for 3 to 4 days; freeze for longer storage.  \n",
      "    \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e537ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c6e4295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilgpt2'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "533cbe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-recipes\",\n",
    "    num_train_epochs=10.,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,    \n",
    "    # push_to_hub=True,\n",
    "    # fp16 didn't lower memory usage in a meaningful way, i guess because\n",
    "    # batch size is already a small % of memory, and also slowed down training\n",
    "    fp16=True, \n",
    "    gradient_accumulation_steps=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a62eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08a77d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0db19a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7b43ca2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  326899 KB |  326899 KB |  326899 KB |       0 B  |\\n|       from large pool |  320512 KB |  320512 KB |  320512 KB |       0 B  |\\n|       from small pool |    6387 KB |    6387 KB |    6387 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  326899 KB |  326899 KB |  326899 KB |       0 B  |\\n|       from large pool |  320512 KB |  320512 KB |  320512 KB |       0 B  |\\n|       from small pool |    6387 KB |    6387 KB |    6387 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  364544 KB |  364544 KB |  364544 KB |       0 B  |\\n|       from large pool |  356352 KB |  356352 KB |  356352 KB |       0 B  |\\n|       from small pool |    8192 KB |    8192 KB |    8192 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   37645 KB |   37654 KB |  144637 KB |  106992 KB |\\n|       from large pool |   35840 KB |   35840 KB |  139520 KB |  103680 KB |\\n|       from small pool |    1805 KB |    2045 KB |    5117 KB |    3312 KB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      88    |      88    |      88    |       0    |\\n|       from large pool |      26    |      26    |      26    |       0    |\\n|       from small pool |      62    |      62    |      62    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      88    |      88    |      88    |       0    |\\n|       from large pool |      26    |      26    |      26    |       0    |\\n|       from small pool |      62    |      62    |      62    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      15    |      15    |      15    |       0    |\\n|       from large pool |      11    |      11    |      11    |       0    |\\n|       from small pool |       4    |       4    |       4    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      12    |      12    |      14    |       2    |\\n|       from large pool |      10    |      10    |      10    |       0    |\\n|       from small pool |       2    |       2    |       4    |       2    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6111ae90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1370\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='850' max='850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [850/850 38:07, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.721223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.562458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.484214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.435942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.399940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.727200</td>\n",
       "      <td>2.375810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.727200</td>\n",
       "      <td>2.358845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.727200</td>\n",
       "      <td>2.348522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.727200</td>\n",
       "      <td>2.340244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.727200</td>\n",
       "      <td>2.338228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/workspace/breadventure/venv/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt2-finetuned-recipes/checkpoint-500\n",
      "Configuration saved in distilgpt2-finetuned-recipes/checkpoint-500/config.json\n",
      "Model weights saved in distilgpt2-finetuned-recipes/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=850, training_loss=2.6216669060202205, metrics={'train_runtime': 2290.2138, 'train_samples_per_second': 5.982, 'train_steps_per_second': 0.371, 'total_flos': 1788576257802240.0, 'train_loss': 2.6216669060202205, 'epoch': 9.99})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85f01edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/151 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 10.36\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53c4cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_local = 'recipes_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85912e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in recipes_model/config.json\n",
      "Model weights saved in recipes_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(model_checkpoint_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1fad8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60fcec1b",
   "metadata": {},
   "source": [
    "# Try to load back the model to see if that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "452eb776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file recipes_model/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file recipes_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at recipes_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model2 = AutoModelForCausalLM.from_pretrained(model_checkpoint_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b78a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3190ceb",
   "metadata": {},
   "source": [
    "# Try to generate text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1e8e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Strawberry Bread Machine Loaf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d2104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "outputs = model2.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60, temperature=1.)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda5a7b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c456c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
