{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.onnx\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import model as rnn_model\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_seed = 1234\n",
    "args_temperature = 1.\n",
    "args_data = '../data/wikitext-2'\n",
    "args_model = 'LSTM'\n",
    "args_emsize = 200\n",
    "args_nhid = 200\n",
    "args_nlayers = 2\n",
    "args_lr = 20\n",
    "args_clip = 0.25\n",
    "args_epochs = 40\n",
    "args_batch_size = 20\n",
    "args_bptt = 35\n",
    "args_dropout = 0.2\n",
    "args_log_interval = 200\n",
    "args_save = 'model.pt'\n",
    "args_save_state = 'model_state.pt'\n",
    "args_tied = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args_seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "corpus = data.Corpus(args_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, args_batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = rnn_model.RNNModel(args_model, ntokens, args_emsize, args_nhid, args_nlayers, args_dropout, args_tied).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Use Adam optimizer\n",
    "###############################################################################\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args_bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args_batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args_bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % args_log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args_log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args_bptt, \n",
    "                elapsed * 1000 / args_log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 29.01 | loss  7.75 | ppl  2314.22\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 27.05 | loss  6.80 | ppl   896.91\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 27.26 | loss  6.42 | ppl   613.78\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 27.20 | loss  6.30 | ppl   545.85\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 27.02 | loss  6.20 | ppl   491.94\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 26.98 | loss  6.15 | ppl   470.22\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 26.89 | loss  6.09 | ppl   441.54\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 27.13 | loss  6.09 | ppl   440.18\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 27.55 | loss  5.97 | ppl   391.36\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 27.08 | loss  5.98 | ppl   394.58\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 27.05 | loss  5.88 | ppl   356.09\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 26.98 | loss  5.90 | ppl   364.77\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 27.00 | loss  5.87 | ppl   355.81\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 27.28 | loss  5.79 | ppl   326.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 84.01s | valid loss  5.60 | valid ppl   269.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 27.52 | loss  5.65 | ppl   285.25\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 27.08 | loss  5.60 | ppl   271.15\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 27.02 | loss  5.51 | ppl   247.52\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 27.06 | loss  5.53 | ppl   253.13\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 27.22 | loss  5.51 | ppl   247.43\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 27.22 | loss  5.52 | ppl   249.13\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 27.05 | loss  5.52 | ppl   249.56\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 27.03 | loss  5.55 | ppl   258.15\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 27.00 | loss  5.45 | ppl   232.37\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 26.92 | loss  5.49 | ppl   242.93\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 26.89 | loss  5.40 | ppl   220.73\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 26.94 | loss  5.42 | ppl   226.46\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 26.90 | loss  5.44 | ppl   230.42\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 27.00 | loss  5.36 | ppl   212.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 83.45s | valid loss  5.36 | valid ppl   212.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 27.04 | loss  5.33 | ppl   207.12\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 26.98 | loss  5.32 | ppl   205.18\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 27.07 | loss  5.22 | ppl   184.52\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 26.95 | loss  5.26 | ppl   193.31\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 27.41 | loss  5.25 | ppl   190.86\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 27.05 | loss  5.26 | ppl   192.04\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 27.09 | loss  5.27 | ppl   195.13\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 27.07 | loss  5.32 | ppl   204.20\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 27.38 | loss  5.23 | ppl   186.84\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 27.04 | loss  5.26 | ppl   193.37\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 27.00 | loss  5.19 | ppl   179.66\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 26.90 | loss  5.20 | ppl   181.24\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 26.98 | loss  5.23 | ppl   187.08\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 27.21 | loss  5.18 | ppl   177.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 83.55s | valid loss  5.28 | valid ppl   196.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 27.29 | loss  5.16 | ppl   173.58\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 27.28 | loss  5.17 | ppl   176.28\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 27.26 | loss  5.05 | ppl   155.60\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 27.10 | loss  5.11 | ppl   165.42\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 27.04 | loss  5.10 | ppl   163.45\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 27.10 | loss  5.11 | ppl   165.34\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 27.12 | loss  5.13 | ppl   168.76\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 27.02 | loss  5.18 | ppl   177.50\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 26.98 | loss  5.10 | ppl   163.36\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 26.90 | loss  5.14 | ppl   169.99\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 27.25 | loss  5.06 | ppl   157.31\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 26.98 | loss  5.09 | ppl   161.81\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 27.12 | loss  5.11 | ppl   165.31\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 26.98 | loss  5.06 | ppl   158.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 83.70s | valid loss  5.25 | valid ppl   190.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 27.24 | loss  5.04 | ppl   154.82\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 27.03 | loss  5.07 | ppl   159.48\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 27.07 | loss  4.94 | ppl   139.63\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 27.09 | loss  5.01 | ppl   149.97\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 27.09 | loss  5.00 | ppl   148.89\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 26.97 | loss  5.02 | ppl   150.70\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 27.00 | loss  5.02 | ppl   152.04\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 26.93 | loss  5.09 | ppl   162.65\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 26.99 | loss  5.00 | ppl   149.01\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 26.98 | loss  5.05 | ppl   155.96\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 27.11 | loss  4.96 | ppl   142.42\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 26.99 | loss  5.01 | ppl   150.00\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 27.00 | loss  5.02 | ppl   151.11\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 26.99 | loss  4.98 | ppl   145.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 83.43s | valid loss  5.29 | valid ppl   197.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 27.47 | loss  4.97 | ppl   144.26\n",
      "| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 27.05 | loss  5.00 | ppl   148.06\n",
      "| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 26.97 | loss  4.87 | ppl   129.95\n",
      "| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 27.09 | loss  4.93 | ppl   138.72\n",
      "| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 27.49 | loss  4.93 | ppl   138.31\n",
      "| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 27.05 | loss  4.94 | ppl   139.25\n",
      "| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  4.96 | ppl   141.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 26.97 | loss  5.02 | ppl   150.83\n",
      "| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.11 | loss  4.94 | ppl   139.38\n",
      "| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 27.09 | loss  4.99 | ppl   147.03\n",
      "| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  4.90 | ppl   133.81\n",
      "| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.09 | loss  4.94 | ppl   139.18\n",
      "| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 27.12 | loss  4.95 | ppl   140.48\n",
      "| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.06 | loss  4.92 | ppl   137.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 83.67s | valid loss  5.28 | valid ppl   196.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2983 batches | lr 1.25 | ms/batch 27.23 | loss  4.90 | ppl   134.71\n",
      "| epoch   7 |   400/ 2983 batches | lr 1.25 | ms/batch 27.17 | loss  4.94 | ppl   139.95\n",
      "| epoch   7 |   600/ 2983 batches | lr 1.25 | ms/batch 27.15 | loss  4.81 | ppl   122.79\n",
      "| epoch   7 |   800/ 2983 batches | lr 1.25 | ms/batch 27.23 | loss  4.88 | ppl   131.83\n",
      "| epoch   7 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.14 | loss  4.88 | ppl   131.89\n",
      "| epoch   7 |  1200/ 2983 batches | lr 1.25 | ms/batch 27.08 | loss  4.89 | ppl   133.05\n",
      "| epoch   7 |  1400/ 2983 batches | lr 1.25 | ms/batch 27.07 | loss  4.90 | ppl   134.44\n",
      "| epoch   7 |  1600/ 2983 batches | lr 1.25 | ms/batch 27.21 | loss  4.98 | ppl   144.76\n",
      "| epoch   7 |  1800/ 2983 batches | lr 1.25 | ms/batch 27.01 | loss  4.89 | ppl   133.29\n",
      "| epoch   7 |  2000/ 2983 batches | lr 1.25 | ms/batch 27.35 | loss  4.95 | ppl   141.33\n",
      "| epoch   7 |  2200/ 2983 batches | lr 1.25 | ms/batch 27.08 | loss  4.85 | ppl   127.32\n",
      "| epoch   7 |  2400/ 2983 batches | lr 1.25 | ms/batch 27.13 | loss  4.88 | ppl   131.74\n",
      "| epoch   7 |  2600/ 2983 batches | lr 1.25 | ms/batch 27.10 | loss  4.92 | ppl   136.47\n",
      "| epoch   7 |  2800/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  4.87 | ppl   130.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 83.75s | valid loss  5.32 | valid ppl   204.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2983 batches | lr 0.31 | ms/batch 27.27 | loss  4.85 | ppl   128.27\n",
      "| epoch   8 |   400/ 2983 batches | lr 0.31 | ms/batch 27.02 | loss  4.90 | ppl   134.75\n",
      "| epoch   8 |   600/ 2983 batches | lr 0.31 | ms/batch 27.35 | loss  4.77 | ppl   118.32\n",
      "| epoch   8 |   800/ 2983 batches | lr 0.31 | ms/batch 27.19 | loss  4.84 | ppl   126.40\n",
      "| epoch   8 |  1000/ 2983 batches | lr 0.31 | ms/batch 27.05 | loss  4.85 | ppl   127.17\n",
      "| epoch   8 |  1200/ 2983 batches | lr 0.31 | ms/batch 27.15 | loss  4.85 | ppl   127.44\n",
      "| epoch   8 |  1400/ 2983 batches | lr 0.31 | ms/batch 27.04 | loss  4.85 | ppl   128.08\n",
      "| epoch   8 |  1600/ 2983 batches | lr 0.31 | ms/batch 27.49 | loss  4.92 | ppl   137.62\n",
      "| epoch   8 |  1800/ 2983 batches | lr 0.31 | ms/batch 27.07 | loss  4.85 | ppl   128.05\n",
      "| epoch   8 |  2000/ 2983 batches | lr 0.31 | ms/batch 27.10 | loss  4.90 | ppl   134.86\n",
      "| epoch   8 |  2200/ 2983 batches | lr 0.31 | ms/batch 27.14 | loss  4.80 | ppl   122.08\n",
      "| epoch   8 |  2400/ 2983 batches | lr 0.31 | ms/batch 27.26 | loss  4.85 | ppl   127.24\n",
      "| epoch   8 |  2600/ 2983 batches | lr 0.31 | ms/batch 27.10 | loss  4.87 | ppl   130.85\n",
      "| epoch   8 |  2800/ 2983 batches | lr 0.31 | ms/batch 27.21 | loss  4.84 | ppl   126.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 83.85s | valid loss  5.33 | valid ppl   207.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2983 batches | lr 0.08 | ms/batch 27.29 | loss  4.81 | ppl   123.07\n",
      "| epoch   9 |   400/ 2983 batches | lr 0.08 | ms/batch 26.98 | loss  4.88 | ppl   131.16\n",
      "| epoch   9 |   600/ 2983 batches | lr 0.08 | ms/batch 27.13 | loss  4.75 | ppl   115.03\n",
      "| epoch   9 |   800/ 2983 batches | lr 0.08 | ms/batch 27.13 | loss  4.80 | ppl   121.41\n",
      "| epoch   9 |  1000/ 2983 batches | lr 0.08 | ms/batch 27.00 | loss  4.81 | ppl   123.10\n",
      "| epoch   9 |  1200/ 2983 batches | lr 0.08 | ms/batch 27.01 | loss  4.81 | ppl   122.97\n",
      "| epoch   9 |  1400/ 2983 batches | lr 0.08 | ms/batch 26.95 | loss  4.83 | ppl   124.77\n",
      "| epoch   9 |  1600/ 2983 batches | lr 0.08 | ms/batch 26.97 | loss  4.91 | ppl   135.55\n",
      "| epoch   9 |  1800/ 2983 batches | lr 0.08 | ms/batch 26.91 | loss  4.82 | ppl   124.32\n",
      "| epoch   9 |  2000/ 2983 batches | lr 0.08 | ms/batch 27.11 | loss  4.88 | ppl   130.99\n",
      "| epoch   9 |  2200/ 2983 batches | lr 0.08 | ms/batch 27.26 | loss  4.77 | ppl   118.13\n",
      "| epoch   9 |  2400/ 2983 batches | lr 0.08 | ms/batch 27.02 | loss  4.81 | ppl   122.58\n",
      "| epoch   9 |  2600/ 2983 batches | lr 0.08 | ms/batch 27.12 | loss  4.85 | ppl   127.85\n",
      "| epoch   9 |  2800/ 2983 batches | lr 0.08 | ms/batch 27.22 | loss  4.81 | ppl   122.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 83.56s | valid loss  5.38 | valid ppl   216.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2983 batches | lr 0.02 | ms/batch 27.19 | loss  4.79 | ppl   120.62\n",
      "| epoch  10 |   400/ 2983 batches | lr 0.02 | ms/batch 27.20 | loss  4.85 | ppl   127.84\n",
      "| epoch  10 |   600/ 2983 batches | lr 0.02 | ms/batch 27.22 | loss  4.71 | ppl   111.53\n",
      "| epoch  10 |   800/ 2983 batches | lr 0.02 | ms/batch 27.05 | loss  4.77 | ppl   117.87\n",
      "| epoch  10 |  1000/ 2983 batches | lr 0.02 | ms/batch 27.05 | loss  4.79 | ppl   119.92\n",
      "| epoch  10 |  1200/ 2983 batches | lr 0.02 | ms/batch 27.09 | loss  4.80 | ppl   121.81\n",
      "| epoch  10 |  1400/ 2983 batches | lr 0.02 | ms/batch 27.09 | loss  4.80 | ppl   121.25\n",
      "| epoch  10 |  1600/ 2983 batches | lr 0.02 | ms/batch 26.97 | loss  4.87 | ppl   130.70\n",
      "| epoch  10 |  1800/ 2983 batches | lr 0.02 | ms/batch 27.15 | loss  4.80 | ppl   121.04\n",
      "| epoch  10 |  2000/ 2983 batches | lr 0.02 | ms/batch 27.08 | loss  4.85 | ppl   128.30\n",
      "| epoch  10 |  2200/ 2983 batches | lr 0.02 | ms/batch 27.29 | loss  4.74 | ppl   114.59\n",
      "| epoch  10 |  2400/ 2983 batches | lr 0.02 | ms/batch 26.97 | loss  4.78 | ppl   118.97\n",
      "| epoch  10 |  2600/ 2983 batches | lr 0.02 | ms/batch 27.19 | loss  4.82 | ppl   124.26\n",
      "| epoch  10 |  2800/ 2983 batches | lr 0.02 | ms/batch 26.98 | loss  4.77 | ppl   117.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 83.76s | valid loss  5.28 | valid ppl   196.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 2983 batches | lr 0.00 | ms/batch 27.56 | loss  4.76 | ppl   116.77\n",
      "| epoch  11 |   400/ 2983 batches | lr 0.00 | ms/batch 26.96 | loss  4.82 | ppl   123.61\n",
      "| epoch  11 |   600/ 2983 batches | lr 0.00 | ms/batch 26.99 | loss  4.69 | ppl   108.81\n",
      "| epoch  11 |   800/ 2983 batches | lr 0.00 | ms/batch 26.91 | loss  4.75 | ppl   115.72\n",
      "| epoch  11 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.06 | loss  4.77 | ppl   117.58\n",
      "| epoch  11 |  1200/ 2983 batches | lr 0.00 | ms/batch 27.39 | loss  4.77 | ppl   118.44\n",
      "| epoch  11 |  1400/ 2983 batches | lr 0.00 | ms/batch 27.30 | loss  4.77 | ppl   118.03\n",
      "| epoch  11 |  1600/ 2983 batches | lr 0.00 | ms/batch 27.12 | loss  4.84 | ppl   126.93\n",
      "| epoch  11 |  1800/ 2983 batches | lr 0.00 | ms/batch 26.96 | loss  4.76 | ppl   117.17\n",
      "| epoch  11 |  2000/ 2983 batches | lr 0.00 | ms/batch 26.98 | loss  4.83 | ppl   125.72\n",
      "| epoch  11 |  2200/ 2983 batches | lr 0.00 | ms/batch 26.99 | loss  4.71 | ppl   111.60\n",
      "| epoch  11 |  2400/ 2983 batches | lr 0.00 | ms/batch 27.01 | loss  4.75 | ppl   115.47\n",
      "| epoch  11 |  2600/ 2983 batches | lr 0.00 | ms/batch 27.01 | loss  4.80 | ppl   121.17\n",
      "| epoch  11 |  2800/ 2983 batches | lr 0.00 | ms/batch 26.94 | loss  4.75 | ppl   115.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 83.61s | valid loss  5.27 | valid ppl   193.79\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 |   200/ 2983 batches | lr 0.00 | ms/batch 27.36 | loss  4.74 | ppl   114.11\n",
      "| epoch  12 |   400/ 2983 batches | lr 0.00 | ms/batch 27.13 | loss  4.79 | ppl   120.79\n",
      "| epoch  12 |   600/ 2983 batches | lr 0.00 | ms/batch 27.16 | loss  4.67 | ppl   107.04\n",
      "| epoch  12 |   800/ 2983 batches | lr 0.00 | ms/batch 26.96 | loss  4.73 | ppl   113.24\n",
      "| epoch  12 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.12 | loss  4.74 | ppl   114.48\n",
      "| epoch  12 |  1200/ 2983 batches | lr 0.00 | ms/batch 26.92 | loss  4.75 | ppl   116.05\n",
      "| epoch  12 |  1400/ 2983 batches | lr 0.00 | ms/batch 26.96 | loss  4.75 | ppl   115.07\n",
      "| epoch  12 |  1600/ 2983 batches | lr 0.00 | ms/batch 26.99 | loss  4.82 | ppl   123.76\n",
      "| epoch  12 |  1800/ 2983 batches | lr 0.00 | ms/batch 27.17 | loss  4.76 | ppl   116.23\n",
      "| epoch  12 |  2000/ 2983 batches | lr 0.00 | ms/batch 27.18 | loss  4.82 | ppl   123.45\n",
      "| epoch  12 |  2200/ 2983 batches | lr 0.00 | ms/batch 27.03 | loss  4.70 | ppl   109.70\n",
      "| epoch  12 |  2400/ 2983 batches | lr 0.00 | ms/batch 27.24 | loss  4.73 | ppl   113.01\n",
      "| epoch  12 |  2600/ 2983 batches | lr 0.00 | ms/batch 27.07 | loss  4.78 | ppl   119.46\n",
      "| epoch  12 |  2800/ 2983 batches | lr 0.00 | ms/batch 27.29 | loss  4.72 | ppl   112.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 83.61s | valid loss  5.26 | valid ppl   192.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 2983 batches | lr 0.00 | ms/batch 27.28 | loss  4.72 | ppl   112.60\n",
      "| epoch  13 |   400/ 2983 batches | lr 0.00 | ms/batch 27.02 | loss  4.78 | ppl   119.17\n",
      "| epoch  13 |   600/ 2983 batches | lr 0.00 | ms/batch 27.16 | loss  4.66 | ppl   105.45\n",
      "| epoch  13 |   800/ 2983 batches | lr 0.00 | ms/batch 27.19 | loss  4.71 | ppl   111.05\n",
      "| epoch  13 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.10 | loss  4.73 | ppl   112.84\n",
      "| epoch  13 |  1200/ 2983 batches | lr 0.00 | ms/batch 27.13 | loss  4.74 | ppl   114.37\n",
      "| epoch  13 |  1400/ 2983 batches | lr 0.00 | ms/batch 27.31 | loss  4.74 | ppl   114.19\n",
      "| epoch  13 |  1600/ 2983 batches | lr 0.00 | ms/batch 27.04 | loss  4.81 | ppl   122.60\n",
      "| epoch  13 |  1800/ 2983 batches | lr 0.00 | ms/batch 27.23 | loss  4.74 | ppl   114.22\n",
      "| epoch  13 |  2000/ 2983 batches | lr 0.00 | ms/batch 27.17 | loss  4.80 | ppl   121.32\n",
      "| epoch  13 |  2200/ 2983 batches | lr 0.00 | ms/batch 26.97 | loss  4.68 | ppl   107.95\n",
      "| epoch  13 |  2400/ 2983 batches | lr 0.00 | ms/batch 27.05 | loss  4.70 | ppl   110.04\n",
      "| epoch  13 |  2600/ 2983 batches | lr 0.00 | ms/batch 26.97 | loss  4.77 | ppl   117.74\n",
      "| epoch  13 |  2800/ 2983 batches | lr 0.00 | ms/batch 27.08 | loss  4.70 | ppl   110.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 83.65s | valid loss  5.24 | valid ppl   189.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 2983 batches | lr 0.00 | ms/batch 27.18 | loss  4.71 | ppl   110.98\n",
      "| epoch  14 |   400/ 2983 batches | lr 0.00 | ms/batch 26.97 | loss  4.76 | ppl   116.94\n",
      "| epoch  14 |   600/ 2983 batches | lr 0.00 | ms/batch 27.04 | loss  4.64 | ppl   103.28\n",
      "| epoch  14 |   800/ 2983 batches | lr 0.00 | ms/batch 27.09 | loss  4.69 | ppl   109.34\n",
      "| epoch  14 |  1000/ 2983 batches | lr 0.00 | ms/batch 26.97 | loss  4.71 | ppl   111.47\n",
      "| epoch  14 |  1200/ 2983 batches | lr 0.00 | ms/batch 26.93 | loss  4.72 | ppl   112.64\n",
      "| epoch  14 |  1400/ 2983 batches | lr 0.00 | ms/batch 27.02 | loss  4.73 | ppl   113.10\n",
      "| epoch  14 |  1600/ 2983 batches | lr 0.00 | ms/batch 27.14 | loss  4.79 | ppl   120.84\n",
      "| epoch  14 |  1800/ 2983 batches | lr 0.00 | ms/batch 26.96 | loss  4.72 | ppl   112.02\n",
      "| epoch  14 |  2000/ 2983 batches | lr 0.00 | ms/batch 27.58 | loss  4.78 | ppl   119.65\n",
      "| epoch  14 |  2200/ 2983 batches | lr 0.00 | ms/batch 27.03 | loss  4.67 | ppl   106.31\n",
      "| epoch  14 |  2400/ 2983 batches | lr 0.00 | ms/batch 27.03 | loss  4.68 | ppl   108.05\n",
      "| epoch  14 |  2600/ 2983 batches | lr 0.00 | ms/batch 27.06 | loss  4.76 | ppl   116.53\n",
      "| epoch  14 |  2800/ 2983 batches | lr 0.00 | ms/batch 27.06 | loss  4.68 | ppl   107.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 83.54s | valid loss  5.20 | valid ppl   180.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 2983 batches | lr 0.00 | ms/batch 27.17 | loss  4.70 | ppl   109.57\n",
      "| epoch  15 |   400/ 2983 batches | lr 0.00 | ms/batch 27.06 | loss  4.75 | ppl   115.83\n",
      "| epoch  15 |   600/ 2983 batches | lr 0.00 | ms/batch 27.22 | loss  4.63 | ppl   102.01\n",
      "| epoch  15 |   800/ 2983 batches | lr 0.00 | ms/batch 27.05 | loss  4.67 | ppl   107.19\n",
      "| epoch  15 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.05 | loss  4.69 | ppl   109.39\n",
      "| epoch  15 |  1200/ 2983 batches | lr 0.00 | ms/batch 27.09 | loss  4.70 | ppl   110.01\n",
      "| epoch  15 |  1400/ 2983 batches | lr 0.00 | ms/batch 27.11 | loss  4.71 | ppl   110.83\n",
      "| epoch  15 |  1600/ 2983 batches | lr 0.00 | ms/batch 27.07 | loss  4.82 | ppl   123.67\n",
      "| epoch  15 |  1800/ 2983 batches | lr 0.00 | ms/batch 26.97 | loss  4.70 | ppl   109.86\n",
      "| epoch  15 |  2000/ 2983 batches | lr 0.00 | ms/batch 27.16 | loss  4.77 | ppl   117.46\n",
      "| epoch  15 |  2200/ 2983 batches | lr 0.00 | ms/batch 27.21 | loss  4.65 | ppl   105.01\n",
      "| epoch  15 |  2400/ 2983 batches | lr 0.00 | ms/batch 26.98 | loss  4.67 | ppl   107.12\n",
      "| epoch  15 |  2600/ 2983 batches | lr 0.00 | ms/batch 27.07 | loss  4.74 | ppl   114.64\n",
      "| epoch  15 |  2800/ 2983 batches | lr 0.00 | ms/batch 27.05 | loss  4.67 | ppl   106.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 83.64s | valid loss  5.20 | valid ppl   182.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 2983 batches | lr 0.00 | ms/batch 27.16 | loss  4.70 | ppl   109.98\n",
      "| epoch  16 |   400/ 2983 batches | lr 0.00 | ms/batch 27.13 | loss  4.73 | ppl   113.71\n",
      "| epoch  16 |   600/ 2983 batches | lr 0.00 | ms/batch 26.99 | loss  4.61 | ppl   100.73\n",
      "| epoch  16 |   800/ 2983 batches | lr 0.00 | ms/batch 27.24 | loss  4.66 | ppl   105.82\n",
      "| epoch  16 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.14 | loss  4.68 | ppl   107.94\n",
      "| epoch  16 |  1200/ 2983 batches | lr 0.00 | ms/batch 27.09 | loss  4.70 | ppl   109.61\n",
      "| epoch  16 |  1400/ 2983 batches | lr 0.00 | ms/batch 27.17 | loss  4.70 | ppl   109.51\n",
      "| epoch  16 |  1600/ 2983 batches | lr 0.00 | ms/batch 27.01 | loss  4.80 | ppl   121.49\n",
      "| epoch  16 |  1800/ 2983 batches | lr 0.00 | ms/batch 27.13 | loss  4.69 | ppl   108.86\n",
      "| epoch  16 |  2000/ 2983 batches | lr 0.00 | ms/batch 27.11 | loss  4.76 | ppl   116.51\n",
      "| epoch  16 |  2200/ 2983 batches | lr 0.00 | ms/batch 26.96 | loss  4.64 | ppl   103.10\n",
      "| epoch  16 |  2400/ 2983 batches | lr 0.00 | ms/batch 27.14 | loss  4.66 | ppl   105.27\n",
      "| epoch  16 |  2600/ 2983 batches | lr 0.00 | ms/batch 26.96 | loss  4.72 | ppl   112.71\n",
      "| epoch  16 |  2800/ 2983 batches | lr 0.00 | ms/batch 27.13 | loss  4.66 | ppl   105.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 83.69s | valid loss  5.20 | valid ppl   182.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 2983 batches | lr 0.00 | ms/batch 27.39 | loss  4.68 | ppl   107.96\n",
      "| epoch  17 |   400/ 2983 batches | lr 0.00 | ms/batch 27.08 | loss  4.72 | ppl   111.69\n",
      "| epoch  17 |   600/ 2983 batches | lr 0.00 | ms/batch 27.17 | loss  4.60 | ppl    99.32\n",
      "| epoch  17 |   800/ 2983 batches | lr 0.00 | ms/batch 26.91 | loss  4.64 | ppl   103.88\n",
      "| epoch  17 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.23 | loss  4.66 | ppl   105.60\n",
      "| epoch  17 |  1200/ 2983 batches | lr 0.00 | ms/batch 27.13 | loss  4.69 | ppl   108.36\n",
      "| epoch  17 |  1400/ 2983 batches | lr 0.00 | ms/batch 27.06 | loss  4.69 | ppl   108.97\n",
      "| epoch  17 |  1600/ 2983 batches | lr 0.00 | ms/batch 26.98 | loss  4.78 | ppl   119.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  17 |  1800/ 2983 batches | lr 0.00 | ms/batch 27.28 | loss  4.67 | ppl   107.05\n",
      "| epoch  17 |  2000/ 2983 batches | lr 0.00 | ms/batch 26.97 | loss  4.74 | ppl   114.45\n",
      "| epoch  17 |  2200/ 2983 batches | lr 0.00 | ms/batch 27.01 | loss  4.63 | ppl   102.40\n",
      "| epoch  17 |  2400/ 2983 batches | lr 0.00 | ms/batch 27.09 | loss  4.65 | ppl   104.42\n",
      "| epoch  17 |  2600/ 2983 batches | lr 0.00 | ms/batch 27.01 | loss  4.72 | ppl   111.61\n",
      "| epoch  17 |  2800/ 2983 batches | lr 0.00 | ms/batch 26.99 | loss  4.65 | ppl   105.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 83.59s | valid loss  5.22 | valid ppl   185.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 2983 batches | lr 0.00 | ms/batch 27.16 | loss  4.66 | ppl   105.56\n",
      "| epoch  18 |   400/ 2983 batches | lr 0.00 | ms/batch 27.04 | loss  4.71 | ppl   111.08\n",
      "| epoch  18 |   600/ 2983 batches | lr 0.00 | ms/batch 26.89 | loss  4.59 | ppl    98.31\n",
      "| epoch  18 |   800/ 2983 batches | lr 0.00 | ms/batch 29.87 | loss  4.64 | ppl   103.24\n",
      "| epoch  18 |  1000/ 2983 batches | lr 0.00 | ms/batch 29.50 | loss  4.65 | ppl   105.07\n",
      "| epoch  18 |  1200/ 2983 batches | lr 0.00 | ms/batch 28.37 | loss  4.68 | ppl   107.26\n",
      "| epoch  18 |  1400/ 2983 batches | lr 0.00 | ms/batch 28.87 | loss  4.68 | ppl   107.82\n",
      "| epoch  18 |  1600/ 2983 batches | lr 0.00 | ms/batch 29.07 | loss  4.76 | ppl   116.42\n",
      "| epoch  18 |  1800/ 2983 batches | lr 0.00 | ms/batch 28.20 | loss  4.67 | ppl   106.23\n",
      "| epoch  18 |  2000/ 2983 batches | lr 0.00 | ms/batch 29.73 | loss  4.73 | ppl   113.24\n",
      "| epoch  18 |  2200/ 2983 batches | lr 0.00 | ms/batch 27.92 | loss  4.62 | ppl   101.11\n",
      "| epoch  18 |  2400/ 2983 batches | lr 0.00 | ms/batch 27.22 | loss  4.64 | ppl   103.36\n",
      "| epoch  18 |  2600/ 2983 batches | lr 0.00 | ms/batch 27.88 | loss  4.71 | ppl   110.64\n",
      "| epoch  18 |  2800/ 2983 batches | lr 0.00 | ms/batch 27.46 | loss  4.65 | ppl   104.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 86.90s | valid loss  5.21 | valid ppl   183.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 2983 batches | lr 0.00 | ms/batch 27.82 | loss  4.66 | ppl   105.94\n",
      "| epoch  19 |   400/ 2983 batches | lr 0.00 | ms/batch 27.38 | loss  4.71 | ppl   110.84\n",
      "| epoch  19 |   600/ 2983 batches | lr 0.00 | ms/batch 28.06 | loss  4.58 | ppl    97.16\n",
      "| epoch  19 |   800/ 2983 batches | lr 0.00 | ms/batch 27.19 | loss  4.62 | ppl   101.68\n",
      "| epoch  19 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.42 | loss  4.65 | ppl   104.80\n",
      "| epoch  19 |  1200/ 2983 batches | lr 0.00 | ms/batch 27.96 | loss  4.67 | ppl   107.15\n",
      "| epoch  19 |  1400/ 2983 batches | lr 0.00 | ms/batch 27.56 | loss  4.67 | ppl   106.92\n",
      "| epoch  19 |  1600/ 2983 batches | lr 0.00 | ms/batch 27.90 | loss  4.74 | ppl   114.41\n",
      "| epoch  19 |  1800/ 2983 batches | lr 0.00 | ms/batch 27.32 | loss  4.65 | ppl   104.90\n",
      "| epoch  19 |  2000/ 2983 batches | lr 0.00 | ms/batch 27.60 | loss  4.71 | ppl   111.32\n",
      "| epoch  19 |  2200/ 2983 batches | lr 0.00 | ms/batch 26.76 | loss  4.60 | ppl    99.45\n",
      "| epoch  19 |  2400/ 2983 batches | lr 0.00 | ms/batch 26.87 | loss  4.62 | ppl   101.91\n",
      "| epoch  19 |  2600/ 2983 batches | lr 0.00 | ms/batch 26.79 | loss  4.68 | ppl   108.21\n",
      "| epoch  19 |  2800/ 2983 batches | lr 0.00 | ms/batch 26.85 | loss  4.63 | ppl   102.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 84.30s | valid loss  5.23 | valid ppl   187.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 2983 batches | lr 0.00 | ms/batch 26.94 | loss  4.65 | ppl   104.21\n",
      "| epoch  20 |   400/ 2983 batches | lr 0.00 | ms/batch 26.82 | loss  4.69 | ppl   108.32\n",
      "| epoch  20 |   600/ 2983 batches | lr 0.00 | ms/batch 26.84 | loss  4.57 | ppl    96.55\n",
      "| epoch  20 |   800/ 2983 batches | lr 0.00 | ms/batch 26.82 | loss  4.62 | ppl   101.58\n",
      "| epoch  20 |  1000/ 2983 batches | lr 0.00 | ms/batch 26.81 | loss  4.64 | ppl   103.59\n",
      "| epoch  20 |  1200/ 2983 batches | lr 0.00 | ms/batch 26.83 | loss  4.66 | ppl   105.81\n",
      "| epoch  20 |  1400/ 2983 batches | lr 0.00 | ms/batch 26.81 | loss  4.67 | ppl   106.87\n",
      "| epoch  20 |  1600/ 2983 batches | lr 0.00 | ms/batch 26.82 | loss  4.77 | ppl   118.01\n",
      "| epoch  20 |  1800/ 2983 batches | lr 0.00 | ms/batch 26.82 | loss  4.67 | ppl   106.32\n",
      "| epoch  20 |  2000/ 2983 batches | lr 0.00 | ms/batch 26.81 | loss  4.71 | ppl   111.19\n",
      "| epoch  20 |  2200/ 2983 batches | lr 0.00 | ms/batch 26.81 | loss  4.59 | ppl    98.82\n",
      "| epoch  20 |  2400/ 2983 batches | lr 0.00 | ms/batch 26.85 | loss  4.62 | ppl   101.40\n",
      "| epoch  20 |  2600/ 2983 batches | lr 0.00 | ms/batch 26.84 | loss  4.67 | ppl   106.88\n",
      "| epoch  20 |  2800/ 2983 batches | lr 0.00 | ms/batch 26.81 | loss  4.63 | ppl   102.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 82.73s | valid loss  5.23 | valid ppl   187.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/ 2983 batches | lr 0.00 | ms/batch 26.93 | loss  4.63 | ppl   102.73\n",
      "| epoch  21 |   400/ 2983 batches | lr 0.00 | ms/batch 26.83 | loss  4.68 | ppl   107.97\n",
      "| epoch  21 |   600/ 2983 batches | lr 0.00 | ms/batch 26.77 | loss  4.57 | ppl    96.17\n",
      "| epoch  21 |   800/ 2983 batches | lr 0.00 | ms/batch 26.79 | loss  4.61 | ppl   100.86\n",
      "| epoch  21 |  1000/ 2983 batches | lr 0.00 | ms/batch 26.78 | loss  4.64 | ppl   103.29\n",
      "| epoch  21 |  1200/ 2983 batches | lr 0.00 | ms/batch 26.84 | loss  4.65 | ppl   104.86\n",
      "| epoch  21 |  1400/ 2983 batches | lr 0.00 | ms/batch 28.59 | loss  4.66 | ppl   105.84\n",
      "| epoch  21 |  1600/ 2983 batches | lr 0.00 | ms/batch 30.14 | loss  4.72 | ppl   112.43\n",
      "| epoch  21 |  1800/ 2983 batches | lr 0.00 | ms/batch 28.73 | loss  4.64 | ppl   103.79\n",
      "| epoch  21 |  2000/ 2983 batches | lr 0.00 | ms/batch 28.70 | loss  4.69 | ppl   109.26\n",
      "| epoch  21 |  2200/ 2983 batches | lr 0.00 | ms/batch 30.18 | loss  4.58 | ppl    98.00\n",
      "| epoch  21 |  2400/ 2983 batches | lr 0.00 | ms/batch 29.98 | loss  4.61 | ppl   100.68\n",
      "| epoch  21 |  2600/ 2983 batches | lr 0.00 | ms/batch 28.31 | loss  4.66 | ppl   105.76\n",
      "| epoch  21 |  2800/ 2983 batches | lr 0.00 | ms/batch 27.99 | loss  4.62 | ppl   101.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 86.68s | valid loss  5.22 | valid ppl   184.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/ 2983 batches | lr 0.00 | ms/batch 28.59 | loss  4.63 | ppl   102.76\n",
      "| epoch  22 |   400/ 2983 batches | lr 0.00 | ms/batch 28.18 | loss  4.67 | ppl   106.28\n",
      "| epoch  22 |   600/ 2983 batches | lr 0.00 | ms/batch 27.90 | loss  4.55 | ppl    94.70\n",
      "| epoch  22 |   800/ 2983 batches | lr 0.00 | ms/batch 28.74 | loss  4.60 | ppl    99.14\n",
      "| epoch  22 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.74 | loss  4.62 | ppl   101.23\n",
      "| epoch  22 |  1200/ 2983 batches | lr 0.00 | ms/batch 28.64 | loss  4.65 | ppl   104.16\n",
      "| epoch  22 |  1400/ 2983 batches | lr 0.00 | ms/batch 28.94 | loss  4.65 | ppl   104.87\n",
      "| epoch  22 |  1600/ 2983 batches | lr 0.00 | ms/batch 29.10 | loss  4.72 | ppl   112.50\n",
      "| epoch  22 |  1800/ 2983 batches | lr 0.00 | ms/batch 28.14 | loss  4.64 | ppl   103.58\n",
      "| epoch  22 |  2000/ 2983 batches | lr 0.00 | ms/batch 27.04 | loss  4.69 | ppl   108.48\n",
      "| epoch  22 |  2200/ 2983 batches | lr 0.00 | ms/batch 27.05 | loss  4.57 | ppl    96.86\n",
      "| epoch  22 |  2400/ 2983 batches | lr 0.00 | ms/batch 27.05 | loss  4.60 | ppl    99.01\n",
      "| epoch  22 |  2600/ 2983 batches | lr 0.00 | ms/batch 27.18 | loss  4.66 | ppl   105.20\n",
      "| epoch  22 |  2800/ 2983 batches | lr 0.00 | ms/batch 27.11 | loss  4.60 | ppl    99.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 86.06s | valid loss  5.25 | valid ppl   189.75\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  23 |   200/ 2983 batches | lr 0.00 | ms/batch 27.31 | loss  4.63 | ppl   102.50\n",
      "| epoch  23 |   400/ 2983 batches | lr 0.00 | ms/batch 28.02 | loss  4.67 | ppl   106.80\n",
      "| epoch  23 |   600/ 2983 batches | lr 0.00 | ms/batch 28.65 | loss  4.54 | ppl    94.07\n",
      "| epoch  23 |   800/ 2983 batches | lr 0.00 | ms/batch 30.30 | loss  4.60 | ppl    99.32\n",
      "| epoch  23 |  1000/ 2983 batches | lr 0.00 | ms/batch 28.91 | loss  4.61 | ppl   100.79\n",
      "| epoch  23 |  1200/ 2983 batches | lr 0.00 | ms/batch 27.75 | loss  4.64 | ppl   103.86\n",
      "| epoch  23 |  1400/ 2983 batches | lr 0.00 | ms/batch 28.07 | loss  4.67 | ppl   106.22\n",
      "| epoch  23 |  1600/ 2983 batches | lr 0.00 | ms/batch 27.51 | loss  4.74 | ppl   114.88\n",
      "| epoch  23 |  1800/ 2983 batches | lr 0.00 | ms/batch 27.41 | loss  4.63 | ppl   102.74\n",
      "| epoch  23 |  2000/ 2983 batches | lr 0.00 | ms/batch 30.12 | loss  4.68 | ppl   107.78\n",
      "| epoch  23 |  2200/ 2983 batches | lr 0.00 | ms/batch 28.42 | loss  4.57 | ppl    96.68\n",
      "| epoch  23 |  2400/ 2983 batches | lr 0.00 | ms/batch 28.96 | loss  4.60 | ppl    99.30\n",
      "| epoch  23 |  2600/ 2983 batches | lr 0.00 | ms/batch 27.97 | loss  4.65 | ppl   104.60\n",
      "| epoch  23 |  2800/ 2983 batches | lr 0.00 | ms/batch 29.75 | loss  4.61 | ppl   100.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 88.05s | valid loss  5.25 | valid ppl   189.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/ 2983 batches | lr 0.00 | ms/batch 28.13 | loss  4.62 | ppl   101.29\n",
      "| epoch  24 |   400/ 2983 batches | lr 0.00 | ms/batch 27.23 | loss  4.65 | ppl   104.75\n",
      "| epoch  24 |   600/ 2983 batches | lr 0.00 | ms/batch 28.53 | loss  4.54 | ppl    93.38\n",
      "| epoch  24 |   800/ 2983 batches | lr 0.00 | ms/batch 28.28 | loss  4.60 | ppl    99.33\n",
      "| epoch  24 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.64 | loss  4.61 | ppl   100.47\n",
      "| epoch  24 |  1200/ 2983 batches | lr 0.00 | ms/batch 28.53 | loss  4.64 | ppl   103.39\n",
      "| epoch  24 |  1400/ 2983 batches | lr 0.00 | ms/batch 28.50 | loss  4.65 | ppl   104.28\n",
      "| epoch  24 |  1600/ 2983 batches | lr 0.00 | ms/batch 27.96 | loss  4.73 | ppl   112.77\n",
      "| epoch  24 |  1800/ 2983 batches | lr 0.00 | ms/batch 27.40 | loss  4.63 | ppl   102.11\n",
      "| epoch  24 |  2000/ 2983 batches | lr 0.00 | ms/batch 27.10 | loss  4.68 | ppl   107.48\n",
      "| epoch  24 |  2200/ 2983 batches | lr 0.00 | ms/batch 27.00 | loss  4.57 | ppl    96.10\n",
      "| epoch  24 |  2400/ 2983 batches | lr 0.00 | ms/batch 26.94 | loss  4.58 | ppl    97.89\n",
      "| epoch  24 |  2600/ 2983 batches | lr 0.00 | ms/batch 26.99 | loss  4.64 | ppl   103.58\n",
      "| epoch  24 |  2800/ 2983 batches | lr 0.00 | ms/batch 28.55 | loss  4.59 | ppl    98.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 86.05s | valid loss  5.24 | valid ppl   188.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/ 2983 batches | lr 0.00 | ms/batch 27.83 | loss  4.60 | ppl    99.71\n",
      "| epoch  25 |   400/ 2983 batches | lr 0.00 | ms/batch 27.49 | loss  4.65 | ppl   104.47\n",
      "| epoch  25 |   600/ 2983 batches | lr 0.00 | ms/batch 27.51 | loss  4.52 | ppl    91.99\n",
      "| epoch  25 |   800/ 2983 batches | lr 0.00 | ms/batch 28.88 | loss  4.59 | ppl    98.35\n",
      "| epoch  25 |  1000/ 2983 batches | lr 0.00 | ms/batch 29.42 | loss  4.61 | ppl   100.01\n",
      "| epoch  25 |  1200/ 2983 batches | lr 0.00 | ms/batch 28.41 | loss  4.63 | ppl   102.02\n",
      "| epoch  25 |  1400/ 2983 batches | lr 0.00 | ms/batch 27.12 | loss  4.63 | ppl   102.54\n",
      "| epoch  25 |  1600/ 2983 batches | lr 0.00 | ms/batch 26.97 | loss  4.72 | ppl   111.88\n",
      "| epoch  25 |  1800/ 2983 batches | lr 0.00 | ms/batch 27.21 | loss  4.62 | ppl   101.13\n",
      "| epoch  25 |  2000/ 2983 batches | lr 0.00 | ms/batch 27.67 | loss  4.66 | ppl   106.03\n",
      "| epoch  25 |  2200/ 2983 batches | lr 0.00 | ms/batch 31.65 | loss  4.55 | ppl    94.72\n",
      "| epoch  25 |  2400/ 2983 batches | lr 0.00 | ms/batch 29.86 | loss  4.57 | ppl    96.08\n",
      "| epoch  25 |  2600/ 2983 batches | lr 0.00 | ms/batch 29.39 | loss  4.63 | ppl   102.46\n",
      "| epoch  25 |  2800/ 2983 batches | lr 0.00 | ms/batch 28.85 | loss  4.59 | ppl    98.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 87.68s | valid loss  5.26 | valid ppl   191.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/ 2983 batches | lr 0.00 | ms/batch 27.97 | loss  4.59 | ppl    98.97\n",
      "| epoch  26 |   400/ 2983 batches | lr 0.00 | ms/batch 27.60 | loss  4.63 | ppl   102.98\n",
      "| epoch  26 |   600/ 2983 batches | lr 0.00 | ms/batch 27.69 | loss  4.52 | ppl    92.04\n",
      "| epoch  26 |   800/ 2983 batches | lr 0.00 | ms/batch 27.91 | loss  4.57 | ppl    96.80\n",
      "| epoch  26 |  1000/ 2983 batches | lr 0.00 | ms/batch 27.44 | loss  4.59 | ppl    98.47\n",
      "| epoch  26 |  1200/ 2983 batches | lr 0.00 | ms/batch 27.14 | loss  4.62 | ppl   101.93\n",
      "| epoch  26 |  1400/ 2983 batches | lr 0.00 | ms/batch 27.14 | loss  4.63 | ppl   102.40\n",
      "| epoch  26 |  1600/ 2983 batches | lr 0.00 | ms/batch 27.36 | loss  4.71 | ppl   111.23\n",
      "| epoch  26 |  1800/ 2983 batches | lr 0.00 | ms/batch 27.62 | loss  4.61 | ppl   100.13\n",
      "| epoch  26 |  2000/ 2983 batches | lr 0.00 | ms/batch 28.86 | loss  4.66 | ppl   105.49\n",
      "| epoch  26 |  2200/ 2983 batches | lr 0.00 | ms/batch 28.76 | loss  4.54 | ppl    93.81\n",
      "| epoch  26 |  2400/ 2983 batches | lr 0.00 | ms/batch 29.09 | loss  4.57 | ppl    96.32\n",
      "| epoch  26 |  2600/ 2983 batches | lr 0.00 | ms/batch 27.66 | loss  4.62 | ppl   101.69\n",
      "| epoch  26 |  2800/ 2983 batches | lr 0.00 | ms/batch 28.65 | loss  4.58 | ppl    97.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 86.23s | valid loss  5.27 | valid ppl   193.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/ 2983 batches | lr 0.00 | ms/batch 28.25 | loss  4.59 | ppl    98.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, args_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args_save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            ## Save State Dictionary\n",
    "            with open(args_save_state, 'wb') as f:\n",
    "                torch.save(model.state_dict(), f)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.11 | test ppl   165.73\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(args_save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to generate with this model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_data = '../data/wikitext-2'\n",
    "args_checkpoint = './model.pt'\n",
    "args_state_dict = './model_state.pt'\n",
    "args_outf = 'generated.txt'\n",
    "args_words = 1000\n",
    "args_seed = 1234\n",
    "args_temperature = 1.0\n",
    "args_log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args_seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_temperature < 1e-3:\n",
    "    print(\"args_temperature has to be greater or equal 1e-3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus(args_data)\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(args_checkpoint, 'rb') as f:\n",
    "#     model = torch.load(f).to(device)\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = rnn_model.RNNModel(args_model, ntokens, args_emsize, args_nhid, args_nlayers, args_dropout, args_tied).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden(1)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.2)\n",
       "  (encoder): Embedding(33278, 200)\n",
       "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
       "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 0/1000 words\n",
      "| Generated 100/1000 words\n",
      "| Generated 200/1000 words\n",
      "| Generated 300/1000 words\n",
      "| Generated 400/1000 words\n",
      "| Generated 500/1000 words\n",
      "| Generated 600/1000 words\n",
      "| Generated 700/1000 words\n",
      "| Generated 800/1000 words\n",
      "| Generated 900/1000 words\n"
     ]
    }
   ],
   "source": [
    "with open(args_outf, 'w') as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(args_words):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(args_temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % args_log_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i, args_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.weight',\n",
       "              tensor([[-0.0741, -0.0362,  0.0747,  ...,  0.0200,  0.0351, -0.0910],\n",
       "                      [ 0.0956,  0.0994, -0.0807,  ...,  0.0909,  0.0632, -0.0181],\n",
       "                      [ 0.0822, -0.0848, -0.0954,  ..., -0.0596, -0.0452,  0.0552],\n",
       "                      ...,\n",
       "                      [-0.0194, -0.0107,  0.0577,  ..., -0.0625, -0.0364, -0.0657],\n",
       "                      [-0.0690, -0.0153, -0.0525,  ...,  0.0393,  0.0737,  0.0737],\n",
       "                      [-0.0307,  0.0043,  0.0422,  ..., -0.0410, -0.0566, -0.0087]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_ih_l0',\n",
       "              tensor([[ 0.0594,  0.0334, -0.0147,  ...,  0.0363,  0.0219, -0.0698],\n",
       "                      [-0.0242,  0.0591,  0.0053,  ...,  0.0654,  0.0063,  0.0345],\n",
       "                      [-0.0500,  0.0657,  0.0313,  ..., -0.0396,  0.0262, -0.0506],\n",
       "                      ...,\n",
       "                      [ 0.0207,  0.0382, -0.0321,  ...,  0.0099,  0.0692, -0.0553],\n",
       "                      [-0.0659,  0.0022, -0.0577,  ..., -0.0109, -0.0555, -0.0470],\n",
       "                      [ 0.0381,  0.0314,  0.0440,  ...,  0.0557, -0.0035, -0.0575]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_hh_l0',\n",
       "              tensor([[-0.0134,  0.0657,  0.0613,  ...,  0.0234, -0.0374, -0.0276],\n",
       "                      [ 0.0529,  0.0299,  0.0012,  ..., -0.0665,  0.0257,  0.0689],\n",
       "                      [ 0.0585, -0.0239, -0.0486,  ...,  0.0242, -0.0036, -0.0547],\n",
       "                      ...,\n",
       "                      [-0.0508, -0.0048,  0.0370,  ..., -0.0665,  0.0239, -0.0095],\n",
       "                      [ 0.0336,  0.0293, -0.0008,  ..., -0.0206, -0.0154, -0.0397],\n",
       "                      [ 0.0135,  0.0588, -0.0506,  ..., -0.0528, -0.0345, -0.0003]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_ih_l0',\n",
       "              tensor([ 0.0103, -0.0329,  0.0235, -0.0250, -0.0484,  0.0677, -0.0216,  0.0199,\n",
       "                       0.0197,  0.0021, -0.0269,  0.0216,  0.0501,  0.0683,  0.0196,  0.0128,\n",
       "                      -0.0020,  0.0521, -0.0672, -0.0090, -0.0313,  0.0371,  0.0449, -0.0288,\n",
       "                       0.0589,  0.0074,  0.0458,  0.0077, -0.0537,  0.0284, -0.0283, -0.0003,\n",
       "                       0.0564,  0.0122,  0.0418,  0.0482,  0.0409, -0.0234, -0.0062,  0.0366,\n",
       "                      -0.0593,  0.0618, -0.0357,  0.0596, -0.0693,  0.0325, -0.0651, -0.0663,\n",
       "                       0.0117, -0.0689,  0.0273,  0.0038,  0.0086,  0.0492,  0.0293,  0.0102,\n",
       "                      -0.0554,  0.0009,  0.0588, -0.0503, -0.0011, -0.0086, -0.0079,  0.0706,\n",
       "                       0.0329, -0.0074,  0.0596, -0.0409,  0.0591,  0.0420,  0.0494,  0.0401,\n",
       "                      -0.0270, -0.0601, -0.0110, -0.0022, -0.0260,  0.0689, -0.0068,  0.0594,\n",
       "                       0.0225, -0.0545, -0.0581, -0.0519,  0.0486,  0.0635, -0.0161, -0.0198,\n",
       "                       0.0262,  0.0190, -0.0486,  0.0517, -0.0686, -0.0550, -0.0421,  0.0223,\n",
       "                       0.0020, -0.0408, -0.0323, -0.0510,  0.0079,  0.0434, -0.0333, -0.0675,\n",
       "                       0.0224, -0.0192,  0.0487,  0.0667,  0.0601,  0.0533,  0.0694,  0.0639,\n",
       "                      -0.0106, -0.0669,  0.0210,  0.0347,  0.0163, -0.0313, -0.0577, -0.0011,\n",
       "                      -0.0364, -0.0525, -0.0253, -0.0040, -0.0617, -0.0648,  0.0201, -0.0230,\n",
       "                      -0.0461, -0.0379, -0.0411, -0.0541,  0.0510,  0.0667,  0.0689, -0.0344,\n",
       "                      -0.0371, -0.0111,  0.0020,  0.0432,  0.0159, -0.0386, -0.0039, -0.0585,\n",
       "                      -0.0315,  0.0372, -0.0314, -0.0094, -0.0139, -0.0702,  0.0595, -0.0257,\n",
       "                      -0.0262, -0.0485,  0.0595, -0.0356,  0.0595,  0.0315,  0.0267, -0.0414,\n",
       "                      -0.0533, -0.0205, -0.0027, -0.0614, -0.0565,  0.0626,  0.0675, -0.0146,\n",
       "                      -0.0565,  0.0508,  0.0184, -0.0675,  0.0523, -0.0197,  0.0332, -0.0669,\n",
       "                       0.0229,  0.0625,  0.0106, -0.0279, -0.0201,  0.0202,  0.0462, -0.0175,\n",
       "                      -0.0069,  0.0480, -0.0595, -0.0592,  0.0621, -0.0568,  0.0547, -0.0403,\n",
       "                       0.0165, -0.0314,  0.0229,  0.0014, -0.0540,  0.0661, -0.0643, -0.0707,\n",
       "                      -0.0394,  0.0121,  0.0624, -0.0018, -0.0315,  0.0137,  0.0516, -0.0441,\n",
       "                      -0.0225,  0.0547, -0.0328, -0.0267,  0.0195, -0.0224, -0.0176,  0.0271,\n",
       "                      -0.0210,  0.0091,  0.0001,  0.0694,  0.0441,  0.0107,  0.0511, -0.0513,\n",
       "                       0.0117,  0.0087, -0.0706,  0.0233, -0.0640,  0.0347, -0.0299,  0.0012,\n",
       "                      -0.0531, -0.0029, -0.0472,  0.0537,  0.0387, -0.0602,  0.0054, -0.0222,\n",
       "                       0.0116,  0.0115,  0.0408,  0.0431, -0.0400, -0.0154,  0.0512,  0.0553,\n",
       "                      -0.0129,  0.0435, -0.0451, -0.0064, -0.0269,  0.0066,  0.0646, -0.0529,\n",
       "                      -0.0186, -0.0292, -0.0126, -0.0482,  0.0635,  0.0597,  0.0246,  0.0636,\n",
       "                       0.0243,  0.0455, -0.0104,  0.0647,  0.0573,  0.0049, -0.0576,  0.0369,\n",
       "                      -0.0172,  0.0022,  0.0343, -0.0352,  0.0087, -0.0404, -0.0222, -0.0351,\n",
       "                       0.0441,  0.0275,  0.0380,  0.0198,  0.0580, -0.0636,  0.0547,  0.0485,\n",
       "                       0.0270,  0.0306, -0.0416,  0.0489, -0.0131,  0.0632, -0.0663,  0.0243,\n",
       "                       0.0327, -0.0492, -0.0286, -0.0493, -0.0606, -0.0470, -0.0078,  0.0402,\n",
       "                       0.0341, -0.0140,  0.0102,  0.0591, -0.0637, -0.0003,  0.0249,  0.0612,\n",
       "                       0.0689, -0.0217, -0.0380, -0.0378,  0.0581, -0.0624, -0.0278,  0.0544,\n",
       "                       0.0118,  0.0686,  0.0083, -0.0096,  0.0541, -0.0306,  0.0623, -0.0049,\n",
       "                      -0.0532, -0.0538, -0.0001, -0.0198,  0.0670, -0.0665, -0.0601,  0.0671,\n",
       "                       0.0529, -0.0506, -0.0501, -0.0494, -0.0372, -0.0212, -0.0390, -0.0119,\n",
       "                      -0.0458, -0.0523,  0.0305,  0.0284, -0.0477,  0.0081,  0.0150, -0.0455,\n",
       "                      -0.0019,  0.0269,  0.0106,  0.0115, -0.0495, -0.0590,  0.0508, -0.0188,\n",
       "                      -0.0622,  0.0340, -0.0662, -0.0441, -0.0281, -0.0353, -0.0589, -0.0707,\n",
       "                       0.0681,  0.0666,  0.0143,  0.0163, -0.0560, -0.0494,  0.0053,  0.0625,\n",
       "                      -0.0368,  0.0628, -0.0607, -0.0366, -0.0600, -0.0200, -0.0417, -0.0104,\n",
       "                      -0.0339,  0.0251,  0.0407,  0.0375, -0.0435,  0.0675, -0.0163,  0.0662,\n",
       "                      -0.0396,  0.0245,  0.0050,  0.0691,  0.0079,  0.0160, -0.0468, -0.0497,\n",
       "                      -0.0211,  0.0520,  0.0150, -0.0328, -0.0445,  0.0166,  0.0524,  0.0676,\n",
       "                       0.0666,  0.0210,  0.0497,  0.0432,  0.0232,  0.0688,  0.0261,  0.0049,\n",
       "                      -0.0420,  0.0040,  0.0298, -0.0641,  0.0241,  0.0286,  0.0061,  0.0638,\n",
       "                      -0.0386,  0.0079, -0.0080, -0.0272,  0.0342, -0.0016, -0.0657, -0.0363,\n",
       "                      -0.0361,  0.0274, -0.0241,  0.0474,  0.0481,  0.0390,  0.0442, -0.0213,\n",
       "                      -0.0237,  0.0309,  0.0675, -0.0054,  0.0371, -0.0500,  0.0296, -0.0178,\n",
       "                       0.0244,  0.0374,  0.0217, -0.0299,  0.0019, -0.0686, -0.0641,  0.0434,\n",
       "                      -0.0394,  0.0388, -0.0477,  0.0647,  0.0133,  0.0634,  0.0193, -0.0536,\n",
       "                      -0.0178, -0.0396,  0.0085,  0.0293, -0.0363, -0.0177,  0.0167, -0.0530,\n",
       "                      -0.0602, -0.0237,  0.0665, -0.0214, -0.0654, -0.0013, -0.0694,  0.0408,\n",
       "                       0.0427, -0.0167, -0.0657,  0.0397, -0.0111, -0.0391,  0.0389,  0.0349,\n",
       "                       0.0593,  0.0515, -0.0086, -0.0035,  0.0608,  0.0108,  0.0549, -0.0133,\n",
       "                       0.0558,  0.0180,  0.0500,  0.0308,  0.0496,  0.0696, -0.0344, -0.0301,\n",
       "                      -0.0337, -0.0633,  0.0459,  0.0149, -0.0115,  0.0180,  0.0514,  0.0499,\n",
       "                      -0.0686,  0.0205, -0.0161,  0.0120,  0.0357, -0.0129, -0.0582,  0.0199,\n",
       "                       0.0468, -0.0233, -0.0432, -0.0137, -0.0279, -0.0144,  0.0123, -0.0535,\n",
       "                       0.0009, -0.0245,  0.0130, -0.0107,  0.0699, -0.0373,  0.0501,  0.0026,\n",
       "                      -0.0168, -0.0469,  0.0489, -0.0391, -0.0454,  0.0663,  0.0565,  0.0598,\n",
       "                       0.0297,  0.0078,  0.0170,  0.0444,  0.0558,  0.0095,  0.0562, -0.0389,\n",
       "                      -0.0530,  0.0419,  0.0089,  0.0486, -0.0643, -0.0003, -0.0546, -0.0526,\n",
       "                       0.0695,  0.0302,  0.0083,  0.0358, -0.0216,  0.0139, -0.0642,  0.0188,\n",
       "                      -0.0700,  0.0555,  0.0171, -0.0074,  0.0133,  0.0480,  0.0073, -0.0508,\n",
       "                      -0.0014, -0.0277,  0.0342, -0.0331, -0.0238,  0.0498,  0.0670,  0.0011,\n",
       "                      -0.0570,  0.0274, -0.0035, -0.0001, -0.0441, -0.0237, -0.0042, -0.0248,\n",
       "                       0.0495,  0.0683, -0.0026,  0.0334, -0.0311,  0.0097,  0.0393, -0.0376,\n",
       "                      -0.0660,  0.0467, -0.0109,  0.0001, -0.0397,  0.0074, -0.0381,  0.0697,\n",
       "                       0.0079,  0.0188, -0.0161,  0.0398, -0.0142,  0.0577,  0.0608, -0.0060,\n",
       "                       0.0216,  0.0391, -0.0442,  0.0136, -0.0429,  0.0198, -0.0014, -0.0197,\n",
       "                       0.0248, -0.0078, -0.0698, -0.0010,  0.0277, -0.0012,  0.0647,  0.0339,\n",
       "                      -0.0422, -0.0074, -0.0392, -0.0518, -0.0010, -0.0265,  0.0514,  0.0057,\n",
       "                      -0.0690,  0.0691,  0.0680, -0.0673,  0.0508, -0.0521, -0.0453,  0.0282,\n",
       "                       0.0398, -0.0228, -0.0614,  0.0464, -0.0415,  0.0561,  0.0186,  0.0116,\n",
       "                      -0.0586,  0.0141, -0.0200,  0.0249, -0.0475,  0.0026,  0.0459,  0.0615,\n",
       "                       0.0035, -0.0291,  0.0240,  0.0488, -0.0690,  0.0122, -0.0415, -0.0172,\n",
       "                      -0.0528,  0.0131, -0.0399, -0.0018, -0.0464, -0.0289, -0.0062, -0.0186,\n",
       "                      -0.0164, -0.0629,  0.0543,  0.0112,  0.0131,  0.0058,  0.0052, -0.0646,\n",
       "                       0.0707,  0.0676, -0.0111,  0.0599,  0.0614,  0.0651,  0.0318,  0.0456,\n",
       "                      -0.0491,  0.0433, -0.0098, -0.0185, -0.0029, -0.0410, -0.0294, -0.0113,\n",
       "                       0.0011,  0.0153,  0.0498, -0.0086,  0.0368, -0.0086,  0.0696,  0.0554,\n",
       "                      -0.0520,  0.0471, -0.0596, -0.0329, -0.0676, -0.0029, -0.0411,  0.0121,\n",
       "                       0.0195,  0.0087, -0.0492, -0.0067,  0.0335, -0.0511, -0.0169, -0.0688,\n",
       "                       0.0110,  0.0123,  0.0241, -0.0650,  0.0163,  0.0504, -0.0067,  0.0431,\n",
       "                      -0.0505,  0.0396,  0.0370, -0.0351,  0.0531,  0.0004,  0.0030,  0.0688,\n",
       "                      -0.0136,  0.0046,  0.0629, -0.0645, -0.0358,  0.0393,  0.0497, -0.0459,\n",
       "                       0.0583, -0.0536, -0.0400, -0.0280, -0.0455, -0.0355, -0.0603,  0.0405,\n",
       "                      -0.0280,  0.0174,  0.0640,  0.0079,  0.0098, -0.0290, -0.0430,  0.0172,\n",
       "                       0.0223, -0.0286,  0.0413, -0.0240, -0.0351,  0.0099,  0.0181, -0.0324,\n",
       "                      -0.0282,  0.0103,  0.0593,  0.0661, -0.0638,  0.0039,  0.0016,  0.0260,\n",
       "                       0.0629,  0.0142,  0.0206,  0.0053, -0.0090, -0.0465, -0.0001,  0.0366,\n",
       "                       0.0705, -0.0059,  0.0328, -0.0492, -0.0617, -0.0435, -0.0426,  0.0173],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_hh_l0',\n",
       "              tensor([-0.0062,  0.0041, -0.0561,  0.0234, -0.0297,  0.0350,  0.0062, -0.0308,\n",
       "                      -0.0452,  0.0525,  0.0472, -0.0575, -0.0349,  0.0381,  0.0124,  0.0420,\n",
       "                       0.0453,  0.0335,  0.0495, -0.0220,  0.0224, -0.0301, -0.0115,  0.0139,\n",
       "                       0.0318,  0.0043, -0.0031, -0.0112,  0.0186, -0.0273, -0.0637,  0.0331,\n",
       "                       0.0606,  0.0272, -0.0406, -0.0152,  0.0215, -0.0332,  0.0497,  0.0120,\n",
       "                      -0.0093, -0.0413, -0.0184,  0.0289,  0.0499,  0.0129, -0.0009,  0.0446,\n",
       "                       0.0053,  0.0515,  0.0406,  0.0136,  0.0300,  0.0114, -0.0165, -0.0658,\n",
       "                      -0.0387, -0.0105, -0.0170,  0.0443, -0.0613,  0.0396, -0.0320, -0.0555,\n",
       "                       0.0563,  0.0630,  0.0641,  0.0513, -0.0540,  0.0306, -0.0384,  0.0348,\n",
       "                       0.0683,  0.0121, -0.0135,  0.0420,  0.0181, -0.0332, -0.0075,  0.0367,\n",
       "                      -0.0573, -0.0660,  0.0058,  0.0216, -0.0342,  0.0027,  0.0457,  0.0413,\n",
       "                      -0.0356,  0.0287,  0.0549, -0.0392,  0.0206, -0.0171,  0.0636,  0.0227,\n",
       "                       0.0017,  0.0323,  0.0450, -0.0456,  0.0349,  0.0155,  0.0300, -0.0110,\n",
       "                       0.0357,  0.0133,  0.0127,  0.0500, -0.0651, -0.0160,  0.0292, -0.0128,\n",
       "                       0.0580,  0.0183, -0.0075,  0.0565,  0.0254,  0.0683,  0.0679,  0.0288,\n",
       "                      -0.0065,  0.0306,  0.0613, -0.0181, -0.0572,  0.0543,  0.0005,  0.0235,\n",
       "                      -0.0375,  0.0257,  0.0147, -0.0299, -0.0586, -0.0412,  0.0626, -0.0448,\n",
       "                       0.0544,  0.0446, -0.0501,  0.0704, -0.0227,  0.0498,  0.0627,  0.0323,\n",
       "                      -0.0417, -0.0423, -0.0478,  0.0494,  0.0089,  0.0058, -0.0100,  0.0670,\n",
       "                       0.0361,  0.0572,  0.0287, -0.0092,  0.0082,  0.0462, -0.0150,  0.0388,\n",
       "                       0.0602, -0.0707,  0.0620, -0.0498,  0.0478, -0.0508,  0.0482,  0.0131,\n",
       "                      -0.0593, -0.0382,  0.0024, -0.0645, -0.0177, -0.0378,  0.0552, -0.0428,\n",
       "                       0.0376,  0.0448,  0.0362,  0.0162, -0.0064,  0.0593,  0.0067, -0.0094,\n",
       "                      -0.0565, -0.0660,  0.0536, -0.0166, -0.0377,  0.0066,  0.0149, -0.0414,\n",
       "                      -0.0332,  0.0674,  0.0630, -0.0109,  0.0024,  0.0687, -0.0594,  0.0024,\n",
       "                       0.0189, -0.0243, -0.0485, -0.0450,  0.0613,  0.0285, -0.0481, -0.0551,\n",
       "                       0.0435,  0.0106, -0.0639,  0.0620, -0.0546,  0.0429,  0.0043, -0.0365,\n",
       "                      -0.0660, -0.0593, -0.0690, -0.0450, -0.0468,  0.0337,  0.0343, -0.0472,\n",
       "                       0.0585,  0.0245, -0.0467,  0.0173, -0.0058, -0.0566,  0.0082, -0.0354,\n",
       "                      -0.0358,  0.0533,  0.0250, -0.0216, -0.0664, -0.0295, -0.0402, -0.0010,\n",
       "                      -0.0311, -0.0385,  0.0193, -0.0423,  0.0317,  0.0373, -0.0437, -0.0180,\n",
       "                       0.0078,  0.0172,  0.0145, -0.0640,  0.0467,  0.0056, -0.0429,  0.0584,\n",
       "                       0.0245, -0.0363,  0.0642,  0.0421, -0.0143, -0.0272,  0.0450, -0.0073,\n",
       "                       0.0127, -0.0561, -0.0330,  0.0181,  0.0187, -0.0564,  0.0523, -0.0506,\n",
       "                       0.0273, -0.0268,  0.0379,  0.0091, -0.0101, -0.0659, -0.0218, -0.0144,\n",
       "                      -0.0533, -0.0617,  0.0307,  0.0405,  0.0174,  0.0101,  0.0154,  0.0044,\n",
       "                      -0.0112, -0.0054, -0.0419,  0.0539, -0.0656, -0.0173,  0.0075,  0.0195,\n",
       "                       0.0290,  0.0578,  0.0480, -0.0255,  0.0243, -0.0197,  0.0246, -0.0628,\n",
       "                      -0.0191,  0.0408, -0.0165, -0.0328, -0.0112, -0.0146, -0.0311,  0.0408,\n",
       "                      -0.0484,  0.0205, -0.0613, -0.0096, -0.0176,  0.0372, -0.0103, -0.0348,\n",
       "                       0.0286,  0.0541, -0.0683, -0.0637,  0.0256,  0.0233,  0.0540, -0.0593,\n",
       "                       0.0270,  0.0355,  0.0464, -0.0224, -0.0660, -0.0287, -0.0171, -0.0279,\n",
       "                       0.0027, -0.0184,  0.0685,  0.0622,  0.0269,  0.0226, -0.0667,  0.0196,\n",
       "                      -0.0067,  0.0447, -0.0358, -0.0234, -0.0123,  0.0348, -0.0366,  0.0292,\n",
       "                       0.0619, -0.0632,  0.0692, -0.0678, -0.0390,  0.0531,  0.0058, -0.0603,\n",
       "                      -0.0270,  0.0019,  0.0441, -0.0355,  0.0203, -0.0228,  0.0123, -0.0684,\n",
       "                       0.0600,  0.0681,  0.0441,  0.0368,  0.0341, -0.0061,  0.0539, -0.0394,\n",
       "                       0.0180, -0.0115, -0.0673, -0.0213,  0.0367, -0.0328, -0.0176,  0.0542,\n",
       "                      -0.0453, -0.0120,  0.0423,  0.0685, -0.0198,  0.0096,  0.0703,  0.0047,\n",
       "                      -0.0478,  0.0703,  0.0119,  0.0546, -0.0437,  0.0685, -0.0083,  0.0519,\n",
       "                       0.0655,  0.0241, -0.0318,  0.0402, -0.0085,  0.0248,  0.0118, -0.0381,\n",
       "                      -0.0341, -0.0342, -0.0450, -0.0259, -0.0608, -0.0014, -0.0520, -0.0682,\n",
       "                       0.0533,  0.0131, -0.0644, -0.0209,  0.0023, -0.0370,  0.0508, -0.0663,\n",
       "                      -0.0452,  0.0413, -0.0511,  0.0497, -0.0254, -0.0324,  0.0579, -0.0006,\n",
       "                      -0.0656, -0.0293,  0.0404,  0.0483, -0.0056,  0.0189,  0.0089, -0.0048,\n",
       "                       0.0637, -0.0337, -0.0662, -0.0173,  0.0432, -0.0448, -0.0133, -0.0149,\n",
       "                       0.0303,  0.0500,  0.0272,  0.0551,  0.0217, -0.0009,  0.0107,  0.0496,\n",
       "                      -0.0332,  0.0087, -0.0664,  0.0664,  0.0643, -0.0636,  0.0326,  0.0593,\n",
       "                       0.0168, -0.0396,  0.0358, -0.0508,  0.0262, -0.0323,  0.0473, -0.0058,\n",
       "                       0.0646,  0.0132,  0.0106,  0.0467, -0.0539,  0.0123, -0.0471,  0.0175,\n",
       "                       0.0361,  0.0424, -0.0610, -0.0263, -0.0529, -0.0697,  0.0579, -0.0121,\n",
       "                       0.0232, -0.0587,  0.0503, -0.0453, -0.0616, -0.0342,  0.0447, -0.0250,\n",
       "                       0.0064,  0.0526, -0.0309, -0.0685, -0.0289,  0.0404,  0.0172,  0.0177,\n",
       "                      -0.0502, -0.0244, -0.0351, -0.0366,  0.0453, -0.0618,  0.0063, -0.0308,\n",
       "                       0.0055, -0.0036,  0.0144,  0.0512,  0.0364, -0.0291, -0.0115, -0.0576,\n",
       "                       0.0652,  0.0574,  0.0078,  0.0159, -0.0101, -0.0519, -0.0164, -0.0605,\n",
       "                      -0.0011, -0.0465,  0.0081,  0.0067,  0.0582, -0.0231, -0.0228,  0.0014,\n",
       "                       0.0704,  0.0487,  0.0414,  0.0167,  0.0118, -0.0075,  0.0312,  0.0237,\n",
       "                       0.0416,  0.0641, -0.0210,  0.0041, -0.0705,  0.0595,  0.0316, -0.0174,\n",
       "                      -0.0524,  0.0085, -0.0193, -0.0524, -0.0346,  0.0678, -0.0035, -0.0371,\n",
       "                       0.0063, -0.0004, -0.0356,  0.0063, -0.0042,  0.0353,  0.0400, -0.0517,\n",
       "                      -0.0561,  0.0544,  0.0472, -0.0176,  0.0688, -0.0029, -0.0519, -0.0551,\n",
       "                      -0.0411,  0.0437,  0.0042,  0.0530, -0.0203,  0.0503,  0.0262,  0.0134,\n",
       "                       0.0177, -0.0268,  0.0556,  0.0531,  0.0182, -0.0513, -0.0166,  0.0358,\n",
       "                       0.0358, -0.0021, -0.0515,  0.0063, -0.0448, -0.0680,  0.0481, -0.0349,\n",
       "                      -0.0244,  0.0679,  0.0617, -0.0353,  0.0057,  0.0383, -0.0095,  0.0104,\n",
       "                      -0.0436, -0.0394, -0.0054,  0.0113,  0.0016, -0.0621,  0.0281, -0.0064,\n",
       "                       0.0555,  0.0031,  0.0152,  0.0692, -0.0454,  0.0515,  0.0311,  0.0072,\n",
       "                       0.0051,  0.0103,  0.0490, -0.0634, -0.0641,  0.0350,  0.0163,  0.0392,\n",
       "                       0.0391,  0.0566,  0.0104, -0.0473,  0.0319,  0.0133, -0.0473,  0.0301,\n",
       "                      -0.0665,  0.0213, -0.0510,  0.0577,  0.0640,  0.0129, -0.0207, -0.0062,\n",
       "                       0.0498,  0.0382,  0.0254,  0.0233,  0.0092,  0.0055, -0.0102, -0.0465,\n",
       "                      -0.0017,  0.0281,  0.0040,  0.0402,  0.0643,  0.0208,  0.0442,  0.0505,\n",
       "                      -0.0425,  0.0078,  0.0409,  0.0186,  0.0117, -0.0449, -0.0126,  0.0006,\n",
       "                       0.0607,  0.0466,  0.0088, -0.0425, -0.0076, -0.0236,  0.0487,  0.0455,\n",
       "                       0.0093, -0.0505,  0.0488, -0.0636, -0.0145,  0.0340,  0.0111,  0.0060,\n",
       "                       0.0113, -0.0518,  0.0630,  0.0380,  0.0110, -0.0159, -0.0568, -0.0427,\n",
       "                       0.0261, -0.0364, -0.0404,  0.0269,  0.0644, -0.0517,  0.0435,  0.0387,\n",
       "                       0.0090, -0.0525,  0.0275,  0.0664,  0.0293, -0.0087,  0.0195,  0.0586,\n",
       "                       0.0522,  0.0153, -0.0167,  0.0032,  0.0285, -0.0106, -0.0021, -0.0013,\n",
       "                       0.0233, -0.0036, -0.0301, -0.0660, -0.0594,  0.0211, -0.0476, -0.0526,\n",
       "                       0.0500,  0.0269, -0.0515, -0.0365, -0.0585, -0.0620,  0.0003, -0.0274,\n",
       "                       0.0325,  0.0258,  0.0016, -0.0264, -0.0004, -0.0164,  0.0293, -0.0323,\n",
       "                       0.0429,  0.0190, -0.0188,  0.0528, -0.0699, -0.0181,  0.0238, -0.0369,\n",
       "                       0.0265,  0.0113,  0.0410,  0.0090,  0.0497,  0.0594,  0.0180, -0.0469,\n",
       "                       0.0238, -0.0242,  0.0344,  0.0540, -0.0279, -0.0380, -0.0571,  0.0352,\n",
       "                       0.0541, -0.0144, -0.0391, -0.0044,  0.0583,  0.0570, -0.0629, -0.0262,\n",
       "                      -0.0442,  0.0311,  0.0632,  0.0436, -0.0314,  0.0090,  0.0097, -0.0126,\n",
       "                       0.0117,  0.0322, -0.0255, -0.0443,  0.0047,  0.0671, -0.0065,  0.0558,\n",
       "                       0.0407, -0.0160,  0.0355,  0.0298,  0.0609,  0.0477,  0.0462,  0.0019],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_ih_l1',\n",
       "              tensor([[-0.0213,  0.0536, -0.0414,  ..., -0.0686, -0.0315,  0.0520],\n",
       "                      [-0.0602,  0.0065, -0.0186,  ..., -0.0093, -0.0631, -0.0323],\n",
       "                      [-0.0288, -0.0280, -0.0247,  ..., -0.0309, -0.0372,  0.0677],\n",
       "                      ...,\n",
       "                      [-0.0047, -0.0448, -0.0054,  ...,  0.0345, -0.0293, -0.0333],\n",
       "                      [-0.0473,  0.0526, -0.0054,  ...,  0.0270, -0.0422, -0.0212],\n",
       "                      [-0.0200,  0.0298, -0.0426,  ...,  0.0099,  0.0402, -0.0213]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_hh_l1',\n",
       "              tensor([[ 0.0430, -0.0484,  0.0618,  ..., -0.0433,  0.0473, -0.0105],\n",
       "                      [ 0.0684,  0.0663, -0.0315,  ...,  0.0317, -0.0202, -0.0471],\n",
       "                      [ 0.0194,  0.0113, -0.0504,  ...,  0.0453, -0.0180, -0.0212],\n",
       "                      ...,\n",
       "                      [-0.0233,  0.0227,  0.0399,  ...,  0.0329,  0.0035, -0.0209],\n",
       "                      [-0.0216, -0.0510, -0.0461,  ...,  0.0238, -0.0203,  0.0039],\n",
       "                      [ 0.0329, -0.0018,  0.0668,  ..., -0.0433,  0.0096, -0.0410]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_ih_l1',\n",
       "              tensor([ 0.0116, -0.0630,  0.0448,  0.0042, -0.0317, -0.0417,  0.0142, -0.0546,\n",
       "                      -0.0598, -0.0169,  0.0704, -0.0652,  0.0406,  0.0383,  0.0271,  0.0236,\n",
       "                       0.0396, -0.0109,  0.0283,  0.0272, -0.0446, -0.0170, -0.0211, -0.0026,\n",
       "                       0.0438,  0.0012, -0.0627,  0.0264, -0.0327, -0.0102, -0.0071,  0.0322,\n",
       "                       0.0328, -0.0580,  0.0041, -0.0447,  0.0449,  0.0084,  0.0678,  0.0346,\n",
       "                       0.0645,  0.0645,  0.0572, -0.0588, -0.0276, -0.0234, -0.0535, -0.0586,\n",
       "                      -0.0285, -0.0161,  0.0449, -0.0455,  0.0318, -0.0332,  0.0294,  0.0110,\n",
       "                      -0.0202,  0.0308, -0.0047, -0.0285,  0.0338,  0.0438, -0.0617, -0.0527,\n",
       "                       0.0032,  0.0238,  0.0596,  0.0695, -0.0263,  0.0473,  0.0293,  0.0210,\n",
       "                       0.0520,  0.0307, -0.0146,  0.0381, -0.0541, -0.0339, -0.0579, -0.0401,\n",
       "                      -0.0518, -0.0007, -0.0585, -0.0614,  0.0290, -0.0693,  0.0558, -0.0566,\n",
       "                      -0.0643,  0.0594, -0.0575, -0.0376,  0.0112, -0.0040, -0.0472,  0.0519,\n",
       "                      -0.0648, -0.0191, -0.0106,  0.0022,  0.0033,  0.0654,  0.0420, -0.0595,\n",
       "                      -0.0522, -0.0495,  0.0390, -0.0225,  0.0028, -0.0358, -0.0065,  0.0143,\n",
       "                      -0.0478, -0.0512, -0.0683, -0.0496,  0.0243,  0.0224, -0.0134, -0.0538,\n",
       "                      -0.0094, -0.0431, -0.0171,  0.0391,  0.0395,  0.0138, -0.0473, -0.0660,\n",
       "                      -0.0343,  0.0318, -0.0439,  0.0440,  0.0663, -0.0437,  0.0032,  0.0385,\n",
       "                      -0.0506,  0.0602, -0.0376, -0.0696, -0.0331, -0.0646,  0.0327, -0.0171,\n",
       "                      -0.0615,  0.0684, -0.0363,  0.0219, -0.0512,  0.0256, -0.0252, -0.0006,\n",
       "                       0.0208, -0.0569, -0.0321, -0.0188,  0.0318,  0.0330, -0.0279,  0.0485,\n",
       "                      -0.0209, -0.0236,  0.0547,  0.0522,  0.0063,  0.0072, -0.0619, -0.0554,\n",
       "                      -0.0471,  0.0512,  0.0243,  0.0545,  0.0289,  0.0076, -0.0509,  0.0222,\n",
       "                      -0.0215, -0.0561,  0.0268,  0.0653, -0.0275,  0.0352, -0.0184,  0.0090,\n",
       "                       0.0687, -0.0065,  0.0485,  0.0036, -0.0206,  0.0429,  0.0447,  0.0557,\n",
       "                      -0.0346, -0.0598, -0.0226,  0.0353, -0.0584, -0.0021,  0.0320,  0.0087,\n",
       "                      -0.0010, -0.0425,  0.0412,  0.0578, -0.0243, -0.0011, -0.0149, -0.0355,\n",
       "                      -0.0526, -0.0324,  0.0239, -0.0395, -0.0484, -0.0287,  0.0369, -0.0031,\n",
       "                       0.0117, -0.0348, -0.0364, -0.0584, -0.0529, -0.0064, -0.0392, -0.0706,\n",
       "                      -0.0175, -0.0460, -0.0283,  0.0555, -0.0479, -0.0224, -0.0397,  0.0382,\n",
       "                      -0.0461, -0.0182, -0.0097, -0.0044, -0.0423,  0.0446, -0.0129,  0.0475,\n",
       "                       0.0153,  0.0020,  0.0439, -0.0497,  0.0093,  0.0699,  0.0507,  0.0239,\n",
       "                      -0.0546,  0.0304,  0.0150, -0.0667,  0.0121,  0.0309,  0.0101, -0.0676,\n",
       "                       0.0312, -0.0334,  0.0105,  0.0070, -0.0680, -0.0164, -0.0451,  0.0517,\n",
       "                       0.0048, -0.0672,  0.0363,  0.0239,  0.0388,  0.0418, -0.0221,  0.0074,\n",
       "                       0.0171,  0.0342,  0.0042,  0.0108, -0.0560,  0.0463,  0.0651,  0.0369,\n",
       "                      -0.0026,  0.0577,  0.0381,  0.0626,  0.0697, -0.0398, -0.0284, -0.0270,\n",
       "                       0.0589, -0.0236, -0.0393, -0.0066, -0.0320,  0.0375, -0.0252, -0.0337,\n",
       "                      -0.0043, -0.0126, -0.0019, -0.0066,  0.0433, -0.0131, -0.0038, -0.0581,\n",
       "                      -0.0061, -0.0514,  0.0666, -0.0673,  0.0628, -0.0078, -0.0636,  0.0034,\n",
       "                      -0.0244,  0.0196, -0.0637, -0.0565, -0.0409, -0.0178,  0.0543, -0.0321,\n",
       "                       0.0598,  0.0555, -0.0065, -0.0004, -0.0597, -0.0019,  0.0575,  0.0016,\n",
       "                       0.0270, -0.0245,  0.0516, -0.0409,  0.0427, -0.0682,  0.0112, -0.0526,\n",
       "                       0.0051,  0.0223,  0.0307,  0.0700,  0.0645,  0.0631, -0.0336, -0.0280,\n",
       "                       0.0618, -0.0694, -0.0116, -0.0194,  0.0499, -0.0126, -0.0409, -0.0332,\n",
       "                       0.0150, -0.0385, -0.0387, -0.0362, -0.0384, -0.0340, -0.0666,  0.0062,\n",
       "                       0.0036,  0.0186,  0.0287, -0.0161,  0.0518, -0.0456, -0.0682,  0.0168,\n",
       "                      -0.0226, -0.0021,  0.0302, -0.0414,  0.0569,  0.0111,  0.0297, -0.0018,\n",
       "                      -0.0134, -0.0067,  0.0581,  0.0448, -0.0370, -0.0255, -0.0195, -0.0332,\n",
       "                      -0.0478, -0.0156, -0.0078,  0.0132,  0.0192,  0.0341, -0.0467, -0.0394,\n",
       "                      -0.0681,  0.0016,  0.0103,  0.0054, -0.0553,  0.0685,  0.0118,  0.0011,\n",
       "                       0.0270, -0.0417,  0.0078,  0.0265,  0.0664,  0.0330,  0.0423, -0.0017,\n",
       "                      -0.0407, -0.0093, -0.0467, -0.0275,  0.0044, -0.0548, -0.0079,  0.0362,\n",
       "                       0.0646,  0.0515, -0.0150, -0.0255, -0.0660,  0.0100,  0.0677, -0.0038,\n",
       "                      -0.0302,  0.0077, -0.0620,  0.0450,  0.0529,  0.0176,  0.0245,  0.0211,\n",
       "                      -0.0602, -0.0148, -0.0524, -0.0485, -0.0529, -0.0563, -0.0390,  0.0354,\n",
       "                       0.0300,  0.0268, -0.0328,  0.0559,  0.0528, -0.0550,  0.0689,  0.0169,\n",
       "                      -0.0572, -0.0592,  0.0532,  0.0101, -0.0346,  0.0492,  0.0346, -0.0444,\n",
       "                      -0.0412,  0.0410,  0.0255, -0.0477,  0.0033, -0.0130,  0.0371,  0.0164,\n",
       "                      -0.0341, -0.0003, -0.0003, -0.0685, -0.0236,  0.0472, -0.0294,  0.0552,\n",
       "                      -0.0651,  0.0587, -0.0405, -0.0429,  0.0461, -0.0033,  0.0322,  0.0348,\n",
       "                      -0.0492, -0.0253,  0.0523,  0.0593, -0.0308, -0.0700, -0.0539,  0.0469,\n",
       "                       0.0363, -0.0525,  0.0065, -0.0349, -0.0473,  0.0411, -0.0166, -0.0218,\n",
       "                      -0.0500, -0.0320,  0.0294,  0.0073, -0.0100, -0.0667,  0.0618,  0.0517,\n",
       "                      -0.0084, -0.0568,  0.0145, -0.0414,  0.0125, -0.0277,  0.0134,  0.0005,\n",
       "                       0.0119, -0.0305, -0.0259, -0.0438, -0.0440, -0.0695, -0.0668, -0.0157,\n",
       "                      -0.0099, -0.0585, -0.0536,  0.0311, -0.0189,  0.0573,  0.0621, -0.0131,\n",
       "                      -0.0059,  0.0675, -0.0566, -0.0677,  0.0405, -0.0591, -0.0683, -0.0023,\n",
       "                      -0.0536,  0.0551, -0.0335, -0.0525, -0.0486, -0.0373,  0.0077, -0.0681,\n",
       "                      -0.0513, -0.0627,  0.0016,  0.0490, -0.0656, -0.0676,  0.0700, -0.0091,\n",
       "                       0.0407,  0.0432, -0.0575,  0.0327,  0.0041,  0.0011, -0.0141, -0.0618,\n",
       "                      -0.0264,  0.0449,  0.0016, -0.0401, -0.0578, -0.0276,  0.0102, -0.0259,\n",
       "                       0.0572, -0.0299,  0.0002,  0.0019,  0.0150,  0.0173, -0.0354, -0.0260,\n",
       "                       0.0604,  0.0128, -0.0692, -0.0106, -0.0104,  0.0477, -0.0630,  0.0075,\n",
       "                      -0.0286, -0.0033, -0.0068,  0.0604, -0.0358,  0.0630,  0.0356, -0.0012,\n",
       "                      -0.0536,  0.0559,  0.0579,  0.0220,  0.0183, -0.0229,  0.0047, -0.0649,\n",
       "                      -0.0310, -0.0640, -0.0135,  0.0533, -0.0704, -0.0172, -0.0552,  0.0457,\n",
       "                      -0.0046, -0.0333,  0.0348,  0.0381,  0.0007,  0.0445,  0.0436,  0.0417,\n",
       "                      -0.0538, -0.0109,  0.0222, -0.0558, -0.0486,  0.0080,  0.0448, -0.0179,\n",
       "                       0.0518,  0.0530,  0.0077,  0.0306, -0.0530, -0.0164, -0.0085,  0.0261,\n",
       "                      -0.0398,  0.0702,  0.0039, -0.0622,  0.0470, -0.0297,  0.0456,  0.0420,\n",
       "                      -0.0049,  0.0184,  0.0326,  0.0224, -0.0457,  0.0198,  0.0612, -0.0332,\n",
       "                       0.0582, -0.0210,  0.0683,  0.0316,  0.0034,  0.0509, -0.0359,  0.0481,\n",
       "                       0.0402, -0.0560,  0.0173,  0.0313, -0.0337,  0.0344,  0.0015,  0.0689,\n",
       "                      -0.0444, -0.0359, -0.0480, -0.0424, -0.0023,  0.0112, -0.0274, -0.0325,\n",
       "                       0.0634, -0.0432, -0.0593, -0.0413, -0.0396, -0.0270,  0.0613,  0.0004,\n",
       "                       0.0280,  0.0029,  0.0190, -0.0289, -0.0651,  0.0166, -0.0246, -0.0339,\n",
       "                       0.0236,  0.0366,  0.0188,  0.0111, -0.0162, -0.0037,  0.0327,  0.0613,\n",
       "                       0.0673, -0.0523, -0.0197, -0.0362,  0.0624,  0.0003,  0.0008, -0.0202,\n",
       "                       0.0687, -0.0490,  0.0631, -0.0039,  0.0185,  0.0461, -0.0351,  0.0608,\n",
       "                       0.0268, -0.0328, -0.0152,  0.0157, -0.0400, -0.0518,  0.0615, -0.0033,\n",
       "                       0.0026, -0.0327,  0.0259,  0.0125, -0.0245, -0.0016,  0.0107, -0.0595,\n",
       "                      -0.0265,  0.0322,  0.0585, -0.0186, -0.0416,  0.0201, -0.0491, -0.0102,\n",
       "                       0.0561, -0.0575, -0.0560,  0.0184,  0.0669,  0.0612,  0.0664, -0.0637,\n",
       "                       0.0150,  0.0336,  0.0655,  0.0369, -0.0198,  0.0528,  0.0679, -0.0404,\n",
       "                       0.0153,  0.0227, -0.0599,  0.0664,  0.0021, -0.0324, -0.0251, -0.0236,\n",
       "                       0.0358, -0.0301,  0.0657,  0.0490, -0.0434,  0.0693,  0.0015, -0.0033,\n",
       "                      -0.0191,  0.0483, -0.0394,  0.0221,  0.0118, -0.0688, -0.0445,  0.0466,\n",
       "                       0.0141,  0.0670, -0.0295,  0.0464, -0.0331,  0.0099,  0.0074,  0.0455,\n",
       "                       0.0069, -0.0125, -0.0028, -0.0590,  0.0307, -0.0137,  0.0622,  0.0169,\n",
       "                       0.0113,  0.0231, -0.0283, -0.0496,  0.0554,  0.0585,  0.0481, -0.0692],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_hh_l1',\n",
       "              tensor([ 0.0277,  0.0577, -0.0253, -0.0076, -0.0335, -0.0665,  0.0416, -0.0664,\n",
       "                       0.0092, -0.0299, -0.0506,  0.0480, -0.0490, -0.0360, -0.0626,  0.0131,\n",
       "                      -0.0505, -0.0476,  0.0002,  0.0131, -0.0011, -0.0147, -0.0315, -0.0252,\n",
       "                      -0.0703, -0.0276, -0.0416, -0.0056, -0.0097,  0.0450, -0.0091,  0.0057,\n",
       "                      -0.0298,  0.0313, -0.0390, -0.0227,  0.0017, -0.0521, -0.0325,  0.0020,\n",
       "                      -0.0630,  0.0545,  0.0051, -0.0682, -0.0648,  0.0566,  0.0345, -0.0475,\n",
       "                       0.0515, -0.0475,  0.0427, -0.0313, -0.0377, -0.0302,  0.0025, -0.0684,\n",
       "                      -0.0364,  0.0676, -0.0192,  0.0115,  0.0300,  0.0697,  0.0271, -0.0375,\n",
       "                      -0.0097,  0.0079, -0.0018,  0.0438, -0.0150, -0.0463,  0.0423, -0.0232,\n",
       "                      -0.0102,  0.0528,  0.0460,  0.0084, -0.0448,  0.0012,  0.0358,  0.0686,\n",
       "                       0.0083,  0.0281, -0.0527,  0.0070,  0.0081, -0.0551,  0.0290,  0.0221,\n",
       "                      -0.0635,  0.0661, -0.0501,  0.0305,  0.0271,  0.0142, -0.0419, -0.0526,\n",
       "                      -0.0418,  0.0027,  0.0083,  0.0605,  0.0256, -0.0122,  0.0598,  0.0164,\n",
       "                      -0.0255, -0.0400, -0.0164,  0.0623, -0.0302, -0.0282, -0.0099, -0.0689,\n",
       "                       0.0705,  0.0205,  0.0153, -0.0208, -0.0633, -0.0168,  0.0655, -0.0270,\n",
       "                      -0.0567, -0.0675, -0.0217, -0.0258,  0.0259,  0.0377, -0.0388, -0.0624,\n",
       "                       0.0070,  0.0598, -0.0263,  0.0662, -0.0024, -0.0444,  0.0211,  0.0135,\n",
       "                       0.0502, -0.0397, -0.0113,  0.0630, -0.0154,  0.0698,  0.0563, -0.0680,\n",
       "                      -0.0703, -0.0686,  0.0278, -0.0285, -0.0431,  0.0220,  0.0702, -0.0166,\n",
       "                       0.0287,  0.0674,  0.0664,  0.0383, -0.0125,  0.0077,  0.0467,  0.0024,\n",
       "                      -0.0109, -0.0677,  0.0240, -0.0642,  0.0580,  0.0595,  0.0191, -0.0209,\n",
       "                      -0.0404,  0.0638,  0.0024,  0.0168, -0.0526,  0.0008,  0.0260, -0.0366,\n",
       "                       0.0424, -0.0032, -0.0384,  0.0257, -0.0379,  0.0336,  0.0166, -0.0159,\n",
       "                       0.0083, -0.0680,  0.0637, -0.0485, -0.0586,  0.0228,  0.0198, -0.0118,\n",
       "                      -0.0080,  0.0315,  0.0395,  0.0357, -0.0015, -0.0202, -0.0677,  0.0046,\n",
       "                       0.0459, -0.0322,  0.0333, -0.0291, -0.0407, -0.0004,  0.0200,  0.0444,\n",
       "                       0.0042,  0.0029,  0.0351, -0.0537, -0.0061,  0.0512,  0.0257, -0.0412,\n",
       "                       0.0093, -0.0303,  0.0487, -0.0213,  0.0700, -0.0085,  0.0119,  0.0121,\n",
       "                       0.0556,  0.0554, -0.0056, -0.0212,  0.0517, -0.0448,  0.0223, -0.0157,\n",
       "                       0.0264,  0.0573, -0.0676,  0.0082, -0.0114, -0.0454, -0.0044,  0.0070,\n",
       "                       0.0675, -0.0012,  0.0264, -0.0378,  0.0665,  0.0102,  0.0674, -0.0698,\n",
       "                       0.0556, -0.0263, -0.0327,  0.0629, -0.0272,  0.0301,  0.0099, -0.0352,\n",
       "                       0.0528, -0.0660, -0.0094, -0.0448,  0.0677,  0.0276, -0.0410,  0.0527,\n",
       "                       0.0219,  0.0427, -0.0404, -0.0416,  0.0230,  0.0584,  0.0632, -0.0313,\n",
       "                      -0.0316,  0.0127,  0.0545, -0.0614, -0.0659, -0.0130,  0.0122, -0.0376,\n",
       "                      -0.0230, -0.0192,  0.0361, -0.0649, -0.0130,  0.0279,  0.0551,  0.0164,\n",
       "                      -0.0281,  0.0666,  0.0344,  0.0521,  0.0657, -0.0115, -0.0399,  0.0577,\n",
       "                      -0.0617,  0.0433,  0.0676,  0.0558, -0.0631, -0.0233, -0.0388, -0.0446,\n",
       "                       0.0068, -0.0026, -0.0383, -0.0582,  0.0057,  0.0565, -0.0432, -0.0009,\n",
       "                      -0.0375,  0.0227,  0.0392,  0.0152, -0.0349,  0.0545, -0.0581,  0.0395,\n",
       "                       0.0243,  0.0270, -0.0216, -0.0558, -0.0169,  0.0341,  0.0015,  0.0198,\n",
       "                      -0.0125,  0.0446,  0.0593,  0.0240,  0.0075, -0.0535, -0.0043,  0.0596,\n",
       "                      -0.0611, -0.0440,  0.0548,  0.0304, -0.0455, -0.0405,  0.0290,  0.0429,\n",
       "                      -0.0040, -0.0197,  0.0485,  0.0592, -0.0009, -0.0130, -0.0261, -0.0660,\n",
       "                       0.0242, -0.0488,  0.0511,  0.0537,  0.0589, -0.0030, -0.0559,  0.0004,\n",
       "                       0.0564,  0.0250, -0.0551,  0.0644, -0.0424, -0.0595,  0.0085,  0.0442,\n",
       "                       0.0280,  0.0631,  0.0354,  0.0459, -0.0488, -0.0296,  0.0136, -0.0069,\n",
       "                       0.0073,  0.0395,  0.0089,  0.0235,  0.0291, -0.0538, -0.0655,  0.0578,\n",
       "                       0.0198, -0.0453,  0.0672, -0.0482,  0.0051,  0.0447,  0.0362,  0.0231,\n",
       "                       0.0247,  0.0641, -0.0310, -0.0514,  0.0085,  0.0616, -0.0510, -0.0520,\n",
       "                      -0.0066, -0.0310,  0.0676,  0.0047, -0.0214,  0.0104,  0.0218, -0.0442,\n",
       "                       0.0435,  0.0556, -0.0266, -0.0145,  0.0379,  0.0141, -0.0395,  0.0519,\n",
       "                       0.0629,  0.0090,  0.0020,  0.0506, -0.0678, -0.0091, -0.0473, -0.0596,\n",
       "                      -0.0579,  0.0023, -0.0356, -0.0347, -0.0626,  0.0222,  0.0481,  0.0198,\n",
       "                      -0.0315,  0.0409, -0.0701,  0.0253,  0.0137,  0.0427, -0.0617, -0.0305,\n",
       "                      -0.0570,  0.0477,  0.0249,  0.0349, -0.0303,  0.0180, -0.0431, -0.0015,\n",
       "                      -0.0339,  0.0298,  0.0119,  0.0073, -0.0199, -0.0285, -0.0204,  0.0668,\n",
       "                      -0.0333, -0.0053, -0.0275,  0.0057,  0.0146, -0.0033, -0.0471,  0.0601,\n",
       "                       0.0119,  0.0685, -0.0528,  0.0076,  0.0442,  0.0275, -0.0216, -0.0522,\n",
       "                       0.0359,  0.0309,  0.0256, -0.0638,  0.0061,  0.0403, -0.0197, -0.0484,\n",
       "                       0.0055, -0.0451,  0.0548, -0.0221, -0.0496,  0.0549, -0.0199, -0.0374,\n",
       "                       0.0075,  0.0105, -0.0293, -0.0267,  0.0707,  0.0447, -0.0616, -0.0139,\n",
       "                       0.0596,  0.0013,  0.0164, -0.0364, -0.0318, -0.0230, -0.0548, -0.0606,\n",
       "                       0.0393, -0.0492,  0.0161, -0.0456,  0.0066,  0.0415,  0.0381,  0.0392,\n",
       "                      -0.0594,  0.0641, -0.0258, -0.0182, -0.0417,  0.0368, -0.0105, -0.0074,\n",
       "                      -0.0665, -0.0643,  0.0158,  0.0146, -0.0496,  0.0572, -0.0105,  0.0448,\n",
       "                      -0.0587,  0.0420,  0.0228, -0.0243,  0.0690,  0.0287,  0.0537, -0.0255,\n",
       "                      -0.0336, -0.0172, -0.0587, -0.0294, -0.0464,  0.0084,  0.0435, -0.0079,\n",
       "                      -0.0093,  0.0682, -0.0385,  0.0611,  0.0498, -0.0493,  0.0414,  0.0060,\n",
       "                       0.0378,  0.0559,  0.0273, -0.0312,  0.0472, -0.0116, -0.0611,  0.0122,\n",
       "                      -0.0413, -0.0315, -0.0407,  0.0064,  0.0236,  0.0384,  0.0673, -0.0316,\n",
       "                      -0.0253, -0.0686, -0.0532,  0.0644,  0.0064,  0.0236, -0.0705, -0.0067,\n",
       "                       0.0153, -0.0251, -0.0518, -0.0289,  0.0375, -0.0254,  0.0368,  0.0023,\n",
       "                      -0.0059,  0.0217, -0.0406, -0.0516,  0.0607,  0.0334,  0.0360,  0.0489,\n",
       "                      -0.0177, -0.0308, -0.0350, -0.0105,  0.0176, -0.0116, -0.0282, -0.0007,\n",
       "                       0.0444, -0.0278, -0.0144, -0.0331, -0.0068,  0.0676,  0.0241,  0.0259,\n",
       "                       0.0586, -0.0052,  0.0289, -0.0399,  0.0202, -0.0154, -0.0057,  0.0680,\n",
       "                       0.0111,  0.0624,  0.0170, -0.0174, -0.0383,  0.0639,  0.0684,  0.0690,\n",
       "                       0.0441, -0.0611,  0.0513, -0.0287,  0.0199,  0.0501,  0.0256, -0.0551,\n",
       "                       0.0091, -0.0694, -0.0138,  0.0124, -0.0213,  0.0162, -0.0044, -0.0668,\n",
       "                       0.0090, -0.0224, -0.0163, -0.0582, -0.0692, -0.0296,  0.0111, -0.0490,\n",
       "                       0.0511, -0.0350,  0.0575, -0.0494, -0.0362,  0.0050, -0.0171,  0.0416,\n",
       "                      -0.0426,  0.0285, -0.0237, -0.0099, -0.0335,  0.0586,  0.0058,  0.0572,\n",
       "                       0.0418,  0.0352,  0.0558,  0.0282, -0.0199, -0.0309, -0.0559, -0.0502,\n",
       "                      -0.0036,  0.0537, -0.0644,  0.0300, -0.0423,  0.0679,  0.0406, -0.0159,\n",
       "                      -0.0599,  0.0339, -0.0334,  0.0472, -0.0069, -0.0020, -0.0605, -0.0396,\n",
       "                       0.0546,  0.0388,  0.0036, -0.0509,  0.0661,  0.0007,  0.0364,  0.0516,\n",
       "                      -0.0215,  0.0035, -0.0460,  0.0630, -0.0603, -0.0194,  0.0597,  0.0010,\n",
       "                      -0.0486, -0.0406,  0.0394, -0.0484,  0.0408,  0.0397,  0.0509, -0.0213,\n",
       "                      -0.0121, -0.0185,  0.0086, -0.0028,  0.0279, -0.0030,  0.0667, -0.0301,\n",
       "                       0.0150, -0.0240,  0.0278,  0.0135,  0.0594,  0.0320, -0.0390,  0.0413,\n",
       "                       0.0424, -0.0106, -0.0071, -0.0674,  0.0246, -0.0593,  0.0515, -0.0386,\n",
       "                       0.0165,  0.0027,  0.0045, -0.0566,  0.0582, -0.0524, -0.0184,  0.0166,\n",
       "                       0.0246,  0.0327, -0.0620,  0.0226, -0.0603,  0.0111, -0.0156,  0.0305,\n",
       "                      -0.0666, -0.0166, -0.0162,  0.0670, -0.0275,  0.0538,  0.0477,  0.0263,\n",
       "                      -0.0415,  0.0108,  0.0293,  0.0251, -0.0092,  0.0303, -0.0160, -0.0049,\n",
       "                       0.0456,  0.0498, -0.0014, -0.0614, -0.0090,  0.0122,  0.0081, -0.0595,\n",
       "                      -0.0131,  0.0160,  0.0245,  0.0349,  0.0301, -0.0376,  0.0100, -0.0635,\n",
       "                      -0.0163, -0.0210, -0.0563,  0.0703,  0.0585, -0.0673,  0.0325,  0.0054,\n",
       "                      -0.0029,  0.0299, -0.0411,  0.0353, -0.0418,  0.0597,  0.0511,  0.0656],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.weight',\n",
       "              tensor([[-0.0741, -0.0362,  0.0747,  ...,  0.0200,  0.0351, -0.0910],\n",
       "                      [ 0.0956,  0.0994, -0.0807,  ...,  0.0909,  0.0632, -0.0181],\n",
       "                      [ 0.0822, -0.0848, -0.0954,  ..., -0.0596, -0.0452,  0.0552],\n",
       "                      ...,\n",
       "                      [-0.0194, -0.0107,  0.0577,  ..., -0.0625, -0.0364, -0.0657],\n",
       "                      [-0.0690, -0.0153, -0.0525,  ...,  0.0393,  0.0737,  0.0737],\n",
       "                      [-0.0307,  0.0043,  0.0422,  ..., -0.0410, -0.0566, -0.0087]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do the embeddings look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/miniconda3/envs/breadventure/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.weight', 'rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.bias_hh_l0', 'rnn.weight_ih_l1', 'rnn.weight_hh_l1', 'rnn.bias_ih_l1', 'rnn.bias_hh_l1', 'decoder.weight', 'decoder.bias'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33278, 200])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = model.state_dict()['encoder.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<eos>', '=', 'Valkyria', ..., 'Nests', 'flea', 'gallinae'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(corpus.dictionary.word2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flour',\n",
       " 'water',\n",
       " 'bread',\n",
       " 'coffee',\n",
       " 'driving',\n",
       " 'car',\n",
       " 'horse',\n",
       " 'chicken',\n",
       " 'bird',\n",
       " 'cow',\n",
       " 'leg']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_words = ['flour', 'water', 'bread', 'coffee', 'driving', 'car', 'horse', 'chicken', 'bird', 'cow', 'leg']\n",
    "some_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_idxs = [corpus.dictionary.word2idx[word] for word in some_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.02211978, -0.04078216, -0.05417591, ...,  0.01053627,\n",
       "         0.01473385,  0.01968203],\n",
       "       [-0.05243944,  0.03832772, -0.00496112, ...,  0.00311856,\n",
       "         0.09399863,  0.08048058],\n",
       "       [ 0.03838401, -0.00253663,  0.00355909, ..., -0.05477671,\n",
       "        -0.00039558, -0.06522658],\n",
       "       ...,\n",
       "       [ 0.05418205, -0.02397664, -0.03950721, ...,  0.01659943,\n",
       "        -0.0940535 , -0.01276223],\n",
       "       [ 0.0213261 ,  0.05090193,  0.06393952, ...,  0.09761625,\n",
       "         0.06812478,  0.00376463],\n",
       "       [-0.02371339, -0.08695342,  0.04383665, ...,  0.0956846 ,\n",
       "        -0.08986057,  0.0581846 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embeds[some_idxs].shape)\n",
    "np.array(embeds[some_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=123)\n",
    "#np.set_printoptions(suppress=True)\n",
    "Y = tsne.fit_transform(np.array(embeds[some_idxs]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEACAYAAADx33KKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt8VOW97/HPjwCSggKaaCWiQIsoQkJIuCgBtVhCrQJaEGqVuN1y2Uo9Hms4sD2nBVq7rdjdCq0CWtxY8QbSSL0UFMUQBCE0IcglBiQoCQUsBAXCJfE5f8xKOgkJ4ZLJmgnf9+s1r6x51iW/J0P4Zq155lnmnENERMQvTfwuQEREzm0KIhER8ZWCSEREfKUgEhERXymIRETEVwoiERHxlYJIJAKY2UNm9i2/6xAJBdPniETCn5kVAsnOuS9PY58o51x56KoSqR86IxJpQGY20cwe9JZ/Z2bve8sDzexFM3vGzLLNbKOZTfXWPQi0Az4wsw+8tkFmtsrM/m5mC8yslddeaGY/N7MsYIQvnRQ5TQoikYaVCfT3lpOBVmbWDEgBVgCPOueSgXjgejOLd87NAIqBG51zN5pZDPB/gZuccz2BbODhoO9xxDmX4px7pYH6JHJWmvpdgMg5Zh2QZGbnA0eBvxMIpP7Ag8AdZjaWwO/mpUBXIK/aMfp67SvNDKA5sCpo/auh7IBIfVMQiTQg59xx7/2efwM+IhAyNwLfAUqBR4Bezrn9ZvY/QIsaDmPAu865H9fybQ7Vd90ioaRLcyINL5NA4GQSuBw3HsgFLiAQIgfM7BLgB0H7fA2c7y2vBvqZ2XcBzOxbZnZlA9UuUu8iYtRcTEyM69Chg99liJyRDUUHqjz/5uhhyvYX0+ziTsS3b8snn3xCbGwsl1xyCYWFhRw6dIjmzZvTpEkTWrduTUxMDHv27GHPnj00a9aMLl268NVXX1FUVETF72+7du1o06YNGzZs4Oqrr6ZpU13sEFi3bt2XzrlYv+uoS0T8a+3QoQPZ2dl+lyFyRvo9/j5FJaUntMe1iWblpO/5UJGcK8xsh981nApdmhMJsfTULkQ3i6rSFt0sivTULj5VJBJeIuKMSCSSDUuMA2D6knyKS0pp1yaa9NQule0i5zoFkUgDGJYYp+ARqYUuzYmIiK8URCIi4isFkYiI+EpBJCIivlIQiYiIrxREIiLiKwWRiIj4SkEkIiK+UhCJiPhkxowZXH311cTFxTFhwgS/y/GNZlYQEfHJ008/zTvvvMOHH35YLxM7l5eXExUVVfeGYUZnRGGoVatWfpcgIiE2fvx4PvvsM4YMGcL+/fsr23fs2MHAgQOJj49n4MCBfP755wDcc889LFy4sHK7iv8nli9fzo033sidd95J9+7dG7YT9URBJCLig1mzZtGuXTs++OAD2rZtW9k+YcIERo8eTV5eHj/5yU948MEH6zzWmjVreOyxx9i0aVMoSw4ZXZoLc9OnT+e1117j6NGj3HbbbUydOhWAX/7yl8yfP5/27dsTExNDUlISjzzyiM/VisjJZOQUVZmF/fCx8hO2WbVqFYsWLQLg7rvvZuLEiXUet3fv3nTs2LHe620oCqIwtnTpUgoKClizZg3OOYYMGUJmZibf+ta3eP3118nJyaGsrIyePXuSlJTkd7kichIZOUVMXrSB0uOB8CkqKWX/4WO8nbfrpPuZGQBNmzblm2++AcA5x7Fjxyq3admyZYiqbhj1cmnOzOaa2R4z+ySobYqZFZlZrve4OWjdZDPbamb5ZpZaHzVEuoycIvo9/j4dJ71F6fFyMnKKWLp0KUuXLiUxMZGePXuyZcsWCgoKyMrKYujQoURHR3P++edz6623+l2+iNRh+pL8yhCq4Bz84YOtVdquu+46XnnlFQDmz59PSkoKELhT9bp16wB44403OH78eANU3TDq64zof4A/AC9Ua/+dc+7J4AYz6wqMAq4B2gHvmdmVzrkTz1HPEdX/UnIOJi/awJW7v2by5MmMGzeuyva/+93v/ChTRM5CcQ23iwf4x4Gq7TNmzODee+9l+vTpxMbG8vzzzwMwZswYhg4dSu/evRk4cGDEnwUFM+dc/RzIrAPwpnOum/d8CnCwhiCaDOCc+y/v+RJginNuVW3HTk5OdvUxtDFc9Xv8fYqC/pF+/t/DufzhhbTcs5FWG19n2bJltGrViqKiIpo1a8aOHTsYN24cH330EWVlZSQlJTFmzBi9RyQSxqr/nleIaxPNyknfC8n3NLN1zrnkkBy8HoV61NwEM8vzLt1VDAuJA74I2man11aFmY01s2wzy967d2+Iy/RXbX8pHb74Gu68806uvfZaunfvzvDhw/n666/p1asXQ4YMISEhgdtvv53k5GRat27dwFWLyOlIT+1CdLOqn/GJbhZFemoXnyoKH6E8I7oE+BJwwC+BS51z95rZH4FVzrkXve3+BLztnHu9tmOfa2dEFU72l9LBgwdp1aoVhw8fZsCAAcyZM4eePXuGulQROQvVR82lp3YJ6S3kI+WMKGSj5pxzuyuWzexZ4E3v6U6gfdCmlwHFoaojEqSndqnyHhHU/ZfS2LFj2bRpE0eOHCEtLU0hJBIBhiXGhTR4IlXIgsjMLnXOVYxLvA2oGFG3GHjJzP6bwGCFzsCaUNURCSr+YZ7OX0ovvfRSQ5UnIhJS9RJEZvYycAMQY2Y7gV8AN5hZDwKX5gqBcQDOuY1m9hqwCSgDHjiXR8xV0F9KInKuqrf3iEKpsb9HJCISCpHyHpHmmhMREV8piERExFcKIhER8ZWCSEREfKUgEhERXymIRETEVwoiERHxlYJIRER8pSASERFfKYhERMRXCiIREfGVgkhERHylIBIREV8piERExFcKIhER8ZWCSEREfKUgEhERXymIRETEVwoiERHxlYJIRER8pSASERFfKYhERMRXCiIREfGVgkhERHxVL0FkZnPNbI+ZfRLUdqGZvWtmBd7Xtl67mdkMM9tqZnlm1rM+ahARkchUX2dE/wMMrtY2CVjmnOsMLPOeA/wA6Ow9xgLP1FMNIiISgeoliJxzmcC+as1DgXne8jxgWFD7Cy5gNdDGzC6tjzpERCTyhPI9okucc7sAvK8Xe+1xwBdB2+302qows7Fmlm1m2Xv37g1hmSIi4ic/BitYDW3uhAbn5jjnkp1zybGxsQ1QloiI+CGUQbS74pKb93WP174TaB+03WVAcQjrEBGRMBbKIFoMpHnLacAbQe2jvdFzfYEDFZfwRETk3NO0Pg5iZi8DNwAxZrYT+AXwOPCamf078Dkwwtv8beBmYCtwGPi3+qhBREQiU70EkXPux7WsGljDtg54oD6+r4iIRD7NrCAiIr5SEImIiK8URCIi4isFkYiI+EpBJCIivlIQiUi927t3L3369CExMZEVK1awYMECrr76am688Ua/S5MwVC/Dt0VEgi1btoyrrrqKefMC8x4PHjyYp59+WkEkNdIZkYicshdeeIH4+HgSEhK4++672bFjBwMHDiQ+Pp6BAwfy+eefk5uby8SJE3n77bfp0aMHU6dOJSsri/Hjx5Oenk55eTnp6en06tWL+Ph4Zs+eXXn86dOnV7b/4he/8LGn0pB0RiQip2Tjxo089thjrFy5kpiYGPbt20daWhqjR48mLS2NuXPn8uCDD5KRkcG0adPIzs7mD3/4AwAffPABTz75JMnJycyZM4fWrVuzdu1ajh49Sr9+/Rg0aBAFBQUUFBSwZs0anHMMGTKEzMxMBgwY4HPPJdQURCJSq4ycIqYvyae4pBTb9Dd6pqQSExMDwIUXXsiqVatYtGgRAHfffTcTJ06s85hLly4lLy+PhQsXAnDgwAEKCgpYunQpS5cuJTExEYCDBw9SUFCgIDoHKIhEpEYZOUVMXrSB0uPlABw4fIzl+V+RkVPEsMQTbiEGgFlNd3mpyjnHzJkzSU1NrdK+ZMkSJk+ezLhx486+eIkoeo9IRGo0fUl+ZQgBtLgigQObMvn1ojUA7Nu3j+uuu45XXnkFgPnz55OSklLncVNTU3nmmWc4fvw4AJ9++imHDh0iNTWVuXPncvDgQQCKiorYs2fPyQ4ljYTOiESkRsUlpVWeN4+9gtbXjiR31kMkLJ5CYmIiM2bM4N5772X69OnExsby/PPP13nc++67j8LCQnr27IlzjtjYWDIyMhg0aBCbN2/m2muvBaBVq1a8+OKLXHzxxXUcUSKdBSbDDm/JyckuOzvb7zJEzin9Hn+fomphBBDXJpqVk77nQ0VyusxsnXMu2e866qJLcyJSo/TULkQ3i6rSFt0sivTULj5VJI2VLs2JSI0qBiRUjJpr1yaa9NQutQ5UEDlTCiIRqdWwxDgFj4ScLs2JiIivFEQiIuIrBZGIiPhKQSQiIr5SEImIiK8URCIi4isFkfjqVO5vU15eTqdOnXDOUVJSQpMmTcjMzASgf//+bN261edeiMjZCHkQmVmhmW0ws1wzy/baLjSzd82swPvaNtR1SPipuL/N+++/z/r163nqqaeYMGECo0ePJi8vj5/85Cc8+OCDREVFceWVV7Jp0yaysrJISkpixYoVHD16lJ07d/Ld737X766IyFloqDOiG51zPYLmPJoELHPOdQaWec/lHJGRU0S/x99nwEMzOdQumawvjgL/ur/NnXfeCQTub5OVlQUEznwyMzPJzMxk8uTJZGVlsXbtWnr16uVbP0Skfvh1aW4oMM9bngcM86kOaWAV97gpKinFOcfXR8uZvGgDGTlFNW5fcX+b/v37s2LFCtasWcPNN99MSUkJy5cv103TRBqBhggiByw1s3VmNtZru8Q5twvA+6p53s8Rwfe4aXFFAoe3rODgV/uZviT/pPe36dOnDx999BFNmjShRYsW9OjRg9mzZ9O/f3/f+iIi9aMh5prr55wrNrOLgXfNbMup7OSF1liAyy+/PJT1SQMKvsdNxf1tdr80id3WhIe3XF/r/W3OO+882rdvT9++fYHAGdLLL79M9+7dfemHiNSfBr0fkZlNAQ4CY4AbnHO7zOxSYLlzrta55XU/osZD97gRaTi6HxFgZi3N7PyKZWAQ8AmwGEjzNksD3ghlHRI+dI8bEaku1JfmLgH+4r3h3BR4yTn3NzNbC7xmZv8OfA6MCHEdEiZ0jxsRqU63ChcRaaR0aU5EROQURHQQ3XPPPSxcuPCE9uLiYoYPH37SfTt06MCXX34ZqtJEROQURXQQ1aZdu3Y1BpSIiISfiAqi6hNkAmRmZnLdddfRqVOnyvApLCykW7duAJSXl/PII4/QvXt34uPjmTlzZpVjlpaWMnjwYJ599lkAXnzxRXr37k2PHj0YN24c5eWBD1+2atWKRx99lISEBPr27cvu3bsbqtsiIo1axARRTRNkAuzatYusrCzefPNNJk06ccq6OXPmsH37dnJycion0qxw8OBBbr31Vu68807GjBnD5s2befXVV1m5ciW5ublERUUxf/58AA4dOkTfvn1Zv349AwYMqAwuEZFIYmZTzOyRGtrHm9noOvZ9zsy61ndNDTGzwlnbUHSAWyfPomdKKjExMUBggkyAYcOG0aRJE7p27VrjWcp7773H+PHjadq0aZX9AIYOHcrEiRMrw2nZsmWsW7euciLN0tJSLr44MPtQ8+bNueWWWwBISkri3XffDVFvRUQalpk1dc7Nqms759x9ofj+EXNGVHL4GMvz954wOeZ5551XuVzTUHTnXOXEmdX169ePd955p3I/5xxpaWnk5uaSm5tLfn4+U6ZMAaBZs2aVx4mKiqKsrKw+uiUiEnJm9qiZ5ZvZe0AXr225mf3azD4E/lfFmZKZXW1ma4L27WBmeUH7JHvLB83sMTNbb2arzewSr/073vO1ZjbNzA7WVV/EBFGLKxI4sCmTXy8K/Hz27dt3SvsNGjSIWbNmVQZH8H7Tpk3joosu4v777wdg4MCBLFy4kD179lRuu2PHjvrshohIgzKzJGAUkAjcDgTfO6WNc+5659xvKxqcc5uB5mbWyWsaCbxWw6FbAqudcwlAJoGp2wCeAp5yzvUCik+lxogJoooJMnNnPURCQgIPP/zwKe133333cfnll1cOcnjppZeqrP/973/PkSNHmDhxIl27duVXv/oVgwYNIj4+nu9///vs2rUrFN0REQmZint+Nf/2d5OA/sBfnHOHnXNfEZhircKrtRziNeAOb3lkLdsdA970ltcBHbzla4EF3vJLnIKImFnhvEs7u0vTfg9ockwRkZOpuOdX6fFyds17iGP/2Pq/gbbOuV8AmNl/EzhTuQV4xDlXcefsKcBB59yTZvYdAmEyCnjZOZfkbbO8Yh8zO+ica+W1Dwducc7dY2b/JHCrnzIzuwAortiuNhFzRgSaHFNEpC7B9/zyZAK3mVm0Nwn1rXUdwzm3DSgH/h+1nzXVZjXwI2951KnsEBGj5iBwJqTJMUVETq642m1WnHN/N7NXgVxgB7DiFA/1KjAd6HiaJTwEvGhmPwPeAg7UtUNEXJrTpKciIqcm+J5fu+Y9xNFdBTUPGw4RM/sWUOqcc2Y2Cvixc27oyfaJqEtzIiJycjXd86uBJQG53pDv+4Gf1bVDxFyaExGRugXf88uPMb/OuRVAwunso0tzIiKNlO5HJCIicgoURCIi4isFkYiI+EpBJCIivlIQiYiIrxREIiLiKwWRiIj4SkEkIiK+UhCJiIivfAsiMxvs3bp2q5lN8qsOERHxly9BZGZRwB+BHwBdgR+bWVc/ahEREX/5dUbUG9jqnPvMOXcMeAU46TThIiLSOPkVRHHAF0HPd3ptlcxsrJllm1n23r17G7Q4ERFpOH4FUU03aqoyDbhzbo5zLtk5lxwbG9tAZYmISEPzK4h2Au2Dnl8GFPtUi4iI+MivIFoLdDazjmbWHBgFLPapFhER8ZEvd2h1zpWZ2QRgCRAFzHXObfSjFhER8Zdvtwp3zr0NvO3X9xcRkfCgmRVERMRXCiIREfGVgkhERHylIBIREV8piERExFcKIhER8ZWCSEREfKUgEolAhYWFdOvW7YT2++67j02bNtW5//Lly7nllltCUZrIafPtA60iUv+ee+65GtvLy8uJiopq4GpETo3OiEQiVFlZGWlpacTHxzN8+HAOHz7MDTfcQHZ2NgCtWrXi5z//OX369GHVqlX87W9/46qrriIlJYVFixb5XL3IvyiIRCJUfn4+Y8eOJS8vjwsuuICnn366yvpDhw7RrVs3Pv74Y5KTkxkzZgx//etfWbFiBf/4xz98qlrkRAoikQiRkVNEv8ffp+Okt/jRMx8R8+129OvXD4C77rqLrKysKttHRUXxox/9CIAtW7bQsWNHOnfujJlx1113NXj9IrVREIlEgIycIiYv2kBRSSkO2P3VEUoOl5GRU1S5jVnV+022aNGiyvtC1deLhAsFkUgEmL4kn9Lj5VXayr7aw8/nBN7refnll0lJSal1/6uuuort27ezbdu2yu1FwoWCSCQCFJeUntDW7KL2bF/9NvHx8ezbt4//+I//qHX/Fi1aMGfOHH74wx+SkpLCFVdcEcpyRU6LOef8rqFOycnJrmIkkMi5qN/j71NUQxjFtYlm5aTv+VCRRAIzW+ecS/a7jrrojEgkAqSndiG6WdXPAUU3iyI9tYtPFYnUH32gVSQCDEuMAwLvFRWXlNKuTTTpqV0q20UimYJIJEIMS4xT8EijpEtzIiLiKwWRiIj4SkEkIiK+UhCJiIivFEQiIuKrkAWRmU0xsyIzy/UeNwetm2xmW80s38xSQ1WDiIiEv1AP3/6dc+7J4AYz6wqMAq4B2gHvmdmVzrnymg4gIiKNmx+X5oYCrzjnjjrntgNbgd4+1CEiImEg1EE0wczyzGyumbX12uKAL4K22em1VWFmY80s28yy9+7dG+IyRUTEL2cVRGb2npl9UsNjKPAM8B2gB7AL+G3FbjUc6oSZV51zc5xzyc655NjY2LMpU0REwthZvUfknLvpVLYzs2eBN72nO4H2QasvA4rPpg4REYlcoRw1d2nQ09uAT7zlxcAoMzvPzDoCnYE1oapDRETCWyhHzT1hZj0IXHYrBMYBOOc2mtlrwCagDHhAI+ZERM5dIQsi59zdJ1n3GPBYqL63iIhEDs2sICIivlIQCWVlZX6XICLnMN0Yr5F54YUXePLJJzEz4uPjueOOO/jVr37FsWPHuOiii5g/fz6XXHIJU6ZMobi4mMLCQmJiYnjppZf8Ll1EzlEKokZk48aNPPbYY6xcuZKYmBj27duHmbF69WrMjOeee44nnniC3/428JGudevWkZWVRXR0tM+Vi8i5TEEU4TJyipi+JJ/iklJs09/omZJKTEwMABdeeCEbNmxg5MiR7Nq1i2PHjtGxY8fKfYcMGaIQEhHf6T2iCJaRU8TkRRsoKinFASWHj7E8fy8ZOUWV2/z0pz9lwoQJbNiwgdmzZ3PkyJHKdS1btvShahGRqhREEWz6knxKj//rI1gtrkjgwKZMfr0o8Pngffv2ceDAAeLiAlP5zZs3z5c6RURORkEUwYpLSqs8bx57Ba2vHUnurIdISEjg4YcfZsqUKYwYMYL+/ftXXrITEQkn5twJ842GneTkZJedne13GWGn3+PvU1QtjADi2kSzctL3fKhIRMKJma1zziX7XUdddEYUwdJTuxDdLKpKW3SzKNJTu/hUkYjI6dOouQg2LDHw3k/FqLl2baJJT+1S2S4iEgkURBFuWGKcgkdEIpouzYmIiK8URCIi4isFkYhIPSksLKRbt25+lxFxFEQiImHgXJ4FX0EkIlKPysvLGTNmDNdccw2DBg2itLSU3Nxc+vbtS3x8PLfddhv79+8H4IYbbuA///M/uf7663nqqadYsGAB3bp1IyEhgQEDBlQeLz09nV69ehEfH8/s2bP97F5IKIhEROpRQUEBDzzwABs3bqRNmza8/vrrjB49mt/85jfk5eXRvXt3pk6dWrl9SUkJH374IT/72c+YNm0aS5YsYf369SxevBiAP/3pT7Ru3Zq1a9eydu1ann32WbZv3+5X90JCw7dFRM5C8Az4F7oDXNyuPT169AAgKSmJbdu2UVJSwvXXXw9AWloaI0aMqNx/5MiRlcv9+vXjnnvu4Y477uD2228HYOnSpeTl5bFw4UIADhw4QEFBQZWZ9COdgkhE5AxVzIBfMfnw7q+O8M8jjoycIoYlxhEVFUVJSclJjxE8C/6sWbP4+OOPeeutt+jRowe5ubk455g5cyapqakh7YufdGlOROQMVZ8BH8A5x/Ql+ZXPW7duTdu2bVmxYgUAf/7znyvPjqrbtm0bffr0Ydq0acTExPDFF1+QmprKM888w/HjxwH49NNPOXToUIh65A+dEYmInKHqM+DX1j5v3jzGjx/P4cOH6dSpE88//3yN+6Wnp1NQUIBzjoEDB5KQkEB8fDyFhYX07NkT5xyxsbFkZGTUe1/8pNm3RUTOULjPgK/Zt0VEGjnNgF8/ziqIzGyEmW00s2/MLLnauslmttXM8s0sNah9sNe21cwmnc33FxHx07DEOP7r9u7EtYnGCJwJ/dft3TUR8Wk62/eIPgFuB6p8wsrMugKjgGuAdsB7Znalt/qPwPeBncBaM1vsnNt0lnWIiPhCM+CfvbMKIufcZgAzq75qKPCKc+4osN3MtgK9vXVbnXOfefu94m2rIBIROUeF6j2iOOCLoOc7vbba2k9gZmPNLNvMsvfu3RuiMkVExG91nhGZ2XvAt2tY9ahz7o3adquhzVFz8NU4bM85NweYA4FRc3XVKSIikanOIHLO3XQGx90JtA96fhlQ7C3X1i4iIuegUF2aWwyMMrPzzKwj0BlYA6wFOptZRzNrTmBAw+IQ1SAiIhHgrAYrmNltwEwgFnjLzHKdc6nOuY1m9hqBQQhlwAPOuXJvnwnAEiAKmOuc23hWPRARkYimmRVERBopzawgIiJyChREIiLiKwWRiIj4SkEkIiK+UhCJiIivFEQiIuIrBZGIiPhKQSQiIr5SEImIiK8URCIi4isFkYiI+EpBJCIivlIQiYiIrxREIiLiKwWRiIj4SkEkIiK+UhCJiIivFEQiIVJYWEi3bt1C/n06dOjAl19+GfLvIxIqCiIRH5WXl/tdgojvFEQiIVRWVkZaWhrx8fEMHz6cw4cP06FDB6ZNm0ZKSgoLFixg27ZtDB48mKSkJPr378+WLVsA+Otf/0qfPn1ITEzkpptuYvfu3QD885//ZNCgQSQmJjJu3Dicc352UeSsKYhEQig/P5+xY8eSl5fHBRdcwNNPPw1AixYtyMrKYtSoUYwdO5aZM2eybt06nnzySe6//34AUlJSWL16NTk5OYwaNYonnngCgKlTp5KSkkJOTg5Dhgzh888/961/IvWhqd8FiDQmGTlFTF+ST3FJKRe6A8R8ux39+vUD4K677mLGjBkAjBw5EoCDBw/y0UcfMWLEiMpjHD16FICdO3cycuRIdu3axbFjx+jYsSMAmZmZLFq0CIAf/vCHtG3btsH6JxIKOiMSqScZOUVMXrSBopJSHLD7qyOUHC4jI6eochszA6Bly5YAfPPNN7Rp04bc3NzKx+bNmwH46U9/yoQJE9iwYQOzZ8/myJEjJxxHpDFQEInUk+lL8ik9XnXwQdlXe/j5nMDZy8svv0xKSkqV9RdccAEdO3ZkwYIFADjnWL9+PQAHDhwgLi4OgHnz5lXuM2DAAObPnw/AO++8w/79+0PTIZEGYpHwRqeZ7QV2eE9jgMYyVlV9CU9n1Jfm3/5uUvBzV3ac4/uLadI8mm9KvyoFjgLbgWuAzUBZxa7AFUAzwIB9wC6gDdAeOAYcAloC+UAU0InApfWD3nbBx6uX/oQp9eX0XOGciw3x9zhrERFEwcws2zmX7Hcd9UF9CU+NqS/QuPqjvjROujQnIiK+UhCJiIivIjGI5vhdQD1SX8JTY+oLNK7+qC+NUMS9RyQiIo1LJJ4RiYhII6IgEhERX4VtEJnZdDPbYmZ5ZvYXM2sTtG6ymW01s3wzSw1qH+y1bTWzSf5UXjMzG2FmG83sGzNLDmrvYGalZpbrPWYFrUsysw1ef2ZYmHycvra+eOsi7rWpYGZTzKwo6LWBzNAtAAAD0UlEQVS4OWhdjf0KZ5HwMz8ZMyv0/v3nmlm213ahmb1rZgXe17Cd38jM5prZHjP7JKitxvotYIb3WuWZWU//KveBcy4sH8AgoKm3/BvgN95yV2A9cB7QEdhG4AN+Ud5yJwIfEFwPdPW7H0H9uRroAiwHkoPaOwCf1LLPGuBaAh9yfAf4gd/9qKMvEfnaBNU/BXikhvYa++V3vXX0JSJ+5nX0oRCIqdb2BDDJW55U8f9COD6AAUDP4N/v2uoHbvZ+xw3oC3zsd/0N+QjbMyLn3FLnXMUnxVcDl3nLQ4FXnHNHnXPbga1Ab++x1Tn3mXPuGPCKt21YcM5tds7ln+r2ZnYpcIFzbpUL/Et9ARgWsgJPw0n6EpGvzSmorV/hLNJ/5rUZClTMdzSPMPmdqIlzLpPALBnBaqt/KPCCC1gNtPH+DzgnhG0QVXMvgb8WAOKAL4LW7fTaamuPBB3NLMfMPjSz/l5bHIE+VIiE/jSG12aCd2lkbtBln0iqv0Ik1lydA5aa2TozG+u1XeKc2wXgfb3Yt+rOTG31N4bX64z5ehsIM3sP+HYNqx51zr3hbfMogTm05lfsVsP2jppDtUHHpp9Kf2qwC7jcOfdPM0sCMszsGmrvZ4M4w76E7WtT4WT9Ap4Bfkmgtl8CvyXwR5Cvr8UZisSaq+vnnCs2s4uBd81si98FhVBjeL3OmK9B5Jy76WTrzSwNuAUY6F2egsBfCu2DNrsMKPaWa2tvEHX1p5Z9jhKYDBPn3Doz2wZcSaCflwVt2qD9OZO+EMavTYVT7ZeZPQu86T09Wb/CVSTWXIVzrtj7usfM/kLgcuNuM7vUObfLu3S1x9ciT19t9Uf863U2wvbSnJkNBv4PMMQ5dzho1WJglJmdZ2Ydgc4E3tRfC3Q2s45m1hwY5W0b1sws1syivOVOBPrzmXfa/rWZ9fVGy40GajsTCRcR/dpUuyZ/G1Ax2qm2foWziPiZ18bMWprZ+RXLBAYvfUKgD2neZmmE/+9EdbXVvxgY7Y2e6wscqLiEd07we7REbQ8Cbwh/AeR6j1lB6x4lMCIon6CRZARGnnzqrXvU7z5U689tBP7qOQrsBpZ47T8CNhIY1fR34NagfZIJ/PJtA/6ANxOG34/a+hKpr01QjX8GNgB5BP5juLSufoXzIxJ+5iepvZP3O7He+/141Gu/CFgGFHhfL/S71pP04WUCl96Pe78v/15b/QQuzf3Re602EDQa9Vx4aIofERHxVdhemhMRkXODgkhERHylIBIREV8piERExFcKIhER8ZWCSEREfKUgEhERX/1/ijgdIjyfFOQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_coords = Y[:, 0]\n",
    "y_coords = Y[:, 1]\n",
    "# display scatter plot\n",
    "plt.scatter(x_coords, y_coords)\n",
    "\n",
    "for label, x, y in zip(some_words, x_coords, y_coords):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this is obviously terrible. It must not have found any association. Looking at the generated words, it doesn't make much sense so this isn't terribly surprising. Probably R-200 for embedding space is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
