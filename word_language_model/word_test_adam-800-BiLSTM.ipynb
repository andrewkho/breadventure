{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.onnx\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import model as rnn_model\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_seed = 1234\n",
    "args_temperature = 1.\n",
    "args_data = '../data/wikitext-2'\n",
    "args_model = 'BiLSTM'\n",
    "args_emsize = 650\n",
    "args_nhid = 650\n",
    "args_nlayers = 2\n",
    "args_clip = 0.25\n",
    "args_epochs = 40\n",
    "args_batch_size = 20\n",
    "args_bptt = 35\n",
    "args_dropout = 0.2\n",
    "args_log_interval = 200\n",
    "args_save = 'model_800_bilstm.pt'\n",
    "args_save_state = 'model_state_800_bilstm.pt'\n",
    "args_tied = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args_seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "corpus = data.Corpus(args_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, args_batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, BiLSTM can not have tied weights!\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = rnn_model.RNNModel(args_model, ntokens, args_emsize, args_nhid, args_nlayers, args_dropout, args_tied).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Use Adam optimizer\n",
    "###############################################################################\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args_bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args_batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args_bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % args_log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args_log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args_bptt, \n",
    "                elapsed * 1000 / args_log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | ms/batch 173.20 | loss  9.11 | ppl  9072.09\n",
      "| epoch   1 |   400/ 2983 batches | ms/batch 159.34 | loss  9.89 | ppl 19745.22\n",
      "| epoch   1 |   600/ 2983 batches | ms/batch 159.42 | loss 11.06 | ppl 63393.51\n",
      "| epoch   1 |   800/ 2983 batches | ms/batch 157.12 | loss 11.49 | ppl 98058.75\n",
      "| epoch   1 |  1000/ 2983 batches | ms/batch 160.20 | loss 11.43 | ppl 92060.07\n",
      "| epoch   1 |  1200/ 2983 batches | ms/batch 159.26 | loss 12.25 | ppl 208559.52\n",
      "| epoch   1 |  1400/ 2983 batches | ms/batch 157.40 | loss 13.28 | ppl 584499.78\n",
      "| epoch   1 |  1600/ 2983 batches | ms/batch 159.97 | loss 13.32 | ppl 611625.56\n",
      "| epoch   1 |  1800/ 2983 batches | ms/batch 167.79 | loss 12.95 | ppl 419242.55\n",
      "| epoch   1 |  2000/ 2983 batches | ms/batch 169.17 | loss 13.17 | ppl 525054.45\n",
      "| epoch   1 |  2200/ 2983 batches | ms/batch 157.24 | loss 13.37 | ppl 639367.40\n",
      "| epoch   1 |  2400/ 2983 batches | ms/batch 157.13 | loss 13.90 | ppl 1089058.14\n",
      "| epoch   1 |  2600/ 2983 batches | ms/batch 157.17 | loss 13.63 | ppl 831122.26\n",
      "| epoch   1 |  2800/ 2983 batches | ms/batch 160.96 | loss 13.27 | ppl 576824.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 497.68s | valid loss 12.56 | valid ppl 283833.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | ms/batch 158.88 | loss  9.28 | ppl 10679.43\n",
      "| epoch   2 |   400/ 2983 batches | ms/batch 158.46 | loss  7.96 | ppl  2876.52\n",
      "| epoch   2 |   600/ 2983 batches | ms/batch 158.27 | loss  8.06 | ppl  3155.43\n",
      "| epoch   2 |   800/ 2983 batches | ms/batch 158.39 | loss  7.93 | ppl  2776.10\n",
      "| epoch   2 |  1000/ 2983 batches | ms/batch 158.46 | loss  7.81 | ppl  2453.14\n",
      "| epoch   2 |  1200/ 2983 batches | ms/batch 158.92 | loss  7.68 | ppl  2171.80\n",
      "| epoch   2 |  1400/ 2983 batches | ms/batch 158.47 | loss  7.43 | ppl  1681.88\n",
      "| epoch   2 |  1600/ 2983 batches | ms/batch 158.41 | loss  7.39 | ppl  1620.56\n",
      "| epoch   2 |  1800/ 2983 batches | ms/batch 158.45 | loss  7.16 | ppl  1286.03\n",
      "| epoch   2 |  2000/ 2983 batches | ms/batch 158.89 | loss  7.20 | ppl  1338.91\n",
      "| epoch   2 |  2200/ 2983 batches | ms/batch 158.45 | loss  7.14 | ppl  1265.22\n",
      "| epoch   2 |  2400/ 2983 batches | ms/batch 159.13 | loss  7.03 | ppl  1126.53\n",
      "| epoch   2 |  2600/ 2983 batches | ms/batch 158.70 | loss  6.97 | ppl  1068.01\n",
      "| epoch   2 |  2800/ 2983 batches | ms/batch 158.75 | loss  6.83 | ppl   924.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 490.14s | valid loss  6.06 | valid ppl   427.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | ms/batch 160.31 | loss  7.05 | ppl  1155.17\n",
      "| epoch   3 |   400/ 2983 batches | ms/batch 158.71 | loss  6.89 | ppl   982.37\n",
      "| epoch   3 |   600/ 2983 batches | ms/batch 158.47 | loss  6.73 | ppl   833.36\n",
      "| epoch   3 |   800/ 2983 batches | ms/batch 158.45 | loss  6.68 | ppl   796.06\n",
      "| epoch   3 |  1000/ 2983 batches | ms/batch 158.66 | loss  6.58 | ppl   722.12\n",
      "| epoch   3 |  1200/ 2983 batches | ms/batch 158.83 | loss  6.56 | ppl   706.57\n",
      "| epoch   3 |  1400/ 2983 batches | ms/batch 158.55 | loss  6.51 | ppl   669.05\n",
      "| epoch   3 |  1600/ 2983 batches | ms/batch 158.52 | loss  6.57 | ppl   716.45\n",
      "| epoch   3 |  1800/ 2983 batches | ms/batch 158.79 | loss  6.27 | ppl   529.94\n",
      "| epoch   3 |  2000/ 2983 batches | ms/batch 158.74 | loss  6.28 | ppl   534.62\n",
      "| epoch   3 |  2200/ 2983 batches | ms/batch 158.48 | loss  6.27 | ppl   530.65\n",
      "| epoch   3 |  2400/ 2983 batches | ms/batch 158.43 | loss  6.26 | ppl   524.65\n",
      "| epoch   3 |  2600/ 2983 batches | ms/batch 158.72 | loss  6.26 | ppl   523.40\n",
      "| epoch   3 |  2800/ 2983 batches | ms/batch 158.86 | loss  6.12 | ppl   453.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 490.51s | valid loss  5.36 | valid ppl   212.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | ms/batch 159.17 | loss  6.43 | ppl   619.13\n",
      "| epoch   4 |   400/ 2983 batches | ms/batch 158.72 | loss  6.36 | ppl   577.33\n",
      "| epoch   4 |   600/ 2983 batches | ms/batch 158.38 | loss  6.19 | ppl   485.49\n",
      "| epoch   4 |   800/ 2983 batches | ms/batch 158.36 | loss  6.14 | ppl   461.84\n",
      "| epoch   4 |  1000/ 2983 batches | ms/batch 158.64 | loss  6.04 | ppl   421.66\n",
      "| epoch   4 |  1200/ 2983 batches | ms/batch 158.82 | loss  6.08 | ppl   439.00\n",
      "| epoch   4 |  1400/ 2983 batches | ms/batch 158.51 | loss  6.05 | ppl   422.46\n",
      "| epoch   4 |  1600/ 2983 batches | ms/batch 158.45 | loss  6.08 | ppl   436.71\n",
      "| epoch   4 |  1800/ 2983 batches | ms/batch 158.94 | loss  5.84 | ppl   344.70\n",
      "| epoch   4 |  2000/ 2983 batches | ms/batch 158.58 | loss  5.88 | ppl   359.31\n",
      "| epoch   4 |  2200/ 2983 batches | ms/batch 158.45 | loss  5.87 | ppl   353.15\n",
      "| epoch   4 |  2400/ 2983 batches | ms/batch 158.64 | loss  5.87 | ppl   353.30\n",
      "| epoch   4 |  2600/ 2983 batches | ms/batch 158.75 | loss  5.87 | ppl   355.99\n",
      "| epoch   4 |  2800/ 2983 batches | ms/batch 158.41 | loss  5.78 | ppl   322.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 489.83s | valid loss  5.09 | valid ppl   161.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | ms/batch 157.57 | loss  5.95 | ppl   382.85\n",
      "| epoch   5 |   400/ 2983 batches | ms/batch 156.97 | loss  5.86 | ppl   350.19\n",
      "| epoch   5 |   600/ 2983 batches | ms/batch 156.95 | loss  5.79 | ppl   325.86\n",
      "| epoch   5 |   800/ 2983 batches | ms/batch 157.05 | loss  5.79 | ppl   326.99\n",
      "| epoch   5 |  1000/ 2983 batches | ms/batch 157.03 | loss  5.67 | ppl   288.72\n",
      "| epoch   5 |  1200/ 2983 batches | ms/batch 157.04 | loss  5.75 | ppl   314.79\n",
      "| epoch   5 |  1400/ 2983 batches | ms/batch 157.08 | loss  5.71 | ppl   303.13\n",
      "| epoch   5 |  1600/ 2983 batches | ms/batch 157.04 | loss  5.79 | ppl   326.63\n",
      "| epoch   5 |  1800/ 2983 batches | ms/batch 157.04 | loss  5.58 | ppl   265.31\n",
      "| epoch   5 |  2000/ 2983 batches | ms/batch 157.04 | loss  5.64 | ppl   280.59\n",
      "| epoch   5 |  2200/ 2983 batches | ms/batch 157.03 | loss  5.64 | ppl   282.78\n",
      "| epoch   5 |  2400/ 2983 batches | ms/batch 157.07 | loss  5.60 | ppl   269.76\n",
      "| epoch   5 |  2600/ 2983 batches | ms/batch 156.96 | loss  5.62 | ppl   274.65\n",
      "| epoch   5 |  2800/ 2983 batches | ms/batch 156.97 | loss  5.53 | ppl   252.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 485.35s | valid loss  4.90 | valid ppl   134.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | ms/batch 157.69 | loss  5.57 | ppl   262.56\n",
      "| epoch   6 |   400/ 2983 batches | ms/batch 159.19 | loss  5.67 | ppl   290.53\n",
      "| epoch   6 |   600/ 2983 batches | ms/batch 166.08 | loss  5.60 | ppl   269.22\n",
      "| epoch   6 |   800/ 2983 batches | ms/batch 161.14 | loss  5.53 | ppl   252.02\n",
      "| epoch   6 |  1000/ 2983 batches | ms/batch 164.38 | loss  5.45 | ppl   233.39\n",
      "| epoch   6 |  1200/ 2983 batches | ms/batch 164.02 | loss  5.50 | ppl   244.06\n",
      "| epoch   6 |  1400/ 2983 batches | ms/batch 167.45 | loss  5.50 | ppl   245.78\n",
      "| epoch   6 |  1600/ 2983 batches | ms/batch 167.71 | loss  5.57 | ppl   261.53\n",
      "| epoch   6 |  1800/ 2983 batches | ms/batch 158.03 | loss  5.37 | ppl   214.67\n",
      "| epoch   6 |  2000/ 2983 batches | ms/batch 157.73 | loss  5.42 | ppl   226.45\n",
      "| epoch   6 |  2200/ 2983 batches | ms/batch 158.02 | loss  5.39 | ppl   219.86\n",
      "| epoch   6 |  2400/ 2983 batches | ms/batch 157.96 | loss  5.41 | ppl   224.68\n",
      "| epoch   6 |  2600/ 2983 batches | ms/batch 157.83 | loss  5.45 | ppl   233.57\n",
      "| epoch   6 |  2800/ 2983 batches | ms/batch 157.69 | loss  5.39 | ppl   218.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 496.86s | valid loss  4.66 | valid ppl   105.35\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |   200/ 2983 batches | ms/batch 158.05 | loss  5.25 | ppl   190.75\n",
      "| epoch   7 |   400/ 2983 batches | ms/batch 157.40 | loss  5.30 | ppl   200.32\n",
      "| epoch   7 |   600/ 2983 batches | ms/batch 157.59 | loss  5.31 | ppl   203.34\n",
      "| epoch   7 |   800/ 2983 batches | ms/batch 157.91 | loss  5.30 | ppl   200.47\n",
      "| epoch   7 |  1000/ 2983 batches | ms/batch 157.40 | loss  5.26 | ppl   192.83\n",
      "| epoch   7 |  1200/ 2983 batches | ms/batch 157.44 | loss  5.35 | ppl   209.63\n",
      "| epoch   7 |  1400/ 2983 batches | ms/batch 157.74 | loss  5.35 | ppl   210.28\n",
      "| epoch   7 |  1600/ 2983 batches | ms/batch 157.48 | loss  5.47 | ppl   238.64\n",
      "| epoch   7 |  1800/ 2983 batches | ms/batch 157.40 | loss  5.26 | ppl   192.30\n",
      "| epoch   7 |  2000/ 2983 batches | ms/batch 157.32 | loss  5.31 | ppl   201.78\n",
      "| epoch   7 |  2200/ 2983 batches | ms/batch 157.84 | loss  5.28 | ppl   197.30\n",
      "| epoch   7 |  2400/ 2983 batches | ms/batch 157.40 | loss  5.23 | ppl   186.93\n",
      "| epoch   7 |  2600/ 2983 batches | ms/batch 157.37 | loss  5.25 | ppl   189.73\n",
      "| epoch   7 |  2800/ 2983 batches | ms/batch 157.40 | loss  5.21 | ppl   183.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 486.88s | valid loss  4.56 | valid ppl    95.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2983 batches | ms/batch 158.13 | loss  5.26 | ppl   192.62\n",
      "| epoch   8 |   400/ 2983 batches | ms/batch 157.33 | loss  5.27 | ppl   194.77\n",
      "| epoch   8 |   600/ 2983 batches | ms/batch 157.86 | loss  5.17 | ppl   175.44\n",
      "| epoch   8 |   800/ 2983 batches | ms/batch 157.38 | loss  5.17 | ppl   175.33\n",
      "| epoch   8 |  1000/ 2983 batches | ms/batch 157.36 | loss  5.10 | ppl   164.15\n",
      "| epoch   8 |  1200/ 2983 batches | ms/batch 157.36 | loss  5.19 | ppl   178.60\n",
      "| epoch   8 |  1400/ 2983 batches | ms/batch 156.98 | loss  5.18 | ppl   177.74\n",
      "| epoch   8 |  1600/ 2983 batches | ms/batch 157.04 | loss  5.21 | ppl   183.28\n",
      "| epoch   8 |  1800/ 2983 batches | ms/batch 156.99 | loss  5.02 | ppl   150.66\n",
      "| epoch   8 |  2000/ 2983 batches | ms/batch 156.96 | loss  5.09 | ppl   162.49\n",
      "| epoch   8 |  2200/ 2983 batches | ms/batch 156.97 | loss  5.12 | ppl   166.68\n",
      "| epoch   8 |  2400/ 2983 batches | ms/batch 157.00 | loss  5.09 | ppl   162.85\n",
      "| epoch   8 |  2600/ 2983 batches | ms/batch 156.99 | loss  5.10 | ppl   164.66\n",
      "| epoch   8 |  2800/ 2983 batches | ms/batch 156.95 | loss  5.02 | ppl   152.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 487.38s | valid loss  4.40 | valid ppl    81.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2983 batches | ms/batch 160.66 | loss  4.97 | ppl   144.36\n",
      "| epoch   9 |   400/ 2983 batches | ms/batch 157.83 | loss  4.93 | ppl   138.47\n",
      "| epoch   9 |   600/ 2983 batches | ms/batch 157.69 | loss  4.86 | ppl   128.43\n",
      "| epoch   9 |   800/ 2983 batches | ms/batch 159.37 | loss  4.84 | ppl   127.05\n",
      "| epoch   9 |  1000/ 2983 batches | ms/batch 158.16 | loss  4.82 | ppl   123.62\n",
      "| epoch   9 |  1200/ 2983 batches | ms/batch 157.64 | loss  4.93 | ppl   139.00\n",
      "| epoch   9 |  1400/ 2983 batches | ms/batch 159.38 | loss  4.90 | ppl   134.80\n",
      "| epoch   9 |  1600/ 2983 batches | ms/batch 159.54 | loss  4.93 | ppl   138.75\n",
      "| epoch   9 |  1800/ 2983 batches | ms/batch 163.35 | loss  4.75 | ppl   115.84\n",
      "| epoch   9 |  2000/ 2983 batches | ms/batch 161.81 | loss  4.83 | ppl   125.47\n",
      "| epoch   9 |  2200/ 2983 batches | ms/batch 165.42 | loss  4.80 | ppl   121.26\n",
      "| epoch   9 |  2400/ 2983 batches | ms/batch 169.95 | loss  4.80 | ppl   121.44\n",
      "| epoch   9 |  2600/ 2983 batches | ms/batch 158.49 | loss  4.81 | ppl   123.26\n",
      "| epoch   9 |  2800/ 2983 batches | ms/batch 162.54 | loss  4.79 | ppl   120.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 496.32s | valid loss  4.21 | valid ppl    67.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2983 batches | ms/batch 170.53 | loss  4.69 | ppl   109.11\n",
      "| epoch  10 |   400/ 2983 batches | ms/batch 161.63 | loss  4.69 | ppl   108.35\n",
      "| epoch  10 |   600/ 2983 batches | ms/batch 161.17 | loss  4.60 | ppl    99.69\n",
      "| epoch  10 |   800/ 2983 batches | ms/batch 161.14 | loss  4.60 | ppl    99.51\n",
      "| epoch  10 |  1000/ 2983 batches | ms/batch 168.60 | loss  4.52 | ppl    91.85\n",
      "| epoch  10 |  1200/ 2983 batches | ms/batch 169.07 | loss  4.63 | ppl   102.65\n",
      "| epoch  10 |  1400/ 2983 batches | ms/batch 164.41 | loss  4.68 | ppl   107.24\n",
      "| epoch  10 |  1600/ 2983 batches | ms/batch 165.97 | loss  4.74 | ppl   114.27\n",
      "| epoch  10 |  1800/ 2983 batches | ms/batch 160.96 | loss  4.55 | ppl    94.76\n",
      "| epoch  10 |  2000/ 2983 batches | ms/batch 171.36 | loss  4.61 | ppl   100.75\n",
      "| epoch  10 |  2200/ 2983 batches | ms/batch 174.97 | loss  4.60 | ppl    99.05\n",
      "| epoch  10 |  2400/ 2983 batches | ms/batch 158.69 | loss  4.58 | ppl    97.95\n",
      "| epoch  10 |  2600/ 2983 batches | ms/batch 158.14 | loss  4.62 | ppl   101.48\n",
      "| epoch  10 |  2800/ 2983 batches | ms/batch 157.61 | loss  4.54 | ppl    93.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 506.53s | valid loss  4.04 | valid ppl    56.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 2983 batches | ms/batch 158.10 | loss  4.51 | ppl    90.74\n",
      "| epoch  11 |   400/ 2983 batches | ms/batch 157.40 | loss  4.51 | ppl    90.84\n",
      "| epoch  11 |   600/ 2983 batches | ms/batch 157.34 | loss  4.45 | ppl    85.43\n",
      "| epoch  11 |   800/ 2983 batches | ms/batch 157.36 | loss  4.43 | ppl    84.14\n",
      "| epoch  11 |  1000/ 2983 batches | ms/batch 157.37 | loss  4.39 | ppl    80.28\n",
      "| epoch  11 |  1200/ 2983 batches | ms/batch 157.48 | loss  4.48 | ppl    87.99\n",
      "| epoch  11 |  1400/ 2983 batches | ms/batch 157.37 | loss  4.49 | ppl    89.25\n",
      "| epoch  11 |  1600/ 2983 batches | ms/batch 157.37 | loss  4.54 | ppl    93.75\n",
      "| epoch  11 |  1800/ 2983 batches | ms/batch 157.40 | loss  4.37 | ppl    78.84\n",
      "| epoch  11 |  2000/ 2983 batches | ms/batch 157.33 | loss  4.46 | ppl    86.44\n",
      "| epoch  11 |  2200/ 2983 batches | ms/batch 157.44 | loss  4.41 | ppl    82.25\n",
      "| epoch  11 |  2400/ 2983 batches | ms/batch 157.32 | loss  4.43 | ppl    83.89\n",
      "| epoch  11 |  2600/ 2983 batches | ms/batch 157.33 | loss  4.42 | ppl    83.14\n",
      "| epoch  11 |  2800/ 2983 batches | ms/batch 157.33 | loss  4.37 | ppl    78.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 486.45s | valid loss  3.89 | valid ppl    48.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 2983 batches | ms/batch 158.01 | loss  4.32 | ppl    75.15\n",
      "| epoch  12 |   400/ 2983 batches | ms/batch 157.31 | loss  4.28 | ppl    72.17\n",
      "| epoch  12 |   600/ 2983 batches | ms/batch 157.32 | loss  4.20 | ppl    66.78\n",
      "| epoch  12 |   800/ 2983 batches | ms/batch 157.38 | loss  4.21 | ppl    67.57\n",
      "| epoch  12 |  1000/ 2983 batches | ms/batch 157.38 | loss  4.15 | ppl    63.45\n",
      "| epoch  12 |  1200/ 2983 batches | ms/batch 157.32 | loss  4.26 | ppl    70.74\n",
      "| epoch  12 |  1400/ 2983 batches | ms/batch 157.29 | loss  4.29 | ppl    72.99\n",
      "| epoch  12 |  1600/ 2983 batches | ms/batch 157.32 | loss  4.34 | ppl    76.88\n",
      "| epoch  12 |  1800/ 2983 batches | ms/batch 157.32 | loss  4.17 | ppl    65.02\n",
      "| epoch  12 |  2000/ 2983 batches | ms/batch 157.43 | loss  4.25 | ppl    70.10\n",
      "| epoch  12 |  2200/ 2983 batches | ms/batch 157.25 | loss  4.19 | ppl    66.12\n",
      "| epoch  12 |  2400/ 2983 batches | ms/batch 157.05 | loss  4.21 | ppl    67.14\n",
      "| epoch  12 |  2600/ 2983 batches | ms/batch 157.01 | loss  4.20 | ppl    66.62\n",
      "| epoch  12 |  2800/ 2983 batches | ms/batch 157.02 | loss  4.16 | ppl    64.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 487.64s | valid loss  3.80 | valid ppl    44.89\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  13 |   200/ 2983 batches | ms/batch 165.70 | loss  4.19 | ppl    65.85\n",
      "| epoch  13 |   400/ 2983 batches | ms/batch 168.01 | loss  4.24 | ppl    69.63\n",
      "| epoch  13 |   600/ 2983 batches | ms/batch 159.91 | loss  4.16 | ppl    63.82\n",
      "| epoch  13 |   800/ 2983 batches | ms/batch 163.03 | loss  4.17 | ppl    64.46\n",
      "| epoch  13 |  1000/ 2983 batches | ms/batch 163.91 | loss  4.16 | ppl    64.34\n",
      "| epoch  13 |  1200/ 2983 batches | ms/batch 165.64 | loss  4.24 | ppl    69.43\n",
      "| epoch  13 |  1400/ 2983 batches | ms/batch 161.48 | loss  4.25 | ppl    69.80\n",
      "| epoch  13 |  1600/ 2983 batches | ms/batch 157.45 | loss  4.28 | ppl    72.12\n",
      "| epoch  13 |  1800/ 2983 batches | ms/batch 157.41 | loss  4.15 | ppl    63.54\n",
      "| epoch  13 |  2000/ 2983 batches | ms/batch 162.31 | loss  4.21 | ppl    67.61\n",
      "| epoch  13 |  2200/ 2983 batches | ms/batch 158.75 | loss  4.17 | ppl    64.59\n",
      "| epoch  13 |  2400/ 2983 batches | ms/batch 164.72 | loss  4.16 | ppl    64.05\n",
      "| epoch  13 |  2600/ 2983 batches | ms/batch 162.18 | loss  4.15 | ppl    63.50\n",
      "| epoch  13 |  2800/ 2983 batches | ms/batch 166.81 | loss  4.09 | ppl    59.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 503.27s | valid loss  3.74 | valid ppl    42.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 2983 batches | ms/batch 169.33 | loss  4.13 | ppl    62.03\n",
      "| epoch  14 |   400/ 2983 batches | ms/batch 167.31 | loss  4.09 | ppl    59.65\n",
      "| epoch  14 |   600/ 2983 batches | ms/batch 168.59 | loss  4.05 | ppl    57.32\n",
      "| epoch  14 |   800/ 2983 batches | ms/batch 162.76 | loss  4.04 | ppl    56.55\n",
      "| epoch  14 |  1000/ 2983 batches | ms/batch 163.08 | loss  3.96 | ppl    52.43\n",
      "| epoch  14 |  1200/ 2983 batches | ms/batch 164.03 | loss  4.02 | ppl    55.73\n",
      "| epoch  14 |  1400/ 2983 batches | ms/batch 168.22 | loss  4.05 | ppl    57.32\n",
      "| epoch  14 |  1600/ 2983 batches | ms/batch 164.94 | loss  4.07 | ppl    58.41\n",
      "| epoch  14 |  1800/ 2983 batches | ms/batch 161.21 | loss  3.92 | ppl    50.21\n",
      "| epoch  14 |  2000/ 2983 batches | ms/batch 181.70 | loss  3.95 | ppl    51.85\n",
      "| epoch  14 |  2200/ 2983 batches | ms/batch 170.13 | loss  3.94 | ppl    51.19\n",
      "| epoch  14 |  2400/ 2983 batches | ms/batch 172.95 | loss  3.96 | ppl    52.67\n",
      "| epoch  14 |  2600/ 2983 batches | ms/batch 160.46 | loss  4.01 | ppl    55.08\n",
      "| epoch  14 |  2800/ 2983 batches | ms/batch 157.41 | loss  3.94 | ppl    51.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 512.12s | valid loss  3.56 | valid ppl    35.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 2983 batches | ms/batch 158.12 | loss  3.97 | ppl    52.86\n",
      "| epoch  15 |   400/ 2983 batches | ms/batch 157.33 | loss  3.99 | ppl    53.92\n",
      "| epoch  15 |   600/ 2983 batches | ms/batch 157.32 | loss  3.93 | ppl    50.74\n",
      "| epoch  15 |   800/ 2983 batches | ms/batch 157.37 | loss  3.91 | ppl    49.70\n",
      "| epoch  15 |  1000/ 2983 batches | ms/batch 157.53 | loss  3.86 | ppl    47.68\n",
      "| epoch  15 |  1200/ 2983 batches | ms/batch 159.23 | loss  3.94 | ppl    51.17\n",
      "| epoch  15 |  1400/ 2983 batches | ms/batch 157.40 | loss  3.95 | ppl    52.06\n",
      "| epoch  15 |  1600/ 2983 batches | ms/batch 157.40 | loss  3.99 | ppl    54.02\n",
      "| epoch  15 |  1800/ 2983 batches | ms/batch 157.57 | loss  3.88 | ppl    48.34\n",
      "| epoch  15 |  2000/ 2983 batches | ms/batch 157.43 | loss  3.97 | ppl    52.87\n",
      "| epoch  15 |  2200/ 2983 batches | ms/batch 157.37 | loss  3.95 | ppl    52.08\n",
      "| epoch  15 |  2400/ 2983 batches | ms/batch 157.40 | loss  3.94 | ppl    51.41\n",
      "| epoch  15 |  2600/ 2983 batches | ms/batch 157.36 | loss  3.95 | ppl    51.78\n",
      "| epoch  15 |  2800/ 2983 batches | ms/batch 157.56 | loss  3.89 | ppl    48.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 486.96s | valid loss  3.57 | valid ppl    35.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 2983 batches | ms/batch 158.16 | loss  3.90 | ppl    49.50\n",
      "| epoch  16 |   400/ 2983 batches | ms/batch 157.37 | loss  3.84 | ppl    46.70\n",
      "| epoch  16 |   600/ 2983 batches | ms/batch 157.51 | loss  3.78 | ppl    43.96\n",
      "| epoch  16 |   800/ 2983 batches | ms/batch 157.42 | loss  3.80 | ppl    44.90\n",
      "| epoch  16 |  1000/ 2983 batches | ms/batch 157.32 | loss  3.74 | ppl    42.11\n",
      "| epoch  16 |  1200/ 2983 batches | ms/batch 157.36 | loss  3.82 | ppl    45.45\n",
      "| epoch  16 |  1400/ 2983 batches | ms/batch 157.36 | loss  3.87 | ppl    48.08\n",
      "| epoch  16 |  1600/ 2983 batches | ms/batch 157.48 | loss  3.87 | ppl    48.17\n",
      "| epoch  16 |  1800/ 2983 batches | ms/batch 157.33 | loss  3.76 | ppl    43.10\n",
      "| epoch  16 |  2000/ 2983 batches | ms/batch 157.33 | loss  3.82 | ppl    45.68\n",
      "| epoch  16 |  2200/ 2983 batches | ms/batch 157.33 | loss  3.80 | ppl    44.75\n",
      "| epoch  16 |  2400/ 2983 batches | ms/batch 157.36 | loss  3.79 | ppl    44.18\n",
      "| epoch  16 |  2600/ 2983 batches | ms/batch 157.49 | loss  3.82 | ppl    45.67\n",
      "| epoch  16 |  2800/ 2983 batches | ms/batch 157.44 | loss  3.78 | ppl    43.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 486.49s | valid loss  3.54 | valid ppl    34.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 2983 batches | ms/batch 158.07 | loss  3.79 | ppl    44.46\n",
      "| epoch  17 |   400/ 2983 batches | ms/batch 157.51 | loss  3.77 | ppl    43.51\n",
      "| epoch  17 |   600/ 2983 batches | ms/batch 157.46 | loss  3.70 | ppl    40.28\n",
      "| epoch  17 |   800/ 2983 batches | ms/batch 157.07 | loss  3.72 | ppl    41.14\n",
      "| epoch  17 |  1000/ 2983 batches | ms/batch 157.01 | loss  3.64 | ppl    38.24\n",
      "| epoch  17 |  1200/ 2983 batches | ms/batch 157.01 | loss  3.71 | ppl    40.95\n",
      "| epoch  17 |  1400/ 2983 batches | ms/batch 156.99 | loss  3.78 | ppl    43.82\n",
      "| epoch  17 |  1600/ 2983 batches | ms/batch 157.10 | loss  3.81 | ppl    45.19\n",
      "| epoch  17 |  1800/ 2983 batches | ms/batch 156.98 | loss  3.69 | ppl    40.05\n",
      "| epoch  17 |  2000/ 2983 batches | ms/batch 157.08 | loss  3.75 | ppl    42.49\n",
      "| epoch  17 |  2200/ 2983 batches | ms/batch 157.03 | loss  3.76 | ppl    43.01\n",
      "| epoch  17 |  2400/ 2983 batches | ms/batch 156.97 | loss  3.77 | ppl    43.39\n",
      "| epoch  17 |  2600/ 2983 batches | ms/batch 157.00 | loss  3.82 | ppl    45.42\n",
      "| epoch  17 |  2800/ 2983 batches | ms/batch 156.96 | loss  3.78 | ppl    43.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 485.65s | valid loss  3.52 | valid ppl    33.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 2983 batches | ms/batch 157.78 | loss  3.75 | ppl    42.59\n",
      "| epoch  18 |   400/ 2983 batches | ms/batch 156.98 | loss  3.73 | ppl    41.71\n",
      "| epoch  18 |   600/ 2983 batches | ms/batch 156.98 | loss  3.68 | ppl    39.68\n",
      "| epoch  18 |   800/ 2983 batches | ms/batch 157.01 | loss  3.75 | ppl    42.51\n",
      "| epoch  18 |  1000/ 2983 batches | ms/batch 157.00 | loss  3.70 | ppl    40.46\n",
      "| epoch  18 |  1200/ 2983 batches | ms/batch 156.98 | loss  3.74 | ppl    42.14\n",
      "| epoch  18 |  1400/ 2983 batches | ms/batch 157.02 | loss  3.79 | ppl    44.44\n",
      "| epoch  18 |  1600/ 2983 batches | ms/batch 156.97 | loss  3.84 | ppl    46.33\n",
      "| epoch  18 |  1800/ 2983 batches | ms/batch 156.99 | loss  3.71 | ppl    40.68\n",
      "| epoch  18 |  2000/ 2983 batches | ms/batch 156.93 | loss  3.76 | ppl    42.99\n",
      "| epoch  18 |  2200/ 2983 batches | ms/batch 157.12 | loss  3.73 | ppl    41.48\n",
      "| epoch  18 |  2400/ 2983 batches | ms/batch 156.97 | loss  3.73 | ppl    41.87\n",
      "| epoch  18 |  2600/ 2983 batches | ms/batch 156.99 | loss  3.76 | ppl    42.99\n",
      "| epoch  18 |  2800/ 2983 batches | ms/batch 156.98 | loss  3.68 | ppl    39.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 485.30s | valid loss  3.57 | valid ppl    35.45\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  19 |   200/ 2983 batches | ms/batch 157.68 | loss  3.80 | ppl    44.74\n",
      "| epoch  19 |   400/ 2983 batches | ms/batch 156.93 | loss  3.82 | ppl    45.45\n",
      "| epoch  19 |   600/ 2983 batches | ms/batch 156.94 | loss  3.78 | ppl    43.66\n",
      "| epoch  19 |   800/ 2983 batches | ms/batch 156.99 | loss  3.77 | ppl    43.49\n",
      "| epoch  19 |  1000/ 2983 batches | ms/batch 156.98 | loss  3.74 | ppl    42.01\n",
      "| epoch  19 |  1200/ 2983 batches | ms/batch 156.94 | loss  3.77 | ppl    43.41\n",
      "| epoch  19 |  1400/ 2983 batches | ms/batch 156.92 | loss  3.78 | ppl    43.70\n",
      "| epoch  19 |  1600/ 2983 batches | ms/batch 156.93 | loss  3.81 | ppl    45.24\n",
      "| epoch  19 |  1800/ 2983 batches | ms/batch 156.95 | loss  3.68 | ppl    39.71\n",
      "| epoch  19 |  2000/ 2983 batches | ms/batch 156.97 | loss  3.72 | ppl    41.44\n",
      "| epoch  19 |  2200/ 2983 batches | ms/batch 156.93 | loss  3.67 | ppl    39.37\n",
      "| epoch  19 |  2400/ 2983 batches | ms/batch 156.92 | loss  3.68 | ppl    39.61\n",
      "| epoch  19 |  2600/ 2983 batches | ms/batch 156.97 | loss  3.71 | ppl    40.68\n",
      "| epoch  19 |  2800/ 2983 batches | ms/batch 156.94 | loss  3.66 | ppl    38.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 485.18s | valid loss  3.48 | valid ppl    32.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 2983 batches | ms/batch 157.69 | loss  3.66 | ppl    39.02\n",
      "| epoch  20 |   400/ 2983 batches | ms/batch 156.88 | loss  3.67 | ppl    39.36\n",
      "| epoch  20 |   600/ 2983 batches | ms/batch 156.93 | loss  3.62 | ppl    37.50\n",
      "| epoch  20 |   800/ 2983 batches | ms/batch 156.89 | loss  3.66 | ppl    38.74\n",
      "| epoch  20 |  1000/ 2983 batches | ms/batch 156.88 | loss  3.61 | ppl    36.88\n",
      "| epoch  20 |  1200/ 2983 batches | ms/batch 156.96 | loss  3.69 | ppl    39.89\n",
      "| epoch  20 |  1400/ 2983 batches | ms/batch 156.96 | loss  3.72 | ppl    41.31\n",
      "| epoch  20 |  1600/ 2983 batches | ms/batch 156.95 | loss  3.72 | ppl    41.12\n",
      "| epoch  20 |  1800/ 2983 batches | ms/batch 156.88 | loss  3.57 | ppl    35.67\n",
      "| epoch  20 |  2000/ 2983 batches | ms/batch 156.93 | loss  3.63 | ppl    37.68\n",
      "| epoch  20 |  2200/ 2983 batches | ms/batch 156.86 | loss  3.61 | ppl    36.93\n",
      "| epoch  20 |  2400/ 2983 batches | ms/batch 156.89 | loss  3.61 | ppl    37.00\n",
      "| epoch  20 |  2600/ 2983 batches | ms/batch 156.92 | loss  3.63 | ppl    37.76\n",
      "| epoch  20 |  2800/ 2983 batches | ms/batch 157.02 | loss  3.56 | ppl    35.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 485.29s | valid loss  3.42 | valid ppl    30.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/ 2983 batches | ms/batch 157.81 | loss  3.54 | ppl    34.56\n",
      "| epoch  21 |   400/ 2983 batches | ms/batch 156.94 | loss  3.54 | ppl    34.38\n",
      "| epoch  21 |   600/ 2983 batches | ms/batch 156.95 | loss  3.48 | ppl    32.39\n",
      "| epoch  21 |   800/ 2983 batches | ms/batch 156.93 | loss  3.51 | ppl    33.45\n",
      "| epoch  21 |  1000/ 2983 batches | ms/batch 156.93 | loss  3.47 | ppl    32.22\n",
      "| epoch  21 |  1200/ 2983 batches | ms/batch 156.96 | loss  3.57 | ppl    35.41\n",
      "| epoch  21 |  1400/ 2983 batches | ms/batch 156.93 | loss  3.62 | ppl    37.23\n",
      "| epoch  21 |  1600/ 2983 batches | ms/batch 156.94 | loss  3.63 | ppl    37.85\n",
      "| epoch  21 |  1800/ 2983 batches | ms/batch 156.96 | loss  3.50 | ppl    33.11\n",
      "| epoch  21 |  2000/ 2983 batches | ms/batch 156.92 | loss  3.57 | ppl    35.46\n",
      "| epoch  21 |  2200/ 2983 batches | ms/batch 156.93 | loss  3.52 | ppl    33.75\n",
      "| epoch  21 |  2400/ 2983 batches | ms/batch 156.95 | loss  3.52 | ppl    33.69\n",
      "| epoch  21 |  2600/ 2983 batches | ms/batch 156.95 | loss  3.55 | ppl    34.74\n",
      "| epoch  21 |  2800/ 2983 batches | ms/batch 156.91 | loss  3.49 | ppl    32.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 485.18s | valid loss  3.31 | valid ppl    27.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/ 2983 batches | ms/batch 157.78 | loss  3.56 | ppl    35.18\n",
      "| epoch  22 |   400/ 2983 batches | ms/batch 156.95 | loss  3.54 | ppl    34.58\n",
      "| epoch  22 |   600/ 2983 batches | ms/batch 156.92 | loss  3.50 | ppl    33.04\n",
      "| epoch  22 |   800/ 2983 batches | ms/batch 156.96 | loss  3.49 | ppl    32.87\n",
      "| epoch  22 |  1000/ 2983 batches | ms/batch 156.95 | loss  3.47 | ppl    32.02\n",
      "| epoch  22 |  1200/ 2983 batches | ms/batch 156.92 | loss  3.56 | ppl    35.31\n",
      "| epoch  22 |  1400/ 2983 batches | ms/batch 156.96 | loss  3.63 | ppl    37.66\n",
      "| epoch  22 |  1600/ 2983 batches | ms/batch 156.90 | loss  3.63 | ppl    37.53\n",
      "| epoch  22 |  1800/ 2983 batches | ms/batch 156.94 | loss  3.50 | ppl    32.99\n",
      "| epoch  22 |  2000/ 2983 batches | ms/batch 156.97 | loss  3.55 | ppl    34.92\n",
      "| epoch  22 |  2200/ 2983 batches | ms/batch 156.94 | loss  3.53 | ppl    34.21\n",
      "| epoch  22 |  2400/ 2983 batches | ms/batch 156.93 | loss  3.54 | ppl    34.46\n",
      "| epoch  22 |  2600/ 2983 batches | ms/batch 156.95 | loss  3.54 | ppl    34.51\n",
      "| epoch  22 |  2800/ 2983 batches | ms/batch 156.97 | loss  3.49 | ppl    32.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 485.17s | valid loss  3.36 | valid ppl    28.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/ 2983 batches | ms/batch 157.76 | loss  3.37 | ppl    29.13\n",
      "| epoch  23 |   400/ 2983 batches | ms/batch 156.95 | loss  3.42 | ppl    30.42\n",
      "| epoch  23 |   600/ 2983 batches | ms/batch 156.92 | loss  3.38 | ppl    29.35\n",
      "| epoch  23 |   800/ 2983 batches | ms/batch 156.93 | loss  3.47 | ppl    32.04\n",
      "| epoch  23 |  1000/ 2983 batches | ms/batch 156.92 | loss  3.46 | ppl    31.80\n",
      "| epoch  23 |  1200/ 2983 batches | ms/batch 156.96 | loss  3.62 | ppl    37.19\n",
      "| epoch  23 |  1400/ 2983 batches | ms/batch 156.93 | loss  3.74 | ppl    42.09\n",
      "| epoch  23 |  1600/ 2983 batches | ms/batch 156.92 | loss  3.76 | ppl    42.91\n",
      "| epoch  23 |  1800/ 2983 batches | ms/batch 156.93 | loss  3.60 | ppl    36.64\n",
      "| epoch  23 |  2000/ 2983 batches | ms/batch 156.92 | loss  3.68 | ppl    39.48\n",
      "| epoch  23 |  2200/ 2983 batches | ms/batch 156.93 | loss  3.67 | ppl    39.30\n",
      "| epoch  23 |  2400/ 2983 batches | ms/batch 156.92 | loss  3.62 | ppl    37.49\n",
      "| epoch  23 |  2600/ 2983 batches | ms/batch 156.94 | loss  3.62 | ppl    37.48\n",
      "| epoch  23 |  2800/ 2983 batches | ms/batch 156.93 | loss  3.59 | ppl    36.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 485.14s | valid loss  3.38 | valid ppl    29.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/ 2983 batches | ms/batch 157.68 | loss  3.37 | ppl    28.95\n",
      "| epoch  24 |   400/ 2983 batches | ms/batch 156.98 | loss  3.35 | ppl    28.59\n",
      "| epoch  24 |   600/ 2983 batches | ms/batch 156.90 | loss  3.28 | ppl    26.47\n",
      "| epoch  24 |   800/ 2983 batches | ms/batch 156.93 | loss  3.35 | ppl    28.44\n",
      "| epoch  24 |  1000/ 2983 batches | ms/batch 156.92 | loss  3.33 | ppl    27.98\n",
      "| epoch  24 |  1200/ 2983 batches | ms/batch 156.93 | loss  3.39 | ppl    29.80\n",
      "| epoch  24 |  1400/ 2983 batches | ms/batch 156.98 | loss  3.42 | ppl    30.62\n",
      "| epoch  24 |  1600/ 2983 batches | ms/batch 156.94 | loss  3.42 | ppl    30.66\n",
      "| epoch  24 |  1800/ 2983 batches | ms/batch 156.93 | loss  3.31 | ppl    27.40\n",
      "| epoch  24 |  2000/ 2983 batches | ms/batch 156.93 | loss  3.36 | ppl    28.72\n",
      "| epoch  24 |  2200/ 2983 batches | ms/batch 156.92 | loss  3.34 | ppl    28.24\n",
      "| epoch  24 |  2400/ 2983 batches | ms/batch 156.93 | loss  3.32 | ppl    27.61\n",
      "| epoch  24 |  2600/ 2983 batches | ms/batch 156.91 | loss  3.36 | ppl    28.68\n",
      "| epoch  24 |  2800/ 2983 batches | ms/batch 156.94 | loss  3.34 | ppl    28.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 485.14s | valid loss  3.29 | valid ppl    26.84\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  25 |   200/ 2983 batches | ms/batch 157.74 | loss  3.31 | ppl    27.26\n",
      "| epoch  25 |   400/ 2983 batches | ms/batch 156.91 | loss  3.32 | ppl    27.54\n",
      "| epoch  25 |   600/ 2983 batches | ms/batch 156.98 | loss  3.24 | ppl    25.56\n",
      "| epoch  25 |   800/ 2983 batches | ms/batch 156.92 | loss  3.29 | ppl    26.84\n",
      "| epoch  25 |  1000/ 2983 batches | ms/batch 156.96 | loss  3.28 | ppl    26.54\n",
      "| epoch  25 |  1200/ 2983 batches | ms/batch 156.94 | loss  3.33 | ppl    27.93\n",
      "| epoch  25 |  1400/ 2983 batches | ms/batch 156.96 | loss  3.40 | ppl    29.96\n",
      "| epoch  25 |  1600/ 2983 batches | ms/batch 156.97 | loss  3.41 | ppl    30.16\n",
      "| epoch  25 |  1800/ 2983 batches | ms/batch 156.92 | loss  3.31 | ppl    27.40\n",
      "| epoch  25 |  2000/ 2983 batches | ms/batch 156.92 | loss  3.38 | ppl    29.43\n",
      "| epoch  25 |  2200/ 2983 batches | ms/batch 156.94 | loss  3.32 | ppl    27.65\n",
      "| epoch  25 |  2400/ 2983 batches | ms/batch 156.90 | loss  3.34 | ppl    28.29\n",
      "| epoch  25 |  2600/ 2983 batches | ms/batch 156.94 | loss  3.37 | ppl    29.13\n",
      "| epoch  25 |  2800/ 2983 batches | ms/batch 156.94 | loss  3.31 | ppl    27.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 485.16s | valid loss  3.20 | valid ppl    24.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/ 2983 batches | ms/batch 157.77 | loss  3.24 | ppl    25.50\n",
      "| epoch  26 |   400/ 2983 batches | ms/batch 156.91 | loss  3.24 | ppl    25.44\n",
      "| epoch  26 |   600/ 2983 batches | ms/batch 156.87 | loss  3.16 | ppl    23.61\n",
      "| epoch  26 |   800/ 2983 batches | ms/batch 156.90 | loss  3.19 | ppl    24.22\n",
      "| epoch  26 |  1000/ 2983 batches | ms/batch 156.88 | loss  3.21 | ppl    24.72\n",
      "| epoch  26 |  1200/ 2983 batches | ms/batch 156.89 | loss  3.24 | ppl    25.58\n",
      "| epoch  26 |  1400/ 2983 batches | ms/batch 156.90 | loss  3.29 | ppl    26.81\n",
      "| epoch  26 |  1600/ 2983 batches | ms/batch 156.85 | loss  3.28 | ppl    26.61\n",
      "| epoch  26 |  1800/ 2983 batches | ms/batch 156.90 | loss  3.18 | ppl    23.93\n",
      "| epoch  26 |  2000/ 2983 batches | ms/batch 156.90 | loss  3.24 | ppl    25.59\n",
      "| epoch  26 |  2200/ 2983 batches | ms/batch 156.92 | loss  3.21 | ppl    24.84\n",
      "| epoch  26 |  2400/ 2983 batches | ms/batch 156.85 | loss  3.21 | ppl    24.70\n",
      "| epoch  26 |  2600/ 2983 batches | ms/batch 156.90 | loss  3.24 | ppl    25.54\n",
      "| epoch  26 |  2800/ 2983 batches | ms/batch 156.82 | loss  3.20 | ppl    24.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 485.01s | valid loss  3.16 | valid ppl    23.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/ 2983 batches | ms/batch 157.73 | loss  3.23 | ppl    25.16\n",
      "| epoch  27 |   400/ 2983 batches | ms/batch 156.87 | loss  3.24 | ppl    25.55\n",
      "| epoch  27 |   600/ 2983 batches | ms/batch 156.88 | loss  3.21 | ppl    24.86\n",
      "| epoch  27 |   800/ 2983 batches | ms/batch 156.83 | loss  3.23 | ppl    25.24\n",
      "| epoch  27 |  1000/ 2983 batches | ms/batch 156.86 | loss  3.19 | ppl    24.38\n",
      "| epoch  27 |  1200/ 2983 batches | ms/batch 156.84 | loss  3.25 | ppl    25.78\n",
      "| epoch  27 |  1400/ 2983 batches | ms/batch 156.83 | loss  3.28 | ppl    26.50\n",
      "| epoch  27 |  1600/ 2983 batches | ms/batch 156.93 | loss  3.30 | ppl    27.23\n",
      "| epoch  27 |  1800/ 2983 batches | ms/batch 156.89 | loss  3.19 | ppl    24.19\n",
      "| epoch  27 |  2000/ 2983 batches | ms/batch 156.82 | loss  3.21 | ppl    24.74\n",
      "| epoch  27 |  2200/ 2983 batches | ms/batch 156.85 | loss  3.17 | ppl    23.87\n",
      "| epoch  27 |  2400/ 2983 batches | ms/batch 156.88 | loss  3.20 | ppl    24.53\n",
      "| epoch  27 |  2600/ 2983 batches | ms/batch 156.79 | loss  3.23 | ppl    25.35\n",
      "| epoch  27 |  2800/ 2983 batches | ms/batch 156.86 | loss  3.18 | ppl    24.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 484.93s | valid loss  3.15 | valid ppl    23.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/ 2983 batches | ms/batch 157.64 | loss  3.16 | ppl    23.61\n",
      "| epoch  28 |   400/ 2983 batches | ms/batch 156.84 | loss  3.16 | ppl    23.46\n",
      "| epoch  28 |   600/ 2983 batches | ms/batch 156.82 | loss  3.09 | ppl    21.98\n",
      "| epoch  28 |   800/ 2983 batches | ms/batch 156.81 | loss  3.11 | ppl    22.44\n",
      "| epoch  28 |  1000/ 2983 batches | ms/batch 156.80 | loss  3.08 | ppl    21.81\n",
      "| epoch  28 |  1200/ 2983 batches | ms/batch 156.84 | loss  3.10 | ppl    22.21\n",
      "| epoch  28 |  1400/ 2983 batches | ms/batch 156.89 | loss  3.17 | ppl    23.80\n",
      "| epoch  28 |  1600/ 2983 batches | ms/batch 156.87 | loss  3.18 | ppl    23.93\n",
      "| epoch  28 |  1800/ 2983 batches | ms/batch 156.86 | loss  3.06 | ppl    21.24\n",
      "| epoch  28 |  2000/ 2983 batches | ms/batch 156.83 | loss  3.10 | ppl    22.27\n",
      "| epoch  28 |  2200/ 2983 batches | ms/batch 156.84 | loss  3.06 | ppl    21.33\n",
      "| epoch  28 |  2400/ 2983 batches | ms/batch 156.83 | loss  3.06 | ppl    21.23\n",
      "| epoch  28 |  2600/ 2983 batches | ms/batch 156.84 | loss  3.10 | ppl    22.11\n",
      "| epoch  28 |  2800/ 2983 batches | ms/batch 156.79 | loss  3.04 | ppl    20.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 484.84s | valid loss  3.08 | valid ppl    21.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/ 2983 batches | ms/batch 157.69 | loss  3.03 | ppl    20.78\n",
      "| epoch  29 |   400/ 2983 batches | ms/batch 156.81 | loss  3.05 | ppl    21.11\n",
      "| epoch  29 |   600/ 2983 batches | ms/batch 156.82 | loss  2.97 | ppl    19.44\n",
      "| epoch  29 |   800/ 2983 batches | ms/batch 156.80 | loss  3.01 | ppl    20.19\n",
      "| epoch  29 |  1000/ 2983 batches | ms/batch 156.87 | loss  2.98 | ppl    19.76\n",
      "| epoch  29 |  1200/ 2983 batches | ms/batch 156.82 | loss  2.99 | ppl    19.85\n",
      "| epoch  29 |  1400/ 2983 batches | ms/batch 156.85 | loss  3.03 | ppl    20.79\n",
      "| epoch  29 |  1600/ 2983 batches | ms/batch 156.87 | loss  3.03 | ppl    20.76\n",
      "| epoch  29 |  1800/ 2983 batches | ms/batch 156.79 | loss  2.94 | ppl    18.86\n",
      "| epoch  29 |  2000/ 2983 batches | ms/batch 156.79 | loss  3.00 | ppl    20.05\n",
      "| epoch  29 |  2200/ 2983 batches | ms/batch 156.84 | loss  3.00 | ppl    20.02\n",
      "| epoch  29 |  2400/ 2983 batches | ms/batch 156.82 | loss  2.97 | ppl    19.46\n",
      "| epoch  29 |  2600/ 2983 batches | ms/batch 156.90 | loss  3.00 | ppl    20.12\n",
      "| epoch  29 |  2800/ 2983 batches | ms/batch 156.80 | loss  2.96 | ppl    19.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 484.82s | valid loss  3.03 | valid ppl    20.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/ 2983 batches | ms/batch 157.58 | loss  2.97 | ppl    19.51\n",
      "| epoch  30 |   400/ 2983 batches | ms/batch 156.82 | loss  2.97 | ppl    19.51\n",
      "| epoch  30 |   600/ 2983 batches | ms/batch 156.82 | loss  2.91 | ppl    18.42\n",
      "| epoch  30 |   800/ 2983 batches | ms/batch 156.85 | loss  2.96 | ppl    19.24\n",
      "| epoch  30 |  1000/ 2983 batches | ms/batch 156.93 | loss  2.93 | ppl    18.66\n",
      "| epoch  30 |  1200/ 2983 batches | ms/batch 156.78 | loss  2.95 | ppl    19.13\n",
      "| epoch  30 |  1400/ 2983 batches | ms/batch 156.82 | loss  2.97 | ppl    19.58\n",
      "| epoch  30 |  1600/ 2983 batches | ms/batch 156.77 | loss  2.99 | ppl    19.89\n",
      "| epoch  30 |  1800/ 2983 batches | ms/batch 156.84 | loss  2.88 | ppl    17.82\n",
      "| epoch  30 |  2000/ 2983 batches | ms/batch 156.80 | loss  2.96 | ppl    19.31\n",
      "| epoch  30 |  2200/ 2983 batches | ms/batch 156.81 | loss  2.93 | ppl    18.81\n",
      "| epoch  30 |  2400/ 2983 batches | ms/batch 156.79 | loss  2.94 | ppl    18.85\n",
      "| epoch  30 |  2600/ 2983 batches | ms/batch 156.84 | loss  2.95 | ppl    19.19\n",
      "| epoch  30 |  2800/ 2983 batches | ms/batch 156.78 | loss  2.89 | ppl    17.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 484.78s | valid loss  2.98 | valid ppl    19.74\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  31 |   200/ 2983 batches | ms/batch 157.57 | loss  2.93 | ppl    18.69\n",
      "| epoch  31 |   400/ 2983 batches | ms/batch 156.80 | loss  2.96 | ppl    19.26\n",
      "| epoch  31 |   600/ 2983 batches | ms/batch 156.78 | loss  2.85 | ppl    17.34\n",
      "| epoch  31 |   800/ 2983 batches | ms/batch 156.75 | loss  2.88 | ppl    17.81\n",
      "| epoch  31 |  1000/ 2983 batches | ms/batch 156.79 | loss  2.88 | ppl    17.80\n",
      "| epoch  31 |  1200/ 2983 batches | ms/batch 156.84 | loss  2.90 | ppl    18.12\n",
      "| epoch  31 |  1400/ 2983 batches | ms/batch 156.80 | loss  2.95 | ppl    19.11\n",
      "| epoch  31 |  1600/ 2983 batches | ms/batch 156.77 | loss  2.96 | ppl    19.32\n",
      "| epoch  31 |  1800/ 2983 batches | ms/batch 156.77 | loss  2.87 | ppl    17.58\n",
      "| epoch  31 |  2000/ 2983 batches | ms/batch 156.77 | loss  2.92 | ppl    18.48\n",
      "| epoch  31 |  2200/ 2983 batches | ms/batch 156.80 | loss  2.90 | ppl    18.11\n",
      "| epoch  31 |  2400/ 2983 batches | ms/batch 156.75 | loss  2.88 | ppl    17.83\n",
      "| epoch  31 |  2600/ 2983 batches | ms/batch 156.73 | loss  2.93 | ppl    18.74\n",
      "| epoch  31 |  2800/ 2983 batches | ms/batch 156.77 | loss  2.89 | ppl    17.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 484.67s | valid loss  3.00 | valid ppl    20.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   200/ 2983 batches | ms/batch 157.49 | loss  2.89 | ppl    18.04\n",
      "| epoch  32 |   400/ 2983 batches | ms/batch 156.82 | loss  2.92 | ppl    18.60\n",
      "| epoch  32 |   600/ 2983 batches | ms/batch 156.76 | loss  2.85 | ppl    17.24\n",
      "| epoch  32 |   800/ 2983 batches | ms/batch 156.77 | loss  2.88 | ppl    17.83\n",
      "| epoch  32 |  1000/ 2983 batches | ms/batch 156.79 | loss  2.89 | ppl    17.98\n",
      "| epoch  32 |  1200/ 2983 batches | ms/batch 156.74 | loss  2.94 | ppl    18.88\n",
      "| epoch  32 |  1400/ 2983 batches | ms/batch 156.75 | loss  2.93 | ppl    18.71\n",
      "| epoch  32 |  1600/ 2983 batches | ms/batch 156.72 | loss  2.97 | ppl    19.41\n",
      "| epoch  32 |  1800/ 2983 batches | ms/batch 156.75 | loss  2.87 | ppl    17.56\n",
      "| epoch  32 |  2000/ 2983 batches | ms/batch 156.77 | loss  2.95 | ppl    19.19\n",
      "| epoch  32 |  2200/ 2983 batches | ms/batch 156.78 | loss  2.90 | ppl    18.21\n",
      "| epoch  32 |  2400/ 2983 batches | ms/batch 156.82 | loss  2.90 | ppl    18.10\n",
      "| epoch  32 |  2600/ 2983 batches | ms/batch 156.74 | loss  2.92 | ppl    18.54\n",
      "| epoch  32 |  2800/ 2983 batches | ms/batch 156.76 | loss  2.89 | ppl    17.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 484.60s | valid loss  3.02 | valid ppl    20.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   200/ 2983 batches | ms/batch 157.50 | loss  2.88 | ppl    17.89\n",
      "| epoch  33 |   400/ 2983 batches | ms/batch 156.74 | loss  2.92 | ppl    18.50\n",
      "| epoch  33 |   600/ 2983 batches | ms/batch 156.75 | loss  2.85 | ppl    17.33\n",
      "| epoch  33 |   800/ 2983 batches | ms/batch 156.73 | loss  2.86 | ppl    17.52\n",
      "| epoch  33 |  1000/ 2983 batches | ms/batch 156.83 | loss  2.83 | ppl    16.97\n",
      "| epoch  33 |  1200/ 2983 batches | ms/batch 156.73 | loss  2.86 | ppl    17.54\n",
      "| epoch  33 |  1400/ 2983 batches | ms/batch 156.74 | loss  2.87 | ppl    17.57\n",
      "| epoch  33 |  1600/ 2983 batches | ms/batch 156.75 | loss  2.89 | ppl    17.94\n",
      "| epoch  33 |  1800/ 2983 batches | ms/batch 156.70 | loss  2.80 | ppl    16.40\n",
      "| epoch  33 |  2000/ 2983 batches | ms/batch 156.75 | loss  2.85 | ppl    17.34\n",
      "| epoch  33 |  2200/ 2983 batches | ms/batch 156.73 | loss  2.83 | ppl    16.94\n",
      "| epoch  33 |  2400/ 2983 batches | ms/batch 156.69 | loss  2.83 | ppl    16.96\n",
      "| epoch  33 |  2600/ 2983 batches | ms/batch 156.70 | loss  2.85 | ppl    17.36\n",
      "| epoch  33 |  2800/ 2983 batches | ms/batch 156.68 | loss  2.82 | ppl    16.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 484.51s | valid loss  2.97 | valid ppl    19.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   200/ 2983 batches | ms/batch 157.43 | loss  2.81 | ppl    16.67\n",
      "| epoch  34 |   400/ 2983 batches | ms/batch 156.76 | loss  2.85 | ppl    17.28\n",
      "| epoch  34 |   600/ 2983 batches | ms/batch 156.71 | loss  2.81 | ppl    16.59\n",
      "| epoch  34 |   800/ 2983 batches | ms/batch 156.72 | loss  2.80 | ppl    16.51\n",
      "| epoch  34 |  1000/ 2983 batches | ms/batch 156.74 | loss  2.80 | ppl    16.40\n",
      "| epoch  34 |  1200/ 2983 batches | ms/batch 156.71 | loss  2.84 | ppl    17.11\n",
      "| epoch  34 |  1400/ 2983 batches | ms/batch 156.74 | loss  2.89 | ppl    18.00\n",
      "| epoch  34 |  1600/ 2983 batches | ms/batch 156.79 | loss  2.90 | ppl    18.22\n",
      "| epoch  34 |  1800/ 2983 batches | ms/batch 156.72 | loss  2.80 | ppl    16.48\n",
      "| epoch  34 |  2000/ 2983 batches | ms/batch 156.67 | loss  2.86 | ppl    17.42\n",
      "| epoch  34 |  2200/ 2983 batches | ms/batch 156.71 | loss  2.81 | ppl    16.61\n",
      "| epoch  34 |  2400/ 2983 batches | ms/batch 156.69 | loss  2.81 | ppl    16.67\n",
      "| epoch  34 |  2600/ 2983 batches | ms/batch 156.73 | loss  2.88 | ppl    17.79\n",
      "| epoch  34 |  2800/ 2983 batches | ms/batch 156.67 | loss  2.85 | ppl    17.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 484.48s | valid loss  2.98 | valid ppl    19.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   200/ 2983 batches | ms/batch 157.48 | loss  2.85 | ppl    17.37\n",
      "| epoch  35 |   400/ 2983 batches | ms/batch 156.71 | loss  2.87 | ppl    17.62\n",
      "| epoch  35 |   600/ 2983 batches | ms/batch 156.68 | loss  2.83 | ppl    16.89\n",
      "| epoch  35 |   800/ 2983 batches | ms/batch 156.69 | loss  2.85 | ppl    17.22\n",
      "| epoch  35 |  1000/ 2983 batches | ms/batch 156.69 | loss  2.84 | ppl    17.12\n",
      "| epoch  35 |  1200/ 2983 batches | ms/batch 156.67 | loss  2.89 | ppl    17.94\n",
      "| epoch  35 |  1400/ 2983 batches | ms/batch 156.68 | loss  2.90 | ppl    18.13\n",
      "| epoch  35 |  1600/ 2983 batches | ms/batch 156.70 | loss  2.93 | ppl    18.64\n",
      "| epoch  35 |  1800/ 2983 batches | ms/batch 156.70 | loss  2.83 | ppl    16.89\n",
      "| epoch  35 |  2000/ 2983 batches | ms/batch 156.72 | loss  2.86 | ppl    17.49\n",
      "| epoch  35 |  2200/ 2983 batches | ms/batch 156.69 | loss  2.83 | ppl    17.01\n",
      "| epoch  35 |  2400/ 2983 batches | ms/batch 156.64 | loss  2.85 | ppl    17.21\n",
      "| epoch  35 |  2600/ 2983 batches | ms/batch 156.69 | loss  2.83 | ppl    17.00\n",
      "| epoch  35 |  2800/ 2983 batches | ms/batch 156.68 | loss  2.81 | ppl    16.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 484.38s | valid loss  2.95 | valid ppl    19.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   200/ 2983 batches | ms/batch 157.42 | loss  2.81 | ppl    16.54\n",
      "| epoch  36 |   400/ 2983 batches | ms/batch 156.67 | loss  2.84 | ppl    17.17\n",
      "| epoch  36 |   600/ 2983 batches | ms/batch 156.71 | loss  2.77 | ppl    15.96\n",
      "| epoch  36 |   800/ 2983 batches | ms/batch 156.69 | loss  2.80 | ppl    16.44\n",
      "| epoch  36 |  1000/ 2983 batches | ms/batch 156.66 | loss  2.77 | ppl    15.91\n",
      "| epoch  36 |  1200/ 2983 batches | ms/batch 156.68 | loss  2.80 | ppl    16.52\n",
      "| epoch  36 |  1400/ 2983 batches | ms/batch 156.64 | loss  2.84 | ppl    17.09\n",
      "| epoch  36 |  1600/ 2983 batches | ms/batch 156.69 | loss  2.84 | ppl    17.12\n",
      "| epoch  36 |  1800/ 2983 batches | ms/batch 156.68 | loss  2.75 | ppl    15.69\n",
      "| epoch  36 |  2000/ 2983 batches | ms/batch 156.63 | loss  2.83 | ppl    16.94\n",
      "| epoch  36 |  2200/ 2983 batches | ms/batch 156.69 | loss  2.79 | ppl    16.24\n",
      "| epoch  36 |  2400/ 2983 batches | ms/batch 156.68 | loss  2.77 | ppl    15.99\n",
      "| epoch  36 |  2600/ 2983 batches | ms/batch 156.64 | loss  2.81 | ppl    16.60\n",
      "| epoch  36 |  2800/ 2983 batches | ms/batch 156.64 | loss  2.76 | ppl    15.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 484.32s | valid loss  2.95 | valid ppl    19.06\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  37 |   200/ 2983 batches | ms/batch 157.41 | loss  2.74 | ppl    15.46\n",
      "| epoch  37 |   400/ 2983 batches | ms/batch 156.62 | loss  2.77 | ppl    15.89\n",
      "| epoch  37 |   600/ 2983 batches | ms/batch 156.61 | loss  2.71 | ppl    14.98\n",
      "| epoch  37 |   800/ 2983 batches | ms/batch 156.68 | loss  2.75 | ppl    15.63\n",
      "| epoch  37 |  1000/ 2983 batches | ms/batch 156.64 | loss  2.76 | ppl    15.82\n",
      "| epoch  37 |  1200/ 2983 batches | ms/batch 156.64 | loss  2.79 | ppl    16.29\n",
      "| epoch  37 |  1400/ 2983 batches | ms/batch 156.65 | loss  2.81 | ppl    16.59\n",
      "| epoch  37 |  1600/ 2983 batches | ms/batch 156.64 | loss  2.83 | ppl    16.88\n",
      "| epoch  37 |  1800/ 2983 batches | ms/batch 156.61 | loss  2.77 | ppl    15.97\n",
      "| epoch  37 |  2000/ 2983 batches | ms/batch 156.64 | loss  2.82 | ppl    16.70\n",
      "| epoch  37 |  2200/ 2983 batches | ms/batch 156.65 | loss  2.79 | ppl    16.25\n",
      "| epoch  37 |  2400/ 2983 batches | ms/batch 156.63 | loss  2.79 | ppl    16.28\n",
      "| epoch  37 |  2600/ 2983 batches | ms/batch 156.67 | loss  2.82 | ppl    16.84\n",
      "| epoch  37 |  2800/ 2983 batches | ms/batch 156.64 | loss  2.79 | ppl    16.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 484.23s | valid loss  2.96 | valid ppl    19.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   200/ 2983 batches | ms/batch 157.46 | loss  2.78 | ppl    16.09\n",
      "| epoch  38 |   400/ 2983 batches | ms/batch 156.62 | loss  2.80 | ppl    16.44\n",
      "| epoch  38 |   600/ 2983 batches | ms/batch 156.66 | loss  2.75 | ppl    15.66\n",
      "| epoch  38 |   800/ 2983 batches | ms/batch 156.67 | loss  2.73 | ppl    15.39\n",
      "| epoch  38 |  1000/ 2983 batches | ms/batch 156.61 | loss  2.71 | ppl    14.97\n",
      "| epoch  38 |  1200/ 2983 batches | ms/batch 156.65 | loss  2.75 | ppl    15.69\n",
      "| epoch  38 |  1400/ 2983 batches | ms/batch 156.62 | loss  2.78 | ppl    16.10\n",
      "| epoch  38 |  1600/ 2983 batches | ms/batch 156.68 | loss  2.81 | ppl    16.55\n",
      "| epoch  38 |  1800/ 2983 batches | ms/batch 156.66 | loss  2.72 | ppl    15.12\n",
      "| epoch  38 |  2000/ 2983 batches | ms/batch 156.57 | loss  2.77 | ppl    15.99\n",
      "| epoch  38 |  2200/ 2983 batches | ms/batch 156.67 | loss  2.77 | ppl    15.90\n",
      "| epoch  38 |  2400/ 2983 batches | ms/batch 156.66 | loss  2.76 | ppl    15.83\n",
      "| epoch  38 |  2600/ 2983 batches | ms/batch 156.66 | loss  2.75 | ppl    15.64\n",
      "| epoch  38 |  2800/ 2983 batches | ms/batch 156.59 | loss  2.72 | ppl    15.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 484.25s | valid loss  2.95 | valid ppl    19.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   200/ 2983 batches | ms/batch 157.45 | loss  2.73 | ppl    15.40\n",
      "| epoch  39 |   400/ 2983 batches | ms/batch 156.60 | loss  2.74 | ppl    15.49\n",
      "| epoch  39 |   600/ 2983 batches | ms/batch 156.62 | loss  2.67 | ppl    14.45\n",
      "| epoch  39 |   800/ 2983 batches | ms/batch 156.64 | loss  2.68 | ppl    14.63\n",
      "| epoch  39 |  1000/ 2983 batches | ms/batch 156.59 | loss  2.66 | ppl    14.37\n",
      "| epoch  39 |  1200/ 2983 batches | ms/batch 156.64 | loss  2.71 | ppl    15.05\n",
      "| epoch  39 |  1400/ 2983 batches | ms/batch 156.60 | loss  2.74 | ppl    15.47\n",
      "| epoch  39 |  1600/ 2983 batches | ms/batch 156.65 | loss  2.75 | ppl    15.67\n",
      "| epoch  39 |  1800/ 2983 batches | ms/batch 156.63 | loss  2.66 | ppl    14.36\n",
      "| epoch  39 |  2000/ 2983 batches | ms/batch 156.58 | loss  2.73 | ppl    15.27\n",
      "| epoch  39 |  2200/ 2983 batches | ms/batch 156.61 | loss  2.67 | ppl    14.45\n",
      "| epoch  39 |  2400/ 2983 batches | ms/batch 156.61 | loss  2.69 | ppl    14.73\n",
      "| epoch  39 |  2600/ 2983 batches | ms/batch 156.63 | loss  2.73 | ppl    15.27\n",
      "| epoch  39 |  2800/ 2983 batches | ms/batch 156.63 | loss  2.66 | ppl    14.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 484.17s | valid loss  2.97 | valid ppl    19.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   200/ 2983 batches | ms/batch 157.40 | loss  2.78 | ppl    16.10\n",
      "| epoch  40 |   400/ 2983 batches | ms/batch 156.61 | loss  2.77 | ppl    15.93\n",
      "| epoch  40 |   600/ 2983 batches | ms/batch 156.84 | loss  2.73 | ppl    15.36\n",
      "| epoch  40 |   800/ 2983 batches | ms/batch 174.77 | loss  2.74 | ppl    15.55\n",
      "| epoch  40 |  1000/ 2983 batches | ms/batch 168.01 | loss  2.74 | ppl    15.54\n",
      "| epoch  40 |  1200/ 2983 batches | ms/batch 157.19 | loss  2.77 | ppl    16.00\n",
      "| epoch  40 |  1400/ 2983 batches | ms/batch 156.92 | loss  2.75 | ppl    15.65\n",
      "| epoch  40 |  1600/ 2983 batches | ms/batch 156.93 | loss  2.78 | ppl    16.06\n",
      "| epoch  40 |  1800/ 2983 batches | ms/batch 160.41 | loss  2.70 | ppl    14.89\n",
      "| epoch  40 |  2000/ 2983 batches | ms/batch 161.43 | loss  2.74 | ppl    15.42\n",
      "| epoch  40 |  2200/ 2983 batches | ms/batch 169.01 | loss  2.68 | ppl    14.60\n",
      "| epoch  40 |  2400/ 2983 batches | ms/batch 183.68 | loss  2.67 | ppl    14.39\n",
      "| epoch  40 |  2600/ 2983 batches | ms/batch 167.74 | loss  2.71 | ppl    14.99\n",
      "| epoch  40 |  2800/ 2983 batches | ms/batch 164.28 | loss  2.67 | ppl    14.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 507.88s | valid loss  2.90 | valid ppl    18.20\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, args_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args_save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            ## Save State Dictionary\n",
    "            with open(args_save_state, 'wb') as f:\n",
    "                torch.save(model.state_dict(), f)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  2.77 | test ppl    15.93\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(args_save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to generate with this model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_data = '../data/wikitext-2'\n",
    "args_checkpoint = args_save #'./model.pt'\n",
    "args_state_dict = args_save_state # './model_state.pt'\n",
    "args_outf = 'generated_800_bilstm.txt'\n",
    "args_words = 1000\n",
    "args_seed = 1234\n",
    "args_temperature = 3.0\n",
    "args_log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args_seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_temperature < 1e-3:\n",
    "    print(\"args_temperature has to be greater or equal 1e-3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus(args_data)\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args_checkpoint, 'rb') as f:\n",
    "    model = torch.load(f).to(device)\n",
    "\n",
    "# model = rnn_model.RNNModel(args_model, ntokens, args_emsize, args_nhid, args_nlayers, args_dropout, args_tied).to(device)\n",
    "# with open(args_state_dict, 'rb') as f:\n",
    "#     state_dict = torch.load(f)\n",
    "#     model.load_state_dict(state_dict)\n",
    "\n",
    "model.rnn.flatten_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden(1)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.2)\n",
       "  (encoder): Embedding(33278, 650)\n",
       "  (rnn): LSTM(650, 650, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "  (decoder): Linear(in_features=1300, out_features=33278, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 0/1000 words\n",
      "| Generated 100/1000 words\n",
      "| Generated 200/1000 words\n",
      "| Generated 300/1000 words\n",
      "| Generated 400/1000 words\n",
      "| Generated 500/1000 words\n",
      "| Generated 600/1000 words\n",
      "| Generated 700/1000 words\n",
      "| Generated 800/1000 words\n",
      "| Generated 900/1000 words\n"
     ]
    }
   ],
   "source": [
    "with open(args_outf, 'w') as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(args_words):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(args_temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % args_log_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i, args_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.weight',\n",
       "              tensor([[ 2.6517, -4.3920,  3.4548,  ..., -2.8609,  1.0113,  8.4129],\n",
       "                      [-3.3560,  3.9145,  2.0566,  ..., -2.2402,  3.2369,  0.9164],\n",
       "                      [-1.7510,  0.2326,  1.3499,  ...,  3.2238, -3.2230,  1.1218],\n",
       "                      ...,\n",
       "                      [ 0.4716,  2.2483,  0.9069,  ..., -1.7204,  2.0244,  1.2337],\n",
       "                      [-0.9118, -0.0683,  0.4488,  ..., -2.3215,  0.2270,  0.4906],\n",
       "                      [-1.0844, -0.4457,  1.5023,  ..., -0.4776,  3.6123, -1.0992]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_ih_l0',\n",
       "              tensor([[-0.6244,  1.0626,  0.4446,  ...,  0.9139,  2.5111,  0.7431],\n",
       "                      [ 4.1255, -5.7791, -3.3513,  ..., -5.7179,  2.6452, -4.1258],\n",
       "                      [-3.5881,  2.7773, -1.7840,  ...,  1.6418,  7.9454,  4.8219],\n",
       "                      ...,\n",
       "                      [ 0.6993,  0.3756, -2.4934,  ..., -0.7411, -4.5546,  3.6405],\n",
       "                      [ 6.8463, -7.3289, -0.8829,  ..., -2.6401, -2.3310,  1.8062],\n",
       "                      [-0.8810,  6.3653, -3.9477,  ..., -0.8369,  4.7197,  6.2211]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_hh_l0',\n",
       "              tensor([[-0.5206, -1.9764,  5.3979,  ..., -0.1993,  1.4001, -2.5845],\n",
       "                      [-1.0176,  0.5411,  2.5197,  ...,  0.3529,  4.7273,  1.8332],\n",
       "                      [-0.4609,  3.3397,  1.3814,  ..., -4.2268,  3.2768, -1.0616],\n",
       "                      ...,\n",
       "                      [-0.1064, -1.5330, -2.4711,  ...,  5.5791, -0.2460, -5.1198],\n",
       "                      [-2.4462, -1.5552, -2.2583,  ...,  0.8288,  0.2818,  1.6062],\n",
       "                      [-3.8734,  3.6181, -3.0956,  ..., -4.4405,  0.7970, -1.0644]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_ih_l0',\n",
       "              tensor([-0.9280, -6.2366, -5.1091,  ...,  3.1905,  3.9683, -2.4976], device='cuda:0')),\n",
       "             ('rnn.bias_hh_l0',\n",
       "              tensor([-0.8782, -6.1950, -5.1235,  ...,  3.1798,  3.9607, -2.4597], device='cuda:0')),\n",
       "             ('rnn.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.7652,  2.8258,  5.6603,  ..., -0.1416,  4.3964, -1.2130],\n",
       "                      [-2.1160, -4.7589, -3.9643,  ..., -7.9490,  0.0897,  2.2745],\n",
       "                      [-2.2866, -5.9391,  1.5879,  ..., -3.7462,  3.8512,  4.8737],\n",
       "                      ...,\n",
       "                      [ 1.3388,  2.0010, -1.4340,  ..., -1.1311, -0.2392, -2.9471],\n",
       "                      [-1.2429,  1.7957, -0.1122,  ...,  3.0365, -1.6473,  0.7878],\n",
       "                      [ 3.3085,  3.0318, -0.3132,  ...,  2.1498,  0.8380, -3.2858]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_hh_l0_reverse',\n",
       "              tensor([[-0.1110,  0.3217, -1.2518,  ...,  2.6494,  1.9717,  6.1018],\n",
       "                      [-1.1392,  1.4188,  1.9974,  ...,  4.6937, -3.8799, -0.0909],\n",
       "                      [ 0.2987,  1.4900, -2.6691,  ..., -1.4455,  4.0101, -0.0045],\n",
       "                      ...,\n",
       "                      [-4.3923,  3.0011,  1.1823,  ..., -4.7494,  5.3292,  3.3743],\n",
       "                      [-2.4920, -1.7691,  5.0327,  ...,  5.9118, -5.4797,  0.7243],\n",
       "                      [ 2.8975, -6.1565,  1.7184,  ..., -3.0471, -1.0035, -0.4910]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_ih_l0_reverse',\n",
       "              tensor([-3.5078,  1.8946,  0.7713,  ..., -4.7870,  5.5684, -0.0767], device='cuda:0')),\n",
       "             ('rnn.bias_hh_l0_reverse',\n",
       "              tensor([-3.4875,  1.9197,  0.8127,  ..., -4.8079,  5.5420, -0.0334], device='cuda:0')),\n",
       "             ('rnn.weight_ih_l1',\n",
       "              tensor([[-0.1739, -0.9863, -0.3322,  ..., -0.3378,  1.0293, -0.6774],\n",
       "                      [ 0.1355,  0.0254, -0.0028,  ..., -0.1722, -0.3176,  0.4552],\n",
       "                      [ 0.4231,  0.0733, -0.1138,  ...,  0.3402, -0.4451,  0.1808],\n",
       "                      ...,\n",
       "                      [ 0.0179, -0.1951,  0.0155,  ..., -0.1052, -0.0902, -0.5361],\n",
       "                      [-0.0034, -0.0880,  0.2184,  ..., -0.0541,  0.8652, -0.7540],\n",
       "                      [-0.1974, -0.0366, -0.0934,  ..., -0.4611, -0.2868,  0.1108]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_hh_l1',\n",
       "              tensor([[ 0.5811, -0.0193,  0.0361,  ..., -0.0040,  0.0447,  0.3074],\n",
       "                      [-0.0689, -0.0942, -0.0238,  ..., -0.0469, -0.0625, -0.0371],\n",
       "                      [-0.1011, -0.0683,  0.1054,  ..., -0.0769,  0.0567,  0.0743],\n",
       "                      ...,\n",
       "                      [ 0.1650,  0.0195,  0.0529,  ...,  0.1906,  0.0676,  0.4369],\n",
       "                      [ 0.1101,  0.1197, -0.0691,  ...,  0.0959, -0.5112,  0.1163],\n",
       "                      [ 0.0473,  0.0423, -0.0205,  ...,  0.0361, -0.0437, -0.6699]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_ih_l1',\n",
       "              tensor([-0.9386,  0.2155,  0.1079,  ..., -1.0178, -0.9369,  0.9140], device='cuda:0')),\n",
       "             ('rnn.bias_hh_l1',\n",
       "              tensor([-0.9472,  0.2570,  0.1493,  ..., -1.0411, -0.9643,  0.8710], device='cuda:0')),\n",
       "             ('rnn.weight_ih_l1_reverse',\n",
       "              tensor([[-0.3333, -0.1978,  0.2112,  ..., -0.7444,  0.9602, -0.1107],\n",
       "                      [ 0.3535, -0.0352, -0.8503,  ...,  0.1440,  0.4826, -0.1664],\n",
       "                      [ 0.2644,  0.9126,  0.6193,  ..., -1.8344,  3.3169, -2.4867],\n",
       "                      ...,\n",
       "                      [ 1.6964,  0.9314, -1.2248,  ..., -2.4834,  0.6648, -0.6896],\n",
       "                      [ 1.8285,  0.7934,  3.2392,  ..., -0.9678,  4.0396,  0.5622],\n",
       "                      [-3.7052,  0.3001,  0.0315,  ...,  2.0358,  0.6678, -0.4024]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_hh_l1_reverse',\n",
       "              tensor([[ -0.1856,   0.1149,   0.2425,  ...,   0.0316,   0.0289,  -0.0094],\n",
       "                      [  0.0386,  -0.0206,  -0.2541,  ...,   0.1342,  -0.2231,  -0.2486],\n",
       "                      [  0.1847,  -0.3991,   2.2196,  ...,  -0.1009,   0.0805,  -0.1358],\n",
       "                      ...,\n",
       "                      [ -0.1553,  -0.6489,   0.0532,  ...,   0.8655,  -0.0373,   0.7890],\n",
       "                      [ -0.2660,  -0.6264,  -0.1804,  ...,   0.0524,  -0.9147,   0.1183],\n",
       "                      [  0.0646,  -0.0567,  -0.6383,  ...,  -0.3536,   0.9300, -10.5892]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_ih_l1_reverse',\n",
       "              tensor([-2.6021, -0.6104, -3.9720,  ..., -3.5064, -2.0966,  0.4290], device='cuda:0')),\n",
       "             ('rnn.bias_hh_l1_reverse',\n",
       "              tensor([-2.6344, -0.6041, -3.9659,  ..., -3.5230, -2.0916,  0.4428], device='cuda:0')),\n",
       "             ('decoder.weight',\n",
       "              tensor([[ -0.6946,  -2.3455,   0.5366,  ...,  -4.5290,  -3.0686,  -6.0470],\n",
       "                      [ -0.4994,  -2.8259,   0.3551,  ...,  -5.7168,  -7.2108,  -6.1350],\n",
       "                      [ -0.5947,  -0.6791,   0.0453,  ...,  -0.8260,  -0.4541,  -8.8105],\n",
       "                      ...,\n",
       "                      [ -0.5601,  -0.0610,   0.0821,  ...,  -0.0817,   0.2552,  -6.0384],\n",
       "                      [ -0.5593,  -0.1753,  -0.0332,  ...,  -0.1251,  -1.4170, -10.3831],\n",
       "                      [ -0.2591,  -0.0229,  -0.0066,  ...,  -0.1409,  -0.4237,  -8.9816]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.bias',\n",
       "              tensor([6.7815, 6.9254, 0.6788,  ..., 1.5396, 1.6113, 1.3586], device='cuda:0'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do the embeddings look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.weight', 'rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.bias_hh_l0', 'rnn.weight_ih_l0_reverse', 'rnn.weight_hh_l0_reverse', 'rnn.bias_ih_l0_reverse', 'rnn.bias_hh_l0_reverse', 'rnn.weight_ih_l1', 'rnn.weight_hh_l1', 'rnn.bias_ih_l1', 'rnn.bias_hh_l1', 'rnn.weight_ih_l1_reverse', 'rnn.weight_hh_l1_reverse', 'rnn.bias_ih_l1_reverse', 'rnn.bias_hh_l1_reverse', 'decoder.weight', 'decoder.bias'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33278, 650])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = model.state_dict()['encoder.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<eos>', '=', 'Valkyria', ..., 'Nests', 'flea', 'gallinae'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(corpus.dictionary.word2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flour',\n",
       " 'water',\n",
       " 'bread',\n",
       " 'coffee',\n",
       " 'espresso',\n",
       " 'driving',\n",
       " 'car',\n",
       " 'horse',\n",
       " 'chicken',\n",
       " 'bird',\n",
       " 'cow',\n",
       " 'leg']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_words = ['flour', 'water', 'bread', 'coffee', 'espresso', 'driving', 'car', 'horse', 'chicken', 'bird', 'cow', 'leg']\n",
    "some_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_idxs = [corpus.dictionary.word2idx[word] for word in some_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 650])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.879715  ,  2.1987057 , -0.16326854, ..., -1.0510355 ,\n",
       "         3.9122596 ,  0.4961418 ],\n",
       "       [-2.5037746 , -0.0381213 , -3.6407967 , ..., -2.5569913 ,\n",
       "        -2.5834985 ,  2.1187277 ],\n",
       "       [ 1.9972423 , -2.9193065 ,  5.3105016 , ..., -5.7143364 ,\n",
       "        -0.9835462 ,  1.87561   ],\n",
       "       ...,\n",
       "       [ 1.9894309 ,  4.1524463 ,  1.3620085 , ..., -7.8408737 ,\n",
       "         4.8566976 ,  2.6350994 ],\n",
       "       [ 0.8472853 , -1.1799922 ,  3.5122542 , ...,  0.92924136,\n",
       "         3.592625  , -0.26445252],\n",
       "       [-0.95803046,  0.7104165 , -0.06647003, ..., -1.605507  ,\n",
       "        -8.183998  ,  0.5108651 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embeds[some_idxs].shape)\n",
    "np.array(embeds[some_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=123)\n",
    "#np.set_printoptions(suppress=True)\n",
    "Y = tsne.fit_transform(np.array(embeds[some_idxs]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_coords = Y[:, 0]\n",
    "y_coords = Y[:, 1]\n",
    "# display scatter plot\n",
    "plt.scatter(x_coords, y_coords)\n",
    "\n",
    "for label, x, y in zip(some_words, x_coords, y_coords):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is still somewhat terrible. The words seem to be a bit better now, looks like some sentences are there, but this model has hit its limits and it doesn't appear that more training time will help at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
